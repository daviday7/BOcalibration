{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPy\n",
      "  Downloading GPy-1.10.0-cp37-cp37m-win_amd64.whl (1.4 MB)\n",
      "Collecting paramz>=0.9.0\n",
      "  Using cached paramz-0.9.5.tar.gz (71 kB)\n",
      "Requirement already satisfied: six in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from GPy) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from GPy) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.3.0 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from GPy) (1.4.1)\n",
      "Requirement already satisfied: cython>=0.29 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from GPy) (0.29.15)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from paramz>=0.9.0->GPy) (4.4.1)\n",
      "Building wheels for collected packages: paramz\n",
      "  Building wheel for paramz (setup.py): started\n",
      "  Building wheel for paramz (setup.py): finished with status 'done'\n",
      "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102556 sha256=2992c92e590b25e36b31390277dc34ae6dc6f88df0bd0e181a2ea44f311a2586\n",
      "  Stored in directory: c:\\users\\dzerz\\appdata\\local\\pip\\cache\\wheels\\c8\\95\\f5\\ce28482da28162e6028c4b3a32c41d147395825b3cd62bc810\n",
      "Successfully built paramz\n",
      "Installing collected packages: paramz, GPy\n",
      "Successfully installed GPy-1.10.0 paramz-0.9.5\n",
      "Collecting pyGPs\n",
      "  Using cached pyGPs-1.3.5.tar.gz (10.3 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from pyGPs) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.13 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from pyGPs) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from pyGPs) (3.1.3)\n",
      "Requirement already satisfied: future in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from pyGPs) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from matplotlib->pyGPs) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from matplotlib->pyGPs) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from matplotlib->pyGPs) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from matplotlib->pyGPs) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->pyGPs) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->pyGPs) (45.2.0.post20200210)\n",
      "Building wheels for collected packages: pyGPs\n",
      "  Building wheel for pyGPs (setup.py): started\n",
      "  Building wheel for pyGPs (setup.py): finished with status 'done'\n",
      "  Created wheel for pyGPs: filename=pyGPs-1.3.5-py3-none-any.whl size=72636 sha256=ea39929fa21278ddbe99ba391239f120fdee4d926301c72557ea6863fbcef1ba\n",
      "  Stored in directory: c:\\users\\dzerz\\appdata\\local\\pip\\cache\\wheels\\c1\\2e\\0e\\1e78cf102e0a562c9a3c9ae5ab9bd28dbbb823235b7e0d82db\n",
      "Successfully built pyGPs\n",
      "Installing collected packages: pyGPs\n",
      "Successfully installed pyGPs-1.3.5\n",
      "Collecting rpy2\n",
      "  Downloading rpy2-3.5.11.tar.gz (216 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from rpy2) (2.11.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from rpy2) (2019.3)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging; platform_system == \"Windows\" in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from rpy2) (20.1)\n",
      "Requirement already satisfied: cffi>=1.10.0 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from rpy2) (1.14.0)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-4.3-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from jinja2->rpy2) (1.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from packaging; platform_system == \"Windows\"->rpy2) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from packaging; platform_system == \"Windows\"->rpy2) (2.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dzerz\\anaconda3\\lib\\site-packages (from cffi>=1.10.0->rpy2) (2.19)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata; platform_system == \"Windows\"\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting backports.zoneinfo; python_version < \"3.9\"\n",
      "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-win_amd64.whl (38 kB)\n",
      "Building wheels for collected packages: rpy2\n",
      "  Building wheel for rpy2 (PEP 517): started\n",
      "  Building wheel for rpy2 (PEP 517): finished with status 'done'\n",
      "  Created wheel for rpy2: filename=rpy2-3.5.11-py3-none-any.whl size=218566 sha256=b726cf829bd6debf27d9a9d6024ad818ed3fdae1b81842a6d23e065d980ad2bd\n",
      "  Stored in directory: c:\\users\\dzerz\\appdata\\local\\pip\\cache\\wheels\\ff\\f4\\a3\\60a6024163a155268ce89f0fe2ec1a6a9a956f817c29382eb2\n",
      "Successfully built rpy2\n",
      "Installing collected packages: typing-extensions, backports.zoneinfo, tzdata, pytz-deprecation-shim, tzlocal, rpy2\n",
      "Successfully installed backports.zoneinfo-0.2.1 pytz-deprecation-shim-0.1.0.post0 rpy2-3.5.11 typing-extensions-4.5.0 tzdata-2023.3 tzlocal-4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install GPy\n",
    "!pip install pyGPs\n",
    "!pip install rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.signal import convolve\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.sparse import csc_matrix\n",
    "from typing import Dict, Any, List, Optional\n",
    "from functools import reduce\n",
    "from pyDOE import lhs\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import multiprocessing\n",
    "import GPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, Formula\n",
    "import inspect\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "#import multiprocessing as mp\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian mixture\n",
    "\n",
    "def fn_gmm_mixtools(vals):\n",
    "    # Add a small amount of noise to the data\n",
    "    corrected_vals = vals + np.random.uniform(0, 0.0001, size=len(vals))\n",
    "\n",
    "    try:\n",
    "        # Fit a Gaussian mixture model with 3 components using the EM algorithm\n",
    "        gmm = GaussianMixture(n_components=3, covariance_type='full', max_iter=300)\n",
    "        gmm.fit(corrected_vals.reshape(-1, 1))\n",
    "\n",
    "        return {\n",
    "            'mu': gmm.means_.reshape(-1, 1),\n",
    "            'sigma': np.sqrt(gmm.covariances_).reshape(-1, 1),\n",
    "            'lambda': gmm.weights_.reshape(-1, 1),\n",
    "        }\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian mixture - finding peaks\n",
    "\n",
    "def fn_gmm_mixtools_v2(vals):\n",
    "    corrected_vals = vals + np.random.uniform(0, 0.0001, size=len(vals))\n",
    "    try:\n",
    "        kde = gaussian_kde(corrected_vals)\n",
    "        x_vals = np.linspace(min(corrected_vals), max(corrected_vals), 1000)\n",
    "        y_vals = kde(x_vals)\n",
    "        df = np.column_stack((x_vals, y_vals))\n",
    "        df = df[np.insert(np.diff(np.diff(df[:, 1])) >= 0, 0, False), :]    #finding peaks (where 2nd derivative changes sign)\n",
    "        y_max = np.max(df[:, 1])\n",
    "        y_max_limit = y_max * 0.2\n",
    "        x_mean = df[df[:, 1] > y_max_limit, 0]\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=3, means_init=np.reshape(x_mean, (-1, 1)), max_iter=300)\n",
    "        gmm.fit(np.reshape(corrected_vals, (-1, 1)))\n",
    "        \n",
    "        return {\"mu\": np.reshape(gmm.means_, (-1, 1)),\n",
    "                \"sigma\": np.reshape(np.sqrt(gmm.covariances_), (-1, 1)),\n",
    "                \"lambda\": np.reshape(gmm.weights_, (-1, 1))}\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_gmm_mclust(vals):\n",
    "    corrected_vals = vals + np.random.uniform(0, 0.0001, len(vals))\n",
    "    gmm_fit = GaussianMixture(n_components=1, covariance_type='full').fit(corrected_vals.reshape(-1, 1))\n",
    "\n",
    "    mu_est = gmm_fit.means_\n",
    "    sigma_est = np.sqrt(gmm_fit.covariances_)\n",
    "\n",
    "    if len(mu_est) == 1:\n",
    "        lambda_est = np.array([1])\n",
    "    else:\n",
    "        lambda_est = gmm_fit.weights_\n",
    "\n",
    "    return {'mu': mu_est, 'sigma': sigma_est, 'lambda': lambda_est}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_gmm_kclust(vals):\n",
    "    corrected_vals = vals + np.random.uniform(0, 0.0001, size=len(vals))\n",
    "    num_clusters = None\n",
    "    \n",
    "    try:\n",
    "        sil_scores = []\n",
    "        for k in range(2, 6):                                  # not sure why 2-6 clusters\n",
    "            kmeans = KMeans(n_clusters=k, init='k-means++')\n",
    "            kmeans.fit(corrected_vals.reshape(-1, 1))\n",
    "            sil_scores.append(silhouette_score(corrected_vals.reshape(-1, 1), kmeans.labels_))\n",
    "        num_clusters = np.argmax(sil_scores) + 2\n",
    "        \n",
    "    except:\n",
    "        num_clusters = 1\n",
    "    \n",
    "    if num_clusters == 1:\n",
    "        clustered_stats = {\n",
    "            'lambda': 1.0,\n",
    "            'mu': np.mean(corrected_vals),\n",
    "            'sigma': np.std(corrected_vals)\n",
    "        }\n",
    "    else:\n",
    "        gmm_fit = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
    "        gmm_fit.fit(corrected_vals.reshape(-1, 1))\n",
    "        clustered_stats = []\n",
    "        for i in range(num_clusters):\n",
    "            lambda_est = gmm_fit.weights_[i]\n",
    "            mu_est = gmm_fit.means_[i][0]\n",
    "            sigma_est = np.sqrt(gmm_fit.covariances_[i][0])\n",
    "            clustered_stats.append({'lambda': lambda_est, 'mu': mu_est, 'sigma': sigma_est})\n",
    "    \n",
    "    return {'mu': [x['mu'] for x in clustered_stats],\n",
    "            'sigma': [x['sigma'] for x in clustered_stats],\n",
    "            'lambda': [x['lambda'] for x in clustered_stats]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility(utility_func, *args):\n",
    "    return utility_func(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_ei(traits, optimal_response, complement_u=False, estimated_sigma=None):\n",
    "    m = traits['mu']\n",
    "    s = traits['sigma']\n",
    "    \n",
    "    ## If sigma is estimated with other other than empirical estimation\n",
    "    if estimated_sigma is not None:\n",
    "        s = estimated_sigma\n",
    "    \n",
    "    s[s[:, 0] == 0, 0] = np.nan\n",
    "    \n",
    "    gamma = (optimal_response - m) / s\n",
    "    u_val_mat = s * (gamma * norm.cdf(gamma) + norm.pdf(gamma))\n",
    "    u_val_mat = np.hstack((u_val_mat, np.zeros((u_val_mat.shape[0], 7))))\n",
    "    \n",
    "    if complement_u:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response)\n",
    "        complement_utilities_fn = [\n",
    "            {'f': fn_utility_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response}},\n",
    "            {'f': fn_utility_gmix_agg_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_agg_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}}\n",
    "        ]\n",
    "        \n",
    "        c_utilities_val = np.hstack([fn['f'](**fn['params']) for fn in complement_utilities_fn])\n",
    "        u_val_mat = np.hstack((u_val_mat, c_utilities_val))\n",
    "    \n",
    "    return u_val_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################################################\n",
    "########################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Compute variance of estimate from a ranger model\n",
    "# \n",
    "# Computes variances for a prediction from a ranger model, using the infinitesimal jackknife procedure\n",
    "# \n",
    "# This function is a ranger-adaptation of the package \\pkg{randomForestCI} of Wager et al. (2014). Their original can be found on github: \\url{ https://github.com/swager/randomForestCI/}. \n",
    "#\n",
    "# @param pred A nrow(newdata) by no. of trees matrix which contains numeric predictions\n",
    "#        from a random forest trained with trees grown on bootstrap samples of the training data\n",
    "# @param inbag A number of obs. in the training data by no. of trees matrix giving the\n",
    "#        number of times the ith observation in the training data appeared in the bootstrap sample for the jth tree.\n",
    "# @param calibrate whether to apply calibration to mitigate Monte Carlo noise\n",
    "#        warning: if calibrate = FALSE, some variance estimates may be negative\n",
    "#                 due to Monte Carlo effects if the number of trees in rf is too small\n",
    "# @param used.trees set of trees to use for variance estimation; uses all tress if NULL\n",
    "#\n",
    "# @return A two-column matrix is returned, with predictions in the first column and estimates of prediction variance in the second. \n",
    "# @author Stefan Wager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ May needs repair .dot->@  #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rInfJack(pred, inbag, calibrate=True, used_trees=None):\n",
    "    \"\"\"\n",
    "    Compute the infinitesimal jackknife variance estimator for a random forest model.\n",
    "\n",
    "    Args:\n",
    "        pred (ndarray): An n x B matrix of tree-wise predictions for n observations and B trees.\n",
    "        inbag (ndarray): An n x B matrix indicating which observations were included in each bootstrap sample.\n",
    "        calibrate (bool): Whether to calibrate the variance estimates using empirical Bayes. Default is True.\n",
    "        used_trees (list): A list of indices indicating which trees to use in the variance estimation. Default is all trees.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: An n x 2 array of estimated mean and variance for each observation.\n",
    "    \"\"\"\n",
    "    if used_trees is None:\n",
    "        used_trees = range(pred.shape[1])\n",
    "\n",
    "    pred = pred[:, used_trees]\n",
    "    \n",
    "    # Check if sampling without replacement\n",
    "    no_replacement = (np.max(inbag) == 1)\n",
    "    \n",
    "    # Extract tree-wise predictions and variable counts from random forest\n",
    "    B = len(used_trees)\n",
    "    n = inbag.shape[0]\n",
    "    s = np.sum(inbag, axis=1) / inbag.shape[1]\n",
    "    \n",
    "    y_hat = np.mean(pred, axis=1)\n",
    "    pred_centered = pred - np.mean(pred, axis=1, keepdims=True)\n",
    "    \n",
    "    N = csr_matrix(inbag[:, used_trees])\n",
    "    N_avg = N.mean(axis=1)\n",
    "    \n",
    "    # Compute raw infinitesimal jackknife\n",
    "    if B ** 2 > n * pred.shape[0]:\n",
    "        C = N.dot(pred_centered).T - N_avg.dot(np.sum(pred_centered, axis=0))\n",
    "        raw_IJ = np.sum(C ** 2, axis=0) / B ** 2\n",
    "    else:\n",
    "    # Faster implementation when n is large. Uses the fact that colSums((A - B)^2) = T1 - 2 * T2 + T3,\n",
    "    # where T1 = diag(A'A), T2 = diag(B'A), and T3 = diag(B'B)\n",
    "        NTN = N.T.dot(N)\n",
    "        NTNPT_T = pred_centered.T.dot(NTN)  # or NTNPT_T = pred_centered.T @ NTN\n",
    "        T1 = np.sum(pred_centered * NTNPT_T, axis=0)\n",
    "        \n",
    "        RS = np.sum(pred_centered, axis=0)\n",
    "        NbarTN = N_avg.T.dot(N)\n",
    "        T2 = RS * NbarTN.dot(pred_centered)   #.T   or RS * NbarTN @ pred_centered\n",
    "        \n",
    "        T3 = np.sum(N_avg ** 2) * RS ** 2\n",
    "        raw_IJ = (T1 - 2 * T2 + T3) / B ** 2\n",
    "    \n",
    "    # Apply Monte Carlo bias correction\n",
    "    N_var = np.mean(N.power(2).mean(axis=1) - N_avg.power(2))\n",
    "    boot_var = np.sum(pred_centered ** 2, axis=1) / B\n",
    "    bias_correction = n * N_var * boot_var / B\n",
    "    vars = raw_IJ - bias_correction\n",
    "    \n",
    "    # Finite sample correction\n",
    "    if no_replacement:\n",
    "        variance_inflation = 1 / (1 - np.mean(inbag, axis=1)) ** 2\n",
    "        vars *= variance_inflation[:, np.newaxis]\n",
    "    \n",
    "    results = np.hstack((y_hat[:, np.newaxis], vars[:, np.newaxis]))\n",
    "    \n",
    "    if calibrate and results.shape[0] <= 20:\n",
    "        calibrate = False\n",
    "        print(\"Sample size <= 20, no calibration performed.\")\n",
    "    \n",
    "                               \n",
    "\n",
    "    if calibrate:\n",
    "        # Compute variance estimates using half the trees\n",
    "        calibration_ratio = 2\n",
    "        n_sample = np.ceil(B / calibration_ratio).astype(int)\n",
    "        #used_trees=np.random.choice(used.trees, n_sample, replace=False)\n",
    "        results_ss = rInfJack(pred, inbag, calibrate=False, used_trees=np.random.choice(used_trees, n_sample, replace=False))\n",
    "\n",
    "        # Use this second set of variance estimates to estimate scale of Monte Carlo noise\n",
    "        sigma2_ss = np.mean((results_ss['var.hat'] - results['var.hat'])**2)\n",
    "        delta = n_sample / B\n",
    "        sigma2 = (delta**2 + (1 - delta)**2) / (2 * (1 - delta)**2) * sigma2_ss\n",
    "\n",
    "        # Use Monte Carlo noise scale estimate for empirical Bayes calibration\n",
    "        try:\n",
    "            vars_calibrated = calibrateEB(vars, sigma2)\n",
    "            results['var.hat'] = vars_calibrated\n",
    "        except Exception as e:\n",
    "            print(f\"Calibration failed with error:\\n{e}\\nFalling back to non-calibrated variance estimates.\")\n",
    "            #warnings.warn(f\"Calibration failed with error:\\n{str(e)}\\nFalling back to non-calibrated variance estimates.\")\n",
    "            results = rInfJack(pred, inbag, calibrate=False, used_trees=used_trees)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an empirical Bayes prior in the hierarchical model\n",
    "#     mu ~ G, X ~ N(mu, sigma^2)\n",
    "#\n",
    "# @param X a vector of observations\n",
    "# @param sigma noise estimate\n",
    "# @param p tuning parameter -- number of parameters used to fit G\n",
    "# @param nbin tuning parameter -- number of bins used for discrete approximation\n",
    "# @param unif.fraction tuning parameter -- fraction of G modeled as \"slab\"\n",
    "#\n",
    "# @return posterior density estimate g\n",
    "#\n",
    "# @section References:\n",
    "# For more details about \"g-estimation\", see: B Efron. Two modeling strategies for\n",
    "# empirical Bayes estimation. Stat. Sci., 29: 285-301, 2014.\n",
    "# @author Stefan Wager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "####  maybe @ should replaced by .dot() #####\n",
    "############################################\n",
    "\n",
    "def gfit(X, sigma, p=2, nbin=1000, unif_fraction=0.1):\n",
    "    \n",
    "    # Define the bin boundaries and width\n",
    "    xvals = np.linspace(min(np.min(X) - 2 * np.std(X), 0), max(np.max(X) + 2 * np.std(X), np.std(X)), num=nbin)\n",
    "    binw = xvals[1] - xvals[0]\n",
    "    \n",
    "    # Rotate the noise kernel so that it aligns with the zero point\n",
    "    zero_idx = np.max(np.where(xvals <= 0))\n",
    "    noise_kernel = np.exp(-0.5 * (xvals/sigma)**2) * binw / sigma\n",
    "    noise_rotate = np.roll(noise_kernel, -zero_idx)\n",
    "    \n",
    "    # Generate the design matrix XX\n",
    "    XX = np.column_stack([xvals**j * (xvals >= 0) for j in range(1, p+1)])\n",
    "    \n",
    "    # Define the negative log-likelihood function\n",
    "    def neg_loglik(eta):\n",
    "        g_eta_raw = np.exp(XX @ eta) * (xvals >= 0)\n",
    "        if (np.sum(g_eta_raw) == np.inf) or (np.sum(g_eta_raw) <= 100 * np.finfo(np.double).eps):\n",
    "            return 1000 * (len(X) + np.sum(eta**2))\n",
    "        g_eta_main = g_eta_raw / np.sum(g_eta_raw)\n",
    "        g_eta = (1 - unif_fraction) * g_eta_main + unif_fraction * (xvals >= 0) / np.sum(xvals >= 0)\n",
    "        f_eta = convolve(g_eta, noise_rotate, mode='same')\n",
    "        return np.sum(np.interp(X, xvals, -np.log(np.maximum(f_eta, 1e-7))))\n",
    "    \n",
    "    # Fit the model using the Nelder-Mead algorithm\n",
    "    eta_hat = minimize(neg_loglik, np.repeat(-1, p), method='nelder-mead').x\n",
    "    g_eta_raw = np.exp(XX @ eta_hat) * (xvals >= 0)\n",
    "    g_eta_main = g_eta_raw / np.sum(g_eta_raw)\n",
    "    g_eta = (1 - unif_fraction) * g_eta_main + unif_fraction * (xvals >= 0) / np.sum(xvals >= 0)\n",
    "    \n",
    "    # Return the estimated density function\n",
    "    return {'x': xvals, 'g': g_eta}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbayes(x0, gest, sigma):\n",
    "    Kx = np.exp(-(gest['x'] - x0)**2 / (2 * sigma**2)) / np.sqrt(2 * np.pi * sigma**2)\n",
    "    post = Kx * gest['g']\n",
    "    post = post / np.sum(post)\n",
    "    return np.sum(post * gest['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical Bayes calibration of noisy variance estimates\n",
    "#\n",
    "# @param vars list of variance estimates\n",
    "# @param sigma2 estimate of the Monte Carlo noise in vars\n",
    "#\n",
    "# @return calibrated variance estimates\n",
    "# @author Stefan Wager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrateEB(vars, sigma2):\n",
    "    if sigma2 <= 0 or np.min(vars) == np.max(vars):\n",
    "        return np.maximum(vars, 0)\n",
    "    \n",
    "    sigma = np.sqrt(sigma2)\n",
    "    eb_prior = gfit(vars, sigma)\n",
    "    \n",
    "    if len(vars) >= 200:\n",
    "        # If there are many test points, use interpolation to speed up computations\n",
    "        calib_x = np.unique(np.quantile(vars, q=np.arange(0, 1.02, 0.02)))\n",
    "        calib_y = np.array([gbayes(xx, eb_prior, sigma) for xx in calib_x])\n",
    "        f = interp1d(calib_x, calib_y, kind='cubic', fill_value='extrapolate')\n",
    "        calib_all = f(vars)\n",
    "    else:\n",
    "        calib_all = np.array([gbayes(xx, eb_prior, sigma) for xx in vars])\n",
    "    \n",
    "    return calib_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################################################\n",
    "########################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_ei_ij(traits, optimal_response, complement_u=False):\n",
    "    m = traits['mu']\n",
    "    s = traits['sigma']\n",
    "    \n",
    "    # Compute IJ estimator for variance\n",
    "    inbag = np.column_stack(traits['custom']['inbag'])\n",
    "    sigma_IJ = rInfJack(traits['values'], inbag, calibrate=True)['var.hat']\n",
    "    \n",
    "    # If sigma is estimated with other than empirical estimation\n",
    "    if sigma_IJ is not None:\n",
    "        s = np.array(sigma_IJ)\n",
    "    \n",
    "    s[s == 0] = np.nan\n",
    "    \n",
    "    gamma = (optimal_response - m) / s\n",
    "    u_val_mat = s * (gamma * norm.cdf(gamma) + norm.pdf(gamma))\n",
    "    u_val_mat = np.column_stack((u_val_mat, np.zeros(len(m))))  # add column for complement utility\n",
    "    \n",
    "    if complement_u:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response)\n",
    "        \n",
    "        complement_utilities_fn = [\n",
    "            {'f': fn_utility_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response}},\n",
    "            {'f': fn_utility_gmix_agg_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_agg_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}}\n",
    "        ]\n",
    "        \n",
    "        c_utilities_val = np.column_stack([u_f['f'](**u_f['params']) for u_f in complement_utilities_fn])\n",
    "        u_val_mat = np.column_stack((u_val_mat, c_utilities_val))\n",
    "    \n",
    "    return u_val_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_lcb(traits: Dict[str, np.ndarray], optimal_response: np.ndarray, kappa: float = 2.0, complement_u: bool = False, estimated_sigma: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "    m = traits['mu']\n",
    "    s = traits['sigma']\n",
    "    \n",
    "    ## If sigma is estimated with other than empirical estimation\n",
    "    if estimated_sigma is not None:\n",
    "        s = estimated_sigma\n",
    "    \n",
    "    u_val = -1 * (m - kappa * s)\n",
    "    u_val_mat = np.tile(u_val, (len(m), 1)).T\n",
    "    u_val_mat = u_val_mat.astype(float)\n",
    "    u_val_mat = np.hstack([u_val_mat])\n",
    "    \n",
    "    if complement_u:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response)\n",
    "        \n",
    "        complement_utilities_fn = [\n",
    "            {\"f\": fn_utility_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response}},\n",
    "            {\"f\": fn_utility_gmix_agg_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_wmax_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_agg_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_wmax_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}}\n",
    "        ]\n",
    "        \n",
    "        c_utilities_val = np.hstack([u_f[\"f\"](**u_f[\"params\"]) for u_f in complement_utilities_fn])\n",
    "        u_val_mat = np.hstack([u_val_mat, c_utilities_val])\n",
    "    \n",
    "    return u_val_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_lcb_ij(traits, optimal_response, complement_u = False):\n",
    "    sigma_IJ <- rInfJack(traits['values'], np.column_stack(traits['custom']['inbag']), calibrate = True)\n",
    "    return fn_utility_lcb(traits, optimal_response, complement_u, estimated_sigma = float(sigma_IJ['var.hat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_global_gmix(traits, optimal_response, byrow=True):\n",
    "    apply_margin = 1 if byrow else 0\n",
    "    def gmix_fn(vals):\n",
    "        gmix = GaussianMixture(n_components=1).fit(vals.reshape(-1, 1))\n",
    "        mu, sigma = gmix.means_, np.sqrt(gmix.covariances_)\n",
    "        lambda_ = np.array([1.])\n",
    "        return {'mu': mu, 'sigma': sigma, 'lambda': lambda_}\n",
    "    return np.apply_along_axis(gmix_fn, axis=apply_margin, arr=traits['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_gmix_agg_ei(traits, optimal_response, byrow=True, complement_u=False, list_MM=None):\n",
    "    if list_MM is None:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response, byrow)\n",
    "    else:\n",
    "        list_gMix = list_MM\n",
    "        \n",
    "    # Integral part of the utility calculation\n",
    "    utilities_gMix = np.vstack([\n",
    "        np.dot(single_gMix.rx2(\"lambda\")[:, 0], fn_utility_ei(\n",
    "            traits,\n",
    "            optimal_response,\n",
    "            estimated_sigma=np.asarray(single_gMix.rx2(\"var.hat\"))\n",
    "        ))\n",
    "        for single_gMix in list_gMix\n",
    "    ]).T\n",
    "    utilities_gMix = pd.DataFrame(utilities_gMix, columns=[\"gmix_agg_ei\"])\n",
    "    \n",
    "    if complement_u:\n",
    "        complement_utilities_fn = [\n",
    "            {\"f\": fn_utility_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response}},\n",
    "            {\"f\": fn_utility_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response}},\n",
    "            {\"f\": fn_utility_gmix_wmax_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_agg_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_wmax_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}}\n",
    "        ]\n",
    "        \n",
    "        c_utilities_val = pd.concat([\n",
    "            pd.DataFrame(do.call(u_f[\"f\"], u_f[\"params\"])).T\n",
    "            for u_f in complement_utilities_fn\n",
    "        ], axis=1)\n",
    "        utilities_gMix = pd.concat([utilities_gMix, c_utilities_val], axis=1)\n",
    "        \n",
    "    return utilities_gMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_gmix_wmax_ei(traits, optimal_response, byrow=True, complement_u=False, list_MM=None):\n",
    "    list_gMix = list_MM\n",
    "    if list_gMix is None:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response, byrow)\n",
    "    \n",
    "    # Integral part of the utility calculation\n",
    "    utilities_gMix = np.vstack([single_gMix['lambda'][:, 0] * fn_utility_ei(single_gMix, optimal_response)[:, 0]\n",
    "                                for single_gMix in list_gMix])\n",
    "    util_gMix_max_idx = np.argmax(utilities_gMix, axis=0)\n",
    "    utilities_gMix = np.multiply(utilities_gMix[util_gMix_max_idx, :], list_gMix[util_gMix_max_idx]['lambda'][:, 0])\n",
    "    utilities_gMix = utilities_gMix.reshape(-1, 1)\n",
    "    utilities_gMix = np.column_stack((utilities_gMix,))\n",
    "\n",
    "    colnames = [\"gmix_wmax_ei\"]\n",
    "    \n",
    "    if complement_u:\n",
    "        complement_utilities_fn = [\n",
    "            {\"f\": fn_utility_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response}},\n",
    "            {\"f\": fn_utility_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response}},\n",
    "            {\"f\": fn_utility_gmix_agg_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_agg_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_wmax_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}}\n",
    "        ]\n",
    "        \n",
    "        c_utilities_val = np.hstack([do_call(u_f[\"f\"], **u_f[\"params\"]) for u_f in complement_utilities_fn])\n",
    "        utilities_gMix = np.column_stack((utilities_gMix, c_utilities_val))\n",
    "\n",
    "    return pd.DataFrame(utilities_gMix, columns=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_gmix_max_ei(traits, optimal_response, byrow=True, complement_u=False, list_MM=None):\n",
    "    list_gMix = list_MM\n",
    "    if list_gMix is None:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response, byrow)\n",
    "    \n",
    "    utilities_gMix = np.concatenate([fn_utility_ei(single_gMix, optimal_response)[np.argmax(fn_utility_ei(single_gMix, optimal_response)), None] \n",
    "                                     for single_gMix in list_gMix], axis=0)\n",
    "    utilities_gMix = utilities_gMix.reshape(-1, 1)\n",
    "    utilities_gMix = np.hstack([utilities_gMix])\n",
    "    utilities_gMix = np.hstack([utilities_gMix, np.zeros((traits.shape[0], 0))])\n",
    "    \n",
    "    if complement_u:\n",
    "        complement_utilities_fn = [\n",
    "            {'f': fn_utility_ei, 'params': {'traits': traits, 'optimal_response': optimal_response}},\n",
    "            {'f': fn_utility_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response}},\n",
    "            {'f': fn_utility_gmix_agg_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_agg_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}}\n",
    "        ]\n",
    "        \n",
    "        c_utilities_val = np.hstack([f['f'](**f['params']) for f in complement_utilities_fn])\n",
    "        utilities_gMix = np.hstack([utilities_gMix, c_utilities_val])\n",
    "    \n",
    "    return utilities_gMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_gmix_agg_lcb(traits, optimal_response, byrow=True, complement_u=False, list_MM=None):\n",
    "    if list_MM is None:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response, byrow)\n",
    "    else:\n",
    "        list_gMix = list_MM\n",
    "\n",
    "    # Integral part of the utility calculation\n",
    "    utilities_gMix = np.vstack([np.dot(single_gMix['lambda'].T, fn_utility_lcb(single_gMix, optimal_response)) for single_gMix in list_gMix])\n",
    "    utilities_gMix = np.column_stack((utilities_gMix, np.array([[\"gmix_agg_lcb\"]*utilities_gMix.shape[0]]).T))\n",
    "\n",
    "    if complement_u:\n",
    "        complement_utilities_fn = [\n",
    "            {\"f\": fn_utility_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response}},\n",
    "            {\"f\": fn_utility_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response}},\n",
    "            {\"f\": fn_utility_gmix_agg_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_wmax_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_ei, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_wmax_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}},\n",
    "            {\"f\": fn_utility_gmix_max_lcb, \"params\": {\"traits\": traits, \"optimal_response\": optimal_response, \"list_MM\": list_gMix}}\n",
    "        ]\n",
    "        c_utilities_val = reduce(lambda x, y: np.column_stack((x, y)), [f[\"f\"](**f[\"params\"]) for f in complement_utilities_fn])\n",
    "        utilities_gMix = np.column_stack((utilities_gMix, c_utilities_val))\n",
    "\n",
    "    return utilities_gMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_gmix_wmax_lcb(traits, optimal_response, byrow=True, complement_u=False, list_MM=None):\n",
    "    list_gMix = list_MM\n",
    "    if list_gMix is None:\n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response, byrow)\n",
    "\n",
    "    # Integral part of the utility calculation\n",
    "    utilities_gMix = np.concatenate([base_utility(single_gMix, optimal_response)[:, np.argmax(base_utility(single_gMix, optimal_response)[:, 0])] * single_gMix['lambda'][np.argmax(base_utility(single_gMix, optimal_response)[:, 0])] for single_gMix in list_gMix]).reshape(-1, 1)\n",
    "    utilities_gMix = np.hstack((utilities_gMix,))\n",
    "\n",
    "    if complement_u:\n",
    "        complement_utilities_fn = [\n",
    "            {'f': fn_utility_ei, 'params': {'traits': traits, 'optimal_response': optimal_response}},\n",
    "            {'f': fn_utility_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response}},\n",
    "            {'f': fn_utility_gmix_agg_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_agg_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}}\n",
    "        ]\n",
    "\n",
    "        c_utilities_val = np.column_stack([fn['f'](**fn['params']) for fn in complement_utilities_fn])\n",
    "        utilities_gMix = np.column_stack((utilities_gMix, c_utilities_val))\n",
    "\n",
    "    return utilities_gMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_gmix_max_lcb(traits, optimal_response, byrow=True, complement_u=False, list_MM=None):\n",
    "    if list_MM is None:\n",
    "        list_MM = fn_utility_global_gmix(traits, optimal_response, byrow)\n",
    "\n",
    "    ## Integral part of the utility calculation\n",
    "    utilities_gMix = np.vstack([single_gMix['lambda'] * base_utility(single_gMix, optimal_response)[util_gMix.argmax(axis=0)[0], :] for single_gMix in list_gMix])\n",
    "    utilities_gMix = utilities_gMix[:, np.newaxis] # add a singleton dimension to match R's output format\n",
    "    utilities_gMix = np.column_stack((utilities_gMix, ))\n",
    "    utilities_gMix.columns = ['gmix_wmax_lcb']\n",
    "\n",
    "    if complement_u:\n",
    "        complement_utilities_fn = [\n",
    "            dict(f=fn_utility_ei, params=dict(traits=traits, optimal_response=optimal_response)),\n",
    "            dict(f=fn_utility_lcb, params=dict(traits=traits, optimal_response=optimal_response)),\n",
    "            dict(f=fn_utility_gmix_agg_ei, params=dict(traits=traits, optimal_response=optimal_response, list_MM=list_gMix)),\n",
    "            dict(f=fn_utility_gmix_wmax_ei, params=dict(traits=traits, optimal_response=optimal_response, list_MM=list_gMix)),\n",
    "            dict(f=fn_utility_gmix_max_ei, params=dict(traits=traits, optimal_response=optimal_response, list_MM=list_gMix)),\n",
    "            dict(f=fn_utility_gmix_agg_lcb, params=dict(traits=traits, optimal_response=optimal_response, list_MM=list_gMix)),\n",
    "            dict(f=fn_utility_gmix_wmax_lcb, params=dict(traits=traits, optimal_response=optimal_response, list_MM=list_gMix)),\n",
    "        ]\n",
    "\n",
    "        c_utilities_val = np.column_stack([fn(**params) for fn, params in complement_utilities_fn])\n",
    "        utilities_gMix = np.column_stack((utilities_gMix, c_utilities_val))\n",
    "        utilities_gMix.columns = ['gmix_wmax_lcb', 'ei', 'lcb', 'gmix_agg_ei', 'gmix_wmax_ei', 'gmix_max_ei', 'gmix_agg_lcb', 'gmix_max_lcb']\n",
    "\n",
    "    return utilities_gMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_eval_utility(ds, surrogate_predict_fn, surrogate_model, surrogate_utility_fn, optimal_value, col_names, direction=1, is_normalization_required=False, param_def=None, eval_complement=False):\n",
    "    if ds.ndim == 1:\n",
    "        ds = np.reshape(ds, (1, -1))\n",
    "    ds = pd.DataFrame(ds, columns=col_names)\n",
    "\n",
    "    if is_normalization_required:\n",
    "        ds = fn_normalize_min_max(ds, param_def)[0]\n",
    "\n",
    "    surrogate_predictions = fn_predict(surrogate_predict_fn, surrogate_model, ds, param_def=param_def)\n",
    "    surrogate_traits = fn_assemble_prediction_traits(surrogate_predictions)\n",
    "    s_utility = surrogate_utility_fn(traits=surrogate_traits, optimal_response=optimal_value, complement_u=eval_complement)\n",
    "    \n",
    "    s_utility = np.array(s_utility, dtype=float) * direction\n",
    "\n",
    "    return s_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that generates new points in space (parameter vector)\n",
    "# Params:\n",
    "# - type: function name to be called\n",
    "# - params: tible with parameters\n",
    "# - omit: vector of params to be omitted and use current_value\n",
    "# - variation.rate: percentage of attributes to be changed\n",
    "\n",
    "def fn_acquisition(type, param_space, *args, **kwargs):\n",
    "    ## If space has no records, it is initial parameter generation, hence need completely random sampling\n",
    "    # if nrow(param_space$space) == 0:\n",
    "    #     return fn_acquisition_random_share(param_space, ...)\n",
    "    \n",
    "    return type(param_space, *args, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_sampling_lhs(vals=None, param_space=None, sample_size=None):\n",
    "    dim_names = param_space['definition']['parameter']\n",
    "    sample_dim = param_space['definition'].shape[0]\n",
    "    \n",
    "    prior_sample = lhs(sample_dim, samples=sample_size)  # or improvedLHS()\n",
    "    prior_sample = np.asarray(prior_sample)\n",
    "    prior_sample = prior_sample * (param_space['definition']['max'].values - param_space['definition']['min'].values) + param_space['definition']['min'].values\n",
    "    prior_sample = dict(zip(dim_names, prior_sample.T))\n",
    "    \n",
    "    return prior_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fn_sampling_normal_optimal(vals, param_space, sample_size, **kwargs):\n",
    "    dim_names = param_space['definition']['parameter']\n",
    "    sample_dim = param_space['definition'].shape[0]\n",
    "    sampling_intensity = kwargs.get('MUTATION_RATE', 0.5)\n",
    "    sampling_deviation = kwargs.get('MUTATION_SD', 1)\n",
    "  \n",
    "    if 'OPTIMISATION_CORES' in kwargs and kwargs['OPTIMISATION_CORES'] > 1:\n",
    "        with Pool(kwargs['OPTIMISATION_CORES']) as p:\n",
    "            genereted_sample = np.hstack(p.starmap(generate_sample, [(i, param_space['definition'], sample_size, sampling_intensity, sampling_deviation, vals) for i in range(sample_dim)]))\n",
    "    else:\n",
    "        genereted_sample = np.hstack([generate_sample(i, param_space['definition'], sample_size, sampling_intensity, sampling_deviation, vals) for i in range(sample_dim)])\n",
    "  \n",
    "    genereted_sample = genereted_sample.transpose()\n",
    "    genereted_sample = dict(zip(dim_names, genereted_sample))\n",
    "    \n",
    "    return genereted_sample\n",
    "\n",
    "def generate_sample(c_i, p_defs, s_size, s_intensity, s_sd, o_values):\n",
    "    p_def = p_defs.iloc[c_i]\n",
    "    p_lower_bound = p_def['lower_limit']\n",
    "    p_upper_bound = p_def['upper_limit']\n",
    "    p_replaced_size = int(np.ceil(s_size * s_intensity))\n",
    "    p_current = o_values[np.random.randint(o_values.shape[0]), c_i] # In case currently most optimal sets contains more than one sample set, then sample randomly one of them.\n",
    "    p_sample = np.full(s_size, p_current)\n",
    "    p_sample_replace = np.random.choice(s_size, p_replaced_size, replace=False)\n",
    "  \n",
    "    #p_sample[p_sample_replace] = np.random.uniform(p_lower_bound, p_upper_bound, p_replaced_size) # UNIFORMLY - SAMPLING\n",
    "    p_sample[p_sample_replace] = np.random.normal(p_current, s_sd, p_replaced_size) # NORMAL AROUND THE OPTIMAL - SAMPLING\n",
    "      \n",
    "    return p_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimisation_uniform_prior(vals, sampling_fn, surrogate_predict_fn, surrogate_model, surrogate_utility_fn, iteration_space, param_range, param_space, omit, target, sample_size=1, variation_rate=0.3, opt_parallel=True, opt_cores=cpu_count()-2, opt_store_intermediate=True, is_normalization_required=False, eval_complement=False, **kwargs):\n",
    "    \n",
    "    prior_sample = sampling_fn(vals, param_space, sample_size, **kwargs)\n",
    "    prior_names = list(prior_sample.columns)\n",
    "    \n",
    "    if opt_parallel:\n",
    "        sample_chunks = np.array_split(prior_sample, opt_cores)\n",
    "        \n",
    "        with Pool(opt_cores) as pool:\n",
    "            eval_targets = pool.map(lambda s_chunk: fn_eval_utility(\n",
    "                s_chunk,\n",
    "                surrogate_predict_fn=surrogate_predict_fn,\n",
    "                surrogate_model=surrogate_model,\n",
    "                surrogate_utility_fn=surrogate_utility_fn,\n",
    "                optimal_value=iteration_space['optimal']['value'],\n",
    "                col_names=list(vals.keys()),\n",
    "                is_normalization_required=is_normalization_required,\n",
    "                param_def=param_space['definition'],\n",
    "                eval_complement=eval_complement\n",
    "            ), sample_chunks)\n",
    "        \n",
    "        eval_target = pd.concat(eval_targets, axis=0)\n",
    "    else:\n",
    "        eval_target = fn_eval_utility(\n",
    "            prior_sample,\n",
    "            surrogate_predict_fn=surrogate_predict_fn,\n",
    "            surrogate_model=surrogate_model,\n",
    "            surrogate_utility_fn=surrogate_utility_fn,\n",
    "            optimal_value=iteration_space['optimal']['value'],\n",
    "            col_names=list(vals.keys()),\n",
    "            is_normalization_required=is_normalization_required,\n",
    "            param_def=param_space['definition'],\n",
    "            eval_complement=eval_complement\n",
    "        )\n",
    "    \n",
    "    prior_sample[target] = eval_target.iloc[:, 0].values\n",
    "    \n",
    "    if eval_complement and eval_target.shape[1] > 1:\n",
    "        prior_sample[eval_target.columns] = eval_target.values\n",
    "    \n",
    "    best_val_idx = prior_sample[target].idxmax()\n",
    "    optimal_set = prior_sample.drop(target, axis=1).iloc[best_val_idx]\n",
    "    \n",
    "    output_obj = {}\n",
    "    output_obj['space'] = optimal_set.to_frame().T\n",
    "    output_obj['space'].columns = list(vals.keys())\n",
    "    output_obj['utility'] = prior_sample[target].iloc[best_val_idx]\n",
    "    \n",
    "    if opt_store_intermediate or eval_complement:\n",
    "        output_obj['pop'] = prior_sample\n",
    "        output_obj['pop_utility'] = prior_sample[target]\n",
    "        \n",
    "        if eval_complement:\n",
    "            output_obj['complementary_utility_fn'] = eval_target.columns.drop([target] + prior_names)\n",
    "    \n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimisation_quasi_newthon(\n",
    "    vals, \n",
    "    sampling_fn, \n",
    "    surrogate_predict_fn, \n",
    "    surrogate_model, \n",
    "    surrogate_utility_fn, \n",
    "    iteration_space, \n",
    "    param_range, \n",
    "    param_space, \n",
    "    sample_size=1, \n",
    "    opt_parallel=True, \n",
    "    opt_cores=cpu_count()-2, \n",
    "    opt_best_init=False, \n",
    "    opt_init_pop_shufle_rate=0.1, \n",
    "    opt_store_intermediate=False, \n",
    "    is_normalization_required=False,\n",
    "    **kwargs\n",
    "):\n",
    "    enabled_idx = np.arange(len(param_space[\"definition\"]))\n",
    "    if \"enabled\" in param_space[\"definition\"]:\n",
    "        enabled_idx = np.where(param_space[\"definition\"][\"enabled\"] == True)[0]\n",
    "        \n",
    "    reduced_vals = vals[enabled_idx]\n",
    "    reduced_param_space = {\n",
    "        \"space\": param_space[\"space\"][:, np.concatenate([enabled_idx, [-1]])],\n",
    "        \"definition\": param_space[\"definition\"][enabled_idx]\n",
    "    }\n",
    "    \n",
    "    lower_bound = reduced_param_space[\"definition\"][\"lower_limit\"]\n",
    "    upper_bound = reduced_param_space[\"definition\"][\"upper_limit\"]\n",
    "    \n",
    "    def singleton_optimisation(idx, init_grid):\n",
    "        optimal_set = minimize(\n",
    "            fun=fn_eval_utility, \n",
    "            x0=init_grid[idx],\n",
    "            args=(surrogate_predict_fn, surrogate_model, surrogate_utility_fn, iteration_space[\"optimal\"][\"value\"], list(reduced_vals), is_normalization_required, reduced_param_space[\"definition\"]),\n",
    "            method=\"L-BFGS-B\",\n",
    "            bounds=list(zip(lower_bound, upper_bound)),\n",
    "            options={\"maxiter\": 1000},\n",
    "        )\n",
    "        return optimal_set\n",
    "    \n",
    "    initGrid = sampling_fn(reduced_vals, reduced_param_space, sample_size, **kwargs)\n",
    "    \n",
    "    if opt_parallel:\n",
    "        with Pool(processes=opt_cores) as pool:\n",
    "            optimal_sets = pool.starmap(singleton_optimisation, [(idx, initGrid) for idx in range(sample_size)])\n",
    "    else:\n",
    "        optimal_sets = [singleton_optimisation(idx, initGrid) for idx in range(sample_size)]\n",
    "    \n",
    "    optimal_sets = [opt_set for opt_set in optimal_sets if opt_set.success]\n",
    "    best_vals = np.array([opt_set.fun for opt_set in optimal_sets])\n",
    "    best_val_idx = np.where(best_vals == best_vals.max())[0]\n",
    "    \n",
    "    if len(best_val_idx) > 1:\n",
    "        best_val_idx = best_val_idx[0]\n",
    "    \n",
    "    optimal_set = optimal_sets[best_val_idx]\n",
    "    \n",
    "    output_obj = {\n",
    "        \"space\": np.concatenate([param_space[\"definition\"][\"initial\"][enabled_idx], optimal_set.x]),\n",
    "        \"utility\": optimal_set.fun\n",
    "    }\n",
    "    \n",
    "    output_obj[\"space\"] = output_obj[\"space\"].reshape((1, -1))\n",
    "    output_obj[\"space\"] = np.tile(output_obj[\"space\"], (sample_size, 1))\n",
    "    output_obj[\"space\"][:, enabled_idx] = np.tile(optimal_set.x, (sample_size, 1))\n",
    "    \n",
    "    if opt_store_intermediate:\n",
    "        output_obj[\"pop\"] = np.array([opt_set.x for opt_set in optimal_sets])\n",
    "        output_obj[\"pop_utility\"] = best_vals\n",
    "    \n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimisation_differential_evolution(vals, surrogate_predict_fn, surrogate_model, surrogate_utility_fn, iteration_space, param_range, param_space, sample_size = 1, opt_parallel = True, opt_cores = multiprocessing.cpu_count()-2, opt_best_init = False, opt_init_pop_shufle_rate = 0.1, opt_store_intermediate = False, is_normalization_required = False, **kwargs):\n",
    "    \n",
    "    enabled_idx = np.arange(param_space['definition'].shape[0])\n",
    "    if 'enabled' in param_space['definition']:\n",
    "        enabled_idx = np.where(param_space['definition'][:, 'enabled'] == True)[0]\n",
    "    \n",
    "    reduced_vals = vals[enabled_idx]\n",
    "    reduced_param_space = {\n",
    "        'space': param_space['space'][:, np.concatenate([enabled_idx, [-1]])],\n",
    "        'definition': param_space['definition'][enabled_idx]\n",
    "    }\n",
    "    \n",
    "    lower_bound = reduced_param_space['definition']['lower_limit']\n",
    "    upper_bound = reduced_param_space['definition']['upper_limit']\n",
    "    \n",
    "    arg_params = {\n",
    "        'strategy': 6, \n",
    "        'p': 0.01, \n",
    "        'F': 0.8, \n",
    "        'c': 0.8, \n",
    "        'itermax': 1000, \n",
    "        'NP': 10*len(enabled_idx), \n",
    "        'trace': False\n",
    "    }\n",
    "    \n",
    "    if opt_store_intermediate:\n",
    "        arg_params['storepopfrom'] = 1\n",
    "        arg_params['storepopfreq'] = 10\n",
    "    \n",
    "    optimal_sets = []\n",
    "    for x in range(sample_size):\n",
    "        if opt_parallel:\n",
    "            cl = multiprocessing.Pool(processes=opt_cores)\n",
    "            arg_params['parallelType'] = 1\n",
    "            arg_params['cluster'] = cl\n",
    "            cl.map(lambda fn: __import__(fn), ['numpy', 'pandas', 'random', 'scipy'])\n",
    "            cl.map(lambda pkg: __import__(pkg), ['tidyr', 'dplyr', 'ggplot2', 'reshape2', 'purrr', 'GPfit', 'randomForest', 'mixtools', 'mclust', 'NbClust', 'mlegp', 'ranger'])\n",
    "        \n",
    "        opt_output = differential_evolution(\n",
    "            fn_eval_utility, \n",
    "            bounds=list(zip(lower_bound, upper_bound)),\n",
    "            strategy=arg_params['strategy'], \n",
    "            popsize=arg_params['NP'], \n",
    "            tol=arg_params['p'], \n",
    "            mutation=arg_params['F'], \n",
    "            recombination=arg_params['c'], \n",
    "            maxiter=arg_params['itermax'], \n",
    "            args=(\n",
    "                surrogate_predict_fn, \n",
    "                surrogate_model, \n",
    "                surrogate_utility_fn,\n",
    "                iteration_space['optimal']['value'],\n",
    "                reduced_vals,\n",
    "                is_normalization_required,\n",
    "                reduced_param_space['definition'],\n",
    "                -1\n",
    "            )\n",
    "        )\n",
    "        if opt_parallel:\n",
    "            cl.close()\n",
    "        \n",
    "        optimal_sets.append(opt_output)\n",
    "    \n",
    "    best_vals = np.array([opt['fun'] for opt in optimal_sets])\n",
    "    best_val_idx = np.where(best_vals == np.max(best_vals))[0]\n",
    "    \n",
    "    if len(best_val_idx) > 1:\n",
    "        best_val_idx = best_val_idx[0]\n",
    "    optimal_set = optimal_sets[best_val_idx]\n",
    "\n",
    "    output_obj = {}\n",
    "    output_obj['space'] = list(param_space['definition'][:, 'initial'])\n",
    "    output_obj['space'][enabled_idx] = optimal_set['optim']['bestmem']\n",
    "    output_obj['space'] = np.matrix(output_obj['space'], nrow=1)\n",
    "    output_obj['space'].columns = list(vals.keys())\n",
    "\n",
    "    if opt_store_intermediate:\n",
    "        output_obj['pop'] = optimal_set['member']['storepop']\n",
    "\n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_reference_sample(sps, ref_type=0, target=None, **kwargs):\n",
    "    assert ref_type in [0, 1], \"Chosen type of a reference sample point is not correct. Allowed values are either 0 ('optimal') or 1 ('last').\"\n",
    "\n",
    "    if sps.shape[0] > 0:\n",
    "        if ref_type == 0:\n",
    "            return sps.loc[sps[target].idxmin()]\n",
    "        elif ref_type == 1:\n",
    "            return sps.tail(1)\n",
    "\n",
    "    return None\n",
    "\n",
    "# assertthat package is not imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimal_acquisition(param_space, iter_space = None, sample_size = 1, omit = [], inner_optimisation_fn = fn_optimisation_uniform_prior, sampling_fn = fn_sampling_lhs, surrogate_predict_fn = None, surrogate_utility_fn = None, **kwargs):\n",
    "    assert callable(inner_optimisation_fn), \"Provided optimisation function is not a valid function!\"\n",
    "    assert callable(sampling_fn), \"Provided sampling function is not a valid function!\"\n",
    "\n",
    "    sps = param_space['space']\n",
    "    # Bounds (min and max) are taken by default from the first parameter\n",
    "    if param_space['definition'].shape[0] > 0:\n",
    "        lower_limit = float(param_space['definition'].loc[\"lower_limit\"])\n",
    "        upper_limit = float(param_space['definition'].loc[\"upper_limit\"])\n",
    "    else:\n",
    "        lower_limit = -10\n",
    "        upper_limit = 10\n",
    "    \n",
    "    # Get reference sample point for future mutation\n",
    "    ref_param_instance = fn_reference_sample(sps, ref_type = 0, **kwargs)\n",
    "\n",
    "    # Omit any not necessary parameters\n",
    "    if omit:\n",
    "        ref_param_instance = ref_param_instance.drop(columns=omit)\n",
    "\n",
    "    optimal_ps = inner_optimisation_fn(ref_param_instance, \n",
    "                                       sampling_fn = sampling_fn,\n",
    "                                       surrogate_predict_fn = surrogate_predict_fn, \n",
    "                                       surrogate_model = iter_space['surrogate']['model'], \n",
    "                                       surrogate_utility_fn = surrogate_utility_fn, \n",
    "                                       iteration_space = iter_space, \n",
    "                                       param_range = {\"lower_limit\": lower_limit, \"upper_limit\": upper_limit},\n",
    "                                       param_space = param_space,\n",
    "                                       sample_size = sample_size,\n",
    "                                       omit = omit,\n",
    "                                       **kwargs)\n",
    "    return optimal_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_enforce_bounds(value, param, param_def):\n",
    "    pps = param_def[param_def['parameter'] == param]\n",
    "    \n",
    "    if len(pps) == 0:\n",
    "        return value\n",
    "    if pps.iloc[0]['lower_limit'] > value:\n",
    "        return pps.iloc[0]['lower_limit']\n",
    "    if pps.iloc[0]['upper_limit'] < value:\n",
    "        return pps.iloc[0]['upper_limit']\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fn_normalize_min_max(ds, param_def):\n",
    "    if ds.ndim == 1:\n",
    "        ds = pd.DataFrame(ds).T\n",
    "        \n",
    "    param_mat = param_def[['parameter', 'lower_limit', 'upper_limit']].set_index('parameter')\n",
    "    params_used = list(set(param_mat.index) & set(ds.columns))\n",
    "    params_omitted = list(set(ds.columns) - set(params_used))\n",
    "    \n",
    "    input_space_cols = []\n",
    "    for col_name in params_used:\n",
    "        col = ds[col_name]\n",
    "        col_min = param_mat.at[col_name, 'lower_limit']\n",
    "        col_max = param_mat.at[col_name, 'upper_limit']\n",
    "        input_space_cols.append((col - col_min) / (col_max - col_min))\n",
    "    \n",
    "    input_space = pd.concat(input_space_cols, axis=1)\n",
    "    input_space.columns = params_used\n",
    "    \n",
    "    if len(params_omitted) > 0:\n",
    "        ds_omitted = ds[params_omitted]\n",
    "        input_space = pd.concat([input_space, ds_omitted], axis=1)\n",
    "        \n",
    "    return input_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_scaleup_standard(ds, param_def):\n",
    "    if ds.ndim == 1:\n",
    "        ds = pd.DataFrame(ds).T\n",
    "        ds.columns = param_def['parameter']\n",
    "    param_mat = param_def[['parameter', 'lower_limit', 'upper_limit']].set_index('parameter')\n",
    "    params_used = list(set(ds.columns).intersection(set(param_mat.index)))\n",
    "    params_omitted = list(set(ds.columns).difference(set(params_used)))\n",
    "    \n",
    "    input_space_cols = []\n",
    "    for col_name in params_used:\n",
    "        l_lower = param_mat.loc[col_name, 'lower_limit']\n",
    "        l_upper = param_mat.loc[col_name, 'upper_limit']\n",
    "        l_range = l_upper - l_lower\n",
    "        input_space_cols.append(((ds[col_name] * l_range) + l_lower).values)\n",
    "        \n",
    "    i_space = np.column_stack(input_space_cols)\n",
    "    colnames = params_used\n",
    "    if len(params_omitted) > 0:\n",
    "        i_space = np.column_stack((i_space, ds[params_omitted].values))\n",
    "        colnames += params_omitted\n",
    "    i_space = pd.DataFrame(i_space, columns=colnames)\n",
    "    \n",
    "    return i_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit GP model\n",
    "# Params:\n",
    "# - ds: dataset (tibble)\n",
    "# - target: target variable name\n",
    "\n",
    "def fn_fit_gp(ds, target, method_params = {'KERNEL': 'exponential', 'POWER': 1.95}, test_ds = None):\n",
    "    design_mat = ds.drop(target, axis=1).values\n",
    "    \n",
    "    model = GPy.models.GPRegression(\n",
    "        X=design_mat,\n",
    "        Y=ds[target].values.reshape(-1, 1),\n",
    "        kernel=GPy.kern.RBF(input_dim=design_mat.shape[1], ARD=True, lengthscale=method_params['POWER'])\n",
    "    )\n",
    "    \n",
    "    model.optimize()\n",
    "    \n",
    "    output = {'model': model}\n",
    "    \n",
    "    if test_ds is not None:\n",
    "        test_ds_output = test_ds[target].values\n",
    "        y_pred, _ = model.predict(test_ds.drop(target, axis=1).values)\n",
    "        output['performance'] = np.sqrt(np.mean(np.square(y_pred - test_ds_output)))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyGPs import mlegp\n",
    "\n",
    "# def fn_fit_mlegp(ds, target, method_params={}, test_ds=None):\n",
    "#     output = {\n",
    "#         'model': mlegp(X=ds.drop(columns=[target]).values, Y=ds[target].values, **method_params)\n",
    "#     }\n",
    "#     if test_ds is not None:\n",
    "#         test_ds_output = test_ds[target].values\n",
    "#         y_pred = output['model'].predict(test_ds.drop(columns=[target]).values)[0]\n",
    "#         output['performance'] = RMSE(y_pred=y_pred, y_true=test_ds_output)  \n",
    "#     return output\n",
    "\n",
    "# from pyGPs import mlegp\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# def fn_fit_mlegp(ds, target, method_params={}, test_ds=None):\n",
    "#     output = {'model': mlegp(X=ds.drop(columns=target).values, Y=ds[target].values, **method_params)}    \n",
    "#     if test_ds is not None:\n",
    "#         y_true = test_ds[target].values\n",
    "#         y_pred = output['model'].predict(test_ds.drop(columns=target).values)[0]\n",
    "#         output['performance'] = mean_squared_error(y_true, y_pred, squared=False)   \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit GP model (with MLEGP library)\n",
    "# Params:\n",
    "# - ds: dataset (tibble)          R\n",
    "# - target: target variable name\n",
    "# \ta dataset ds             Python\n",
    "# \tthe name of the target variable target\n",
    "# \toptional parameters for the Gaussian process model method_params\n",
    "\n",
    "\n",
    "def fn_fit_mlegp(ds, target, method_params = {}, test_ds = None):\n",
    "    X = ds.loc[:, ds.columns != target].values\n",
    "    y = ds[target].values.reshape(-1,1)\n",
    "    kernel = GPy.kern.RBF(input_dim=X.shape[1], ARD=True)  # Radial basis function kernel with Automatic Relevance Determination\n",
    "    model = GPy.models.GPRegression(X, y, kernel)\n",
    "    model.optimize()\n",
    "    output = {'model': model}\n",
    "    \n",
    "    if test_ds is not None:\n",
    "        test_X = test_ds.loc[:, test_ds.columns != target].values\n",
    "        test_y = test_ds[target].values.reshape(-1,1)\n",
    "        y_pred = model.predict(test_X)[0]\n",
    "        output['performance'] = np.sqrt(((y_pred - test_y) ** 2).mean())  ### May need to delet np.sqrt()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit Random Forest model\n",
    "# Params:\n",
    "# ds: a Pandas DataFrame containing the training data.      \n",
    "# target: a string representing the name of the target variable in the training data.\n",
    "# method_params: a dictionary containing hyperparameters for the Random Forest model. The default values are RF_NSIZE=1 (minimum number of samples required to split an internal node), RF_NTREE=500 (number of trees in the forest), and RF_MTRY (number of features to consider when looking for the best split at each node), which is set to the square root of the number of features by default.\n",
    "# test_ds: a Pandas DataFrame containing the test data. This argument is optional.\n",
    "# keep_inbag: a boolean indicating whether to compute and store the \"in-bag\" score (i.e., the prediction error on the training set).\n",
    "\n",
    "def fn_fit_rf(ds, target, method_params={\"RF_NSIZE\": 1, \"RF_NTREE\": 500}, test_ds=None, keep_inbag=True):\n",
    "    \n",
    "    if method_params.get(\"RF_MTRY\") is None:\n",
    "        method_params[\"RF_MTRY\"] = int(np.sqrt(ds.shape[1] - 1))\n",
    "    \n",
    "    X = ds.drop(target, axis=1).values\n",
    "    y = ds[target].values\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=method_params[\"RF_NTREE\"], \n",
    "                                   min_samples_leaf=method_params[\"RF_NSIZE\"], \n",
    "                                   max_features=method_params[\"RF_MTRY\"], \n",
    "                                   oob_score=keep_inbag)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    output = {\"model\": model}\n",
    "    \n",
    "    if test_ds is not None:\n",
    "        y_true = test_ds[target].values\n",
    "        X_test = test_ds.drop(target, axis=1).values\n",
    "        y_pred = model.predict(X_test)\n",
    "        output[\"performance\"] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit Random Forest model (ranger package)\n",
    "# Params:\n",
    "# dataset (ds) \n",
    "# target variable (target)\n",
    "# optional arguments for method_params\n",
    "# test_ds \n",
    "# keep_inbag\n",
    "\n",
    "def fn_fit_rf_ranger(ds, target, method_params={\"RF_NSIZE\": 1, \"RF_NTREE\": 500}, test_ds=None, keep_inbag=True):\n",
    "    \n",
    "    if method_params.get(\"RF_MTRY\") is None:\n",
    "        method_params[\"RF_MTRY\"] = int(np.sqrt(ds.shape[1] - 1))\n",
    "    \n",
    "    ranger = importr(\"ranger\")\n",
    "    \n",
    "    formula = Formula(f\"{target} ~ .\")\n",
    "    model = ranger.ranger(formula=formula, data=ds, num_trees=method_params[\"RF_NTREE\"],\n",
    "                          mtry=method_params[\"RF_MTRY\"], min_node_size=method_params[\"RF_NSIZE\"],\n",
    "                          keep_inbag=keep_inbag)\n",
    "    \n",
    "    output = {\"model\": model}\n",
    "    \n",
    "    if test_ds is not None:\n",
    "        y_true = test_ds[target].values\n",
    "        X_test = test_ds.drop(target, axis=1).values\n",
    "        y_pred = ranger.predict(model, X_test)\n",
    "        output[\"performance\"] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for creating folds for cross validation\n",
    "## Params:\n",
    "# ds: the dataset to perform cross-validation on\n",
    "# folds: the number of folds to use (default is 10)\n",
    "# loo: whether to use leave-one-out cross-validation (default is False)\n",
    "\n",
    "\n",
    "def fn_do_folding(ds, folds=10, loo=False):\n",
    "    if nrow(ds) <= folds:\n",
    "        folds = nrow(ds)\n",
    "        loo = True\n",
    "    \n",
    "    test_ds_idx = []\n",
    "    \n",
    "    if loo:\n",
    "        test_ds_idx = [list(x) for x in np.random.choice(range(nrow(ds)), size=folds, replace=False)]\n",
    "    else:\n",
    "        fold_size = nrow(ds) // folds\n",
    "        test_ds_idx = [list(x) for x in np.random.choice(range(nrow(ds)), size=fold_size, replace=True) for i in range(folds)]\n",
    "    \n",
    "    return [ {\n",
    "        'train': ds.iloc[np.setdiff1d(range(nrow(ds)), idx), :],\n",
    "        'test': ds.iloc[idx, :]\n",
    "    } for idx in test_ds_idx ]\n",
    "\n",
    "## numpy's random.choice function is used to generate the indices of the test sets. \n",
    "#  ds must be a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to fit a model\n",
    "# Params:\n",
    "# - ds: dataset (tibble)\n",
    "# - target: target variable name\n",
    "\n",
    "\n",
    "# if it doesn t work -> import multiprocessing as mp. I change it cause of imports ... its used elsewhere with the name multiprocessing\n",
    "\n",
    "def fn_fit(type, ds, target, param_def, is_normalization_required=False, method_params=np.nan, cv_parallel=True, cv_cores=multiprocessing.cpu_count() - 2, *args, **kwargs):\n",
    "    col_idx_target = list(ds.columns).index(target)\n",
    "\n",
    "    # Apply filtering in accordance to the enabled feature\n",
    "    if \"enabled\" in param_def.columns:\n",
    "        enabled_idx = list(np.where(param_def[\"enabled\"] == True)[0]) + [col_idx_target]\n",
    "        ds = ds.iloc[:, enabled_idx]\n",
    "        col_idx_target = len(enabled_idx) - 1\n",
    "\n",
    "    # Some methods require normalization\n",
    "    if is_normalization_required:\n",
    "        ds = fn_normalize_min_max(ds, param_def)\n",
    "\n",
    "    if not np.all(np.isnan(method_params)):\n",
    "        method_params_expanded = pd.DataFrame(list(product(*method_params)))\n",
    "        \n",
    "        if len(method_params_expanded) > 1:\n",
    "            # Create cross-validation folds\n",
    "            cv_datasets = fn_do_folding(ds)\n",
    "            \n",
    "            method_params_performance = []\n",
    "            \n",
    "            for conf_idx in range(len(method_params_expanded)):\n",
    "                method_conf = dict(zip(method_params_expanded.columns, method_params_expanded.iloc[conf_idx]))\n",
    "                \n",
    "                if cv_parallel:\n",
    "                    with multiprocessing.Pool(processes=cv_cores) as pool:\n",
    "                        folds_model_performance = pool.starmap(\n",
    "                            lambda f_ds, m_conf, t, l_fit_fun: l_fit_fun(f_ds[\"train\"], target=t, method_params=m_conf, test_ds=f_ds[\"test\"])[\"performance\"],\n",
    "                            [(f_ds, method_conf, target, type) for f_ds in cv_datasets]\n",
    "                        )\n",
    "                else:\n",
    "                    folds_model_performance = [\n",
    "                        type(f_ds[\"train\"], target=target, method_params=method_conf, test_ds=f_ds[\"test\"])[\"performance\"]\n",
    "                        for f_ds in cv_datasets\n",
    "                    ]\n",
    "                \n",
    "                mean_performance = np.mean(folds_model_performance, axis=0)\n",
    "                partial_performance = np.array(folds_model_performance)\n",
    "                method_params_performance.append({\"mean_performance\": mean_performance, \"partial_performance\": partial_performance})\n",
    "            \n",
    "            # Retrieve performance of models built on different folds, find the best, and retrain to whole dataset\n",
    "            mean_performance = np.array([x[\"mean_performance\"] for x in method_params_performance])\n",
    "            best_performer_idx = np.argmin(mean_performance, axis=0)[0]\n",
    "            method_conf = dict(zip(method_params_expanded.columns, method_params_expanded.iloc[best_performer_idx]))\n",
    "            \n",
    "            list_models = [\n",
    "                {\n",
    "                    \"model\": type(ds.iloc[:, list(sub_ds_idx) + [col_idx_target]], target=target, method_params=method_conf, *args, **kwargs)[\"model\"],\n",
    "                    \"sub_ds_idx\": sub_ds_idx\n",
    "                }\n",
    "                for sub_ds_idx in fn_split_space(ds)\n",
    "            ]\n",
    "            \n",
    "            return list_models\n",
    "        \n",
    "        # Not enough params combinations for tuning\n",
    "        list_models = [\n",
    "            {\n",
    "                \"model\": type(ds.iloc[:, list(sub_ds_idx) + [col_idx_target]], target=target, method_params=method_params, *args, **kwargs)[\"model\"],\n",
    "                \"sub_ds_idx\": sub_ds_idx\n",
    "            }\n",
    "            for sub_ds_idx in fn_split_space(ds)\n",
    "        ]\n",
    "        \n",
    "        return list_models\n",
    "\n",
    "    list_models = [\n",
    "        {\n",
    "            \"model\": type(ds.iloc[:, list(sub_ds_idx) + [col_idx_target]], target=target, *args, **kwargs)[\"model\"],\n",
    "            \"sub_ds_idx\": sub_ds_idx\n",
    "        }\n",
    "        for sub_ds_idx in fn_split_space(ds)\n",
    "    ]\n",
    "\n",
    "    return list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for splitting the space into set of subspaces for learning/fitting many models\n",
    "\n",
    "def fn_split_space(ds):\n",
    "    no_subspaces = 1\n",
    "    if \"SUBSPACES_NUMBER\" in shared.env[\"settings\"]:\n",
    "        no_subspaces = shared.env[\"settings\"][\"SUBSPACES_NUMBER\"]\n",
    "\n",
    "    size_subspace = np.ceil((ds.shape[1]-1)/no_subspaces)\n",
    "    if \"SUBSPACES_SIZE\" in shared.env[\"settings\"]:\n",
    "        size_subspace = np.ceil((ds.shape[1]-1)*shared.env[\"settings\"][\"SUBSPACES_SIZE\"])\n",
    "\n",
    "    overlap_subspaces = False\n",
    "    if \"SUBSPACES_OVERLAP\" in shared.env[\"settings\"]:\n",
    "        overlap_subspaces = shared.env[\"settings\"][\"SUBSPACES_OVERLAP\"]\n",
    "    if (ds.shape[1]-1) < (no_subspaces * size_subspace):\n",
    "        overlap_subspaces = True\n",
    "\n",
    "    output = []\n",
    "    if overlap_subspaces:\n",
    "        for idx in range(no_subspaces):\n",
    "            col_idx = np.sort(np.random.choice(ds.shape[1]-1, size_subspace, replace=False))\n",
    "            output.append(col_idx)\n",
    "    else:\n",
    "        col_set = np.arange(ds.shape[1]-1)\n",
    "        for i in range(no_subspaces):\n",
    "            gen_vector = np.sort(np.random.choice(col_set, size_subspace, replace=False))\n",
    "            output.append(gen_vector)\n",
    "            col_set = np.setdiff1d(col_set, gen_vector)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to predict using GP model\n",
    "# Params:\n",
    "# - surrogate_model: model fitted with fn_fit function\n",
    "# - ds: dataset (tibble)\n",
    "\n",
    "def fn_predict_gp(surrogate_model, ds):\n",
    "    pred = surrogate_model.predict(ds)\n",
    "    \n",
    "    return {\n",
    "        'pred': pred,\n",
    "        'mu': pred.mean(),\n",
    "        'sigma': np.sqrt(pred.var())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to predict using GP model (MLEGP Library)\n",
    "# Params:\n",
    "# - surrogate_model: model fitted with fn_fit function\n",
    "# - ds: dataset (tibble)\n",
    "\n",
    "# def fn_predict_mlegp(surrogate_model, ds):\n",
    "#     pred = predict.gp(surrogate_model, newdata = ds, se.fit = True)\n",
    "#     return {\n",
    "#         \"pred\": pred,\n",
    "#         \"mu\": pred[0],\n",
    "#         \"sigma\": pred[1]\n",
    "#     }\n",
    "\n",
    "def fn_predict_mlegp(surrogate_model, ds):\n",
    "    pred = predict.gp(surrogate_model, newdata=ds, se_fit=True)\n",
    "    return {\n",
    "        \"pred\": pred,\n",
    "        \"mu\": pred[0],\n",
    "        \"sigma\": pred[1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to predict using Random Forest model\n",
    "# Params:\n",
    "# - surrogate_model: model fitted with fn_fit function\n",
    "# - ds: dataset (tibble)\n",
    "\n",
    "\n",
    "#predict.all=True   ->individual predictions for all trees in the forest\n",
    "\n",
    "def fn_predict_rf(surrogate_model, ds):\n",
    "    pred = predict(surrogate_model, newdata=ds, predict_all=True)\n",
    "    return {\n",
    "        \"pred\": pred[\"individual\"],\n",
    "        \"mu\": pred[\"aggregate\"],\n",
    "        \"sigma\": np.apply_along_axis(np.std, axis=1, arr=pred[\"individual\"]),\n",
    "        \"inbag\": surrogate_model[\"inbag\"]\n",
    "    }\n",
    "\n",
    "# def fn_predict_rf(surrogate_model, ds):\n",
    "#     pred = surrogate_model.predict(ds, predict_all=True)\n",
    "    \n",
    "#     return {\n",
    "#         \"pred\": pred[\"individual\"],\n",
    "#         \"mu\": pred[\"aggregate\"],\n",
    "#         \"sigma\": np.apply_along_axis(np.std, 1, pred[\"individual\"]),\n",
    "#         \"inbag\": surrogate_model.inbag\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to predict using a model\n",
    "# Params:\n",
    "# - type: mode type - predict function\n",
    "# - surrogate_model: model fitted with fn_fit function (list of surrogates)\n",
    "# - ds: dataset (tibble)\n",
    "\n",
    "def fn_predict(type, surrogate_model, ds, param_def=None, is_normalization_required=False, **kwargs):\n",
    "    # Apply filtering in accordance to the enabled feature\n",
    "    if param_def is not None and \"enabled\" in param_def.columns:\n",
    "        enabled_idx = param_def.index[param_def[\"enabled\"]]\n",
    "        ds = ds.iloc[:, enabled_idx]\n",
    "\n",
    "    if is_normalization_required and param_def is not None:\n",
    "        ds = fn_normalize_min_max(ds, param_def)\n",
    "\n",
    "    # Surrogate model object contains list of lists [surrogate, indices of subspace columns]\n",
    "    list_predictions = [type(l_model_subspace[0], ds.iloc[:, l_model_subspace[1]], **kwargs) for l_model_subspace in surrogate_model]\n",
    "    output_headers = list_predictions[0].keys()\n",
    "    output = {h: [pred[h] for pred in list_predictions] for h in output_headers}\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ B OPT##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-optimize\n",
    "from skopt import gp_minimize, Optimizer\n",
    "from skopt.space import Real\n",
    "import numpy as np\n",
    "\n",
    "def fn_bayes_optimisiation(param_init, param_def, param_addons=[], simulation_fn=None, termination_fn=None, \n",
    "                           verbose_output_fn=None, save_object_fn=None, target=\"inadequacy\", surrogate_fit_fn=\"fn_fit_rf\",\n",
    "                           surrogate_predict_fn=\"fn_predict_rf\", surrogate_utility_fn=\"fn_utility_ei\", \n",
    "                           init_sampling_fn=\"fn_sampling_lhs\", sampling_fn=\"fn_sampling_lhs\", \n",
    "                           acquisition_fn=\"fn_optimal_acquisition\", selection_fn=\"fn_pull_optimal_selection\", \n",
    "                           inner_optimisation_fn=\"fn_optimisation_quasi_newthon\", init_sample_size=10, sample_size=100, \n",
    "                           potential_size=1, initialize_only=False, store_iteratively=False, **kwargs):\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    opt = Optimizer(dimensions=[Real(*bounds) for bounds in param_def.values()],\n",
    "                    base_estimator=surrogate_fit_fn, acq_func=surrogate_utility_fn)\n",
    "    \n",
    "    iteration_no = 0\n",
    "    \n",
    "    # Generate initial set of params to be simulated\n",
    "    init_x = np.array([list(param_init.values())])\n",
    "    init_y = np.array([simulation_fn(**param_init)])\n",
    "    opt.tell(init_x, init_y)\n",
    "    \n",
    "    if verbose_output_fn is not None:\n",
    "        verbose_output_fn(\"Initial sample set acquired.\", count=init_sample_size)\n",
    "    \n",
    "    while not termination_fn(iteration_no, opt):\n",
    "        # Increase iteration number\n",
    "        iteration_no += 1\n",
    "        \n",
    "        # 1. get the best performer and save its parameters\n",
    "        x_next = opt.ask()\n",
    "        y_next = simulation_fn(**dict(zip(param_init.keys(), x_next)))\n",
    "        opt.tell(x_next, y_next)\n",
    "        \n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Retrieved current most optimal sample.\", count=len(x_next))\n",
    "      \n",
    "        # Save current state of optimizer\n",
    "        if store_iteratively and save_object_fn is not None:\n",
    "            param_space = {\"space\": np.concatenate([opt.Xi, np.array(opt.yi).reshape(-1, 1)], axis=1)}\n",
    "            save_object_fn(param_space, \"param_space\", stage=f\"iter_{iteration_no}\")\n",
    "      \n",
    "    # Return final results\n",
    "    param_space = {\"space\": np.concatenate([opt.Xi, np.array(opt.yi).reshape(-1, 1)], axis=1)}\n",
    "    return param_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2695344018.py, line 99)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[57], line 99\u001b[1;36m\u001b[0m\n\u001b[1;33m    iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], ...), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], ...))\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "def fn_bayes_optimisiation(param_init, param_def, param_addons=[], simulation_fn=None, termination_fn=None, \n",
    "                           verbose_output_fn=None, save_object_fn=None, target=\"inadequacy\", surrogate_fit_fn=\"fn_fit_rf\",\n",
    "                           surrogate_predict_fn=\"fn_predict_rf\", surrogate_utility_fn=\"fn_utility_ei\", \n",
    "                           init_sampling_fn=\"fn_sampling_lhs\", sampling_fn=\"fn_sampling_lhs\", \n",
    "                           acquisition_fn=\"fn_optimal_acquisition\", selection_fn=\"fn_pull_optimal_selection\", \n",
    "                           inner_optimisation_fn=\"fn_optimisation_quasi_newthon\", init_sample_size=10, sample_size=100, \n",
    "                           potential_size=1, initialize_only=False, store_iteratively=False, **kwargs):\n",
    "\n",
    "############################### -  Part A  - ##########################\n",
    "# Get global settings object\n",
    "    if \"RETRAIN_AFTER\" not in shared.env[\"settings\"]:\n",
    "        shared.env[\"settings\"][\"RETRAIN_AFTER\"] = 1\n",
    "    settings_obj = shared.env[\"settings\"]\n",
    "\n",
    "    # Compile all functions\n",
    "    try:\n",
    "        fn_compilation = [\"surrogate_fit_fn\", \"surrogate_predict_fn\", \"surrogate_utility_fn\", \"init_sampling_fn\", \"sampling_fn\", \"acquisition_fn\", \"selection_fn\", \"termination_fn\", \"inner_optimisation_fn\"]\n",
    "        for fn in fn_compilation:\n",
    "            exec(\"%s = %s\" % (fn, eval(fn)))\n",
    "        \n",
    "        if not callable(simulation_fn):\n",
    "            simulation_fn = eval(simulation_fn)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    # Check if terminal conditions are set up\n",
    "    if termination_fn is None:\n",
    "        raise ValueError(\"Termination function is required! It must return logical value T (terminate)/F (continue). At input get the iteration number, params and inadequacy through iterations\")\n",
    "\n",
    "    iteration_no = 0\n",
    "\n",
    "    # Assemble the param space\n",
    "    param_space = {\"space\": param_init, \"definition\": param_def}\n",
    "    param_space.update(param_addons)\n",
    "\n",
    "\n",
    "    ###############################_-  Part B  -_#########################\n",
    "    if nrow(param_space['space']) == 0:\n",
    "        # Generate initial set of params to be simulated! (preferably 10 instances)\n",
    "        init_sampling_params = {\n",
    "            'type': init_sampling_fn,\n",
    "            'param_space': param_space\n",
    "        }\n",
    "        init_sampling_params = fn_resolve_fn_params(init_sampling_fn, sample_size=init_sample_size, omit=['target'], **init_sampling_params)\n",
    "        init_samples = do.call(fn_acquisition, init_sampling_params)\n",
    "        param_space['space'] = rbind(param_space['space'], init_samples)\n",
    "        \n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Initial sample set acquired.\", count=init_sample_size)\n",
    "\n",
    "        # Simulate initial set of params\n",
    "        simulation_fn_params = {\n",
    "            'param_def': param_space['definition']\n",
    "        }\n",
    "        simulation_fn_params = fn_resolve_fn_params(simulation_fn, **simulation_fn_params)\n",
    "        simulation_outcome = apply(param_space['space'], 1, simulation_fn, **simulation_fn_params)\n",
    "        simulation_outcome = t(matrix(simulation_outcome, ncol=1))\n",
    "        simulation_outcome.columns = target\n",
    "        #colnames(simulation_outcome) = target\n",
    "        param_space['space'] = cbind(param_space['space'], simulation_outcome)\n",
    "        \n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Initial sample set simulated.\")\n",
    "        \n",
    "        if save_object_fn is not None:\n",
    "            save_object_fn(param_space, \"param_space\", stage=\"initial\")\n",
    "        \n",
    "    if initialize_only:\n",
    "        return None\n",
    "\n",
    "    termination_fn_params = fn_resolve_fn_params(termination_fn, **{'space': param_space})\n",
    "\n",
    "    ###############################_-  Part C  -_#########################\n",
    "    # Perform optimization\n",
    "    while not termination_fn(iteration_no, param_space):\n",
    "        # Increase iteration number\n",
    "        iteration_no += 1\n",
    "        iteration_space = {}\n",
    "\n",
    "        try:\n",
    "            # 1. get the best performer and save its parameters\n",
    "            \n",
    "            #iteration_space['optimal'] = fn_pull_optimal_samples(space=param_space['space'], selection_fn=selection_fn, target=target, **fn_resolve_fn_params(selection_fn, _omit=[\"space\", \"selection_fn\", \"target\"], **locals()))\n",
    "            #iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, ds=param_space['space'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], ...), target=target, param_def=param_space['definition'], **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], ...))\n",
    "            #iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], ...), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], ...))\n",
    "            #iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, param_space['space'], target, param_def=param_space['definition'], *fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], **{}), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], **{}))\n",
    "            #iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, param_space['space'], target, param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], ...), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], ...))\n",
    "            #iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], **locals()), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], **locals()))\n",
    "            iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\"]), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"]))\n",
    "\n",
    "\n",
    "            #iteration_space['optimal'] = fn_pull_optimal_samples(param_space['space'], selection_fn, target, **fn_resolve_fn_params(selection_fn, omit=[\"space\", \"selection_fn\", \"target\"], ...))\n",
    "            if verbose_output_fn is not None:\n",
    "                verbose_output_fn(\"Retrieved current most optimal sample.\", count = nrow(iteration_space['optimal']['space']))\n",
    "        \n",
    "            # 2. fit surrogate model\n",
    "            if (iteration_no == 1) or (iteration_no % settings_obj['RETRAIN_AFTER'] == 0):\n",
    "                iteration_space['surrogate'] = {}\n",
    "                iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], ...), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], ...))\n",
    "                param_space['surrogate'] = iteration_space['surrogate']\n",
    "                if verbose_output_fn is not None:\n",
    "                    verbose_output_fn(\"Fitted surrogate model(s).\", training_set_size=nrow(param_space['space']))\n",
    "            else:\n",
    "                iteration_space['surrogate'] = param_space['surrogate']\n",
    "                if verbose_output_fn is not None:\n",
    "                    verbose_output_fn(\"Loaded previously trained surrogate model(s).\", training_set_size=nrow(param_space['space']))\n",
    "\n",
    "        except Exception as e:\n",
    "            # handle errors during optimization\n",
    "            if error_handling_fn is not None:\n",
    "                error_handling_fn(e, iteration_no, param_space)\n",
    "            else:\n",
    "                raise e\n",
    "            \n",
    "    ###############################_-  Part D  -_#########################\n",
    "    # 3. optimise acquisition utility\n",
    "    iteration_space[\"potential\"] = fn_acquisition(type=acquisition_fn, param_space=param_space, **fn_resolve_fn_params(acquisition_fn, {\"type\", \"param_space\"}, iter_space=iteration_space, sample_size=sample_size, omit=[target], sampling_fn=sampling_fn, surrogate_predict_fn=surrogate_predict_fn, surrogate_utility_fn=surrogate_utility_fn, inner_optimisation_fn=inner_optimisation_fn, ...))\n",
    "\n",
    "    if verbose_output_fn is not None:\n",
    "        verbose_output_fn(\"New optimal sample set acquired.\")\n",
    "\n",
    "    # 4. simulate results of the picked-up instances\n",
    "    simulation_outcome = apply(iteration_space[\"potential\"][\"space\"], 1, simulation_fn, **fn_resolve_fn_params(simulation_fn, {1}, param_def=param_space[\"definition\"], ...))\n",
    "    iteration_space[\"potential\"][\"space\"] = np.concatenate((iteration_space[\"potential\"][\"space\"], simulation_outcome), axis=1)\n",
    "    iteration_space[\"potential\"][\"space\"][:, -1] = target\n",
    "    if verbose_output_fn is not None:\n",
    "        verbose_output_fn(\"Simulation of the most promising samples is completed.\")\n",
    "\n",
    "    if \"complementary_utility_fn\" in iteration_space[\"potential\"] and len(iteration_space[\"potential\"][\"complementary_utility_fn\"]) > 0:\n",
    "        # 4b. optionally simulate results of the picked-up instances by complementary acquisition functions\n",
    "        compl_space = pd.concat([pd.DataFrame({\"select_idx\": None, \"acq\": fn_header, \"utility\": None, \"simulation\": None, \"iteration\": iteration_no}, index=[0], columns=iteration_space[\"potential\"][\"pop\"].columns.tolist() + [\"select_idx\", \"acq\", \"utility\", \"simulation\", \"iteration\"]) for fn_header in iteration_space[\"potential\"][\"complementary_utility_fn\"]])\n",
    "        for i, row in iteration_space[\"potential\"][\"pop\"].iterrows():\n",
    "            best_val_idx = row[iteration_space[\"potential\"][\"complementary_utility_fn\"]].argmax()\n",
    "            eval_set = pd.DataFrame(row[iteration_space[\"potential\"][\"pop\"].columns.tolist()]).T\n",
    "            simulation_outcome = apply(eval_set, 1, simulation_fn, **fn_resolve_fn_params(simulation_fn, {1}, param_def=param_space[\"definition\"], ...))\n",
    "            eval_set[\"select_idx\"] = best_val_idx\n",
    "            eval_set[\"acq\"] = fn_header\n",
    "            eval_set[\"utility\"] = row[fn_header]\n",
    "            eval_set[\"simulation\"] = simulation_outcome\n",
    "            eval_set[\"iteration\"] = iteration_no\n",
    "            compl_space.iloc[i] = eval_set.iloc[0]\n",
    "        \n",
    "        if \"complementary_space\" in param_space:\n",
    "            param_space[\"complementary_space\"] = pd.concat([param_space[\"complementary_space\"], compl_space])\n",
    "        else:\n",
    "            param_space[\"complementary_space\"] = compl_space\n",
    "            \n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Simulation of the most promising samples from complementary acquisition functions is completed.\") \n",
    "\n",
    "\n",
    "\n",
    "    ###############################_-  Part E  -_#########################\n",
    "    try:\n",
    "                iteration_space = simulate_fn(param_space)\n",
    "                param_space.space = pd.concat([param_space.space, pd.DataFrame(iteration_space.potential.space)])\n",
    "                if verbose_output_fn is not None:\n",
    "                    verbose_output_fn(\"Simulated results are added to the global set.\")\n",
    "                    \n",
    "                if save_object_fn is not None and store_iteratively:\n",
    "                    save_object_fn(iteration_space, \"param_space\", stage=\"optimisation\", iteration=iteration_no)\n",
    "                if save_object_fn is not None:\n",
    "                    save_object_fn(param_space, \"output_object\", stage=\"partial\")\n",
    "                if 'shared.env' in globals() and 'param_space' in globals()['shared.env']:\n",
    "                    globals()['shared.env']['param_space'] = param_space\n",
    "            \n",
    "    except Exception as err:\n",
    "        if save_object_fn is not None:\n",
    "            save_object_fn(iteration_space, \"param_space\", stage=\"failed_optimisation_iter\", iteration=iteration_no)\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(f\"ERROR: Iteration failed! iteration={iteration_no}, ex={err}\")\n",
    "        raise err\n",
    "        \n",
    "    return param_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], ...), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], ...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4188850852.py, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 82\u001b[1;36m\u001b[0m\n\u001b[1;33m    iteration_no += 1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "##################\n",
    "#######################\n",
    "def fn_bayes_optimisiation(\n",
    "        param_init,\n",
    "        param_def,\n",
    "        param_addons=None,\n",
    "        simulation_fn=None,\n",
    "        termination_fn=None,\n",
    "        verbose_output_fn=None,\n",
    "        save_object_fn=None,\n",
    "        target=\"inadequacy\",\n",
    "        surrogate_fit_fn=\"fn_fit_rf\",\n",
    "        surrogate_predict_fn=\"fn_predict_rf\",\n",
    "        surrogate_utility_fn=\"fn_utility_ei\",\n",
    "        init_sampling_fn=\"fn_sampling_lhs\",\n",
    "        sampling_fn=\"fn_sampling_lhs\",\n",
    "        acquisition_fn=\"fn_optimal_acquisition\",\n",
    "        selection_fn=\"fn_pull_optimal_selection\",\n",
    "        inner_optimisation_fn=\"fn_optimisation_quasi_newthon\",\n",
    "        init_sample_size=10,\n",
    "        sample_size=100,\n",
    "        potential_size=1,\n",
    "        initialize_only=False,\n",
    "        store_iteratively=False,\n",
    "        **kwargs):\n",
    "    # Get global settings object\n",
    "    if \"RETRAIN_AFTER\" not in shared.env[\"settings\"]:\n",
    "        shared.env[\"settings\"][\"RETRAIN_AFTER\"] = 1\n",
    "    settings_obj = shared.env[\"settings\"]\n",
    "\n",
    "    # Compile all functions\n",
    "    for fn_compilation in [\"surrogate_fit_fn\", \"surrogate_predict_fn\", \"surrogate_utility_fn\",\n",
    "                           \"init_sampling_fn\", \"sampling_fn\", \"acquisition_fn\", \"selection_fn\",\n",
    "                           \"termination_fn\", \"inner_optimisation_fn\"]:\n",
    "        try:\n",
    "            locals()[fn_compilation] = globals()[eval(fn_compilation)]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Function {fn_compilation} not found.\")\n",
    "\n",
    "    if not callable(simulation_fn):\n",
    "        simulation_fn = globals()[simulation_fn]\n",
    "\n",
    "    # Check if terminal conditions are set up\n",
    "    if termination_fn is None:\n",
    "        raise ValueError(\"Termination function is required! It must return logical value T (terminate)/F (continiue). \"\n",
    "                         \"At input get the iteration number, params and inadequacy through iterations\")\n",
    "\n",
    "    iteration_no = 0\n",
    "\n",
    "    # Assemble the param space\n",
    "    param_space = {\"space\": param_init, \"definition\": param_def}\n",
    "    if param_addons is not None:\n",
    "        param_space.update(param_addons)\n",
    "\n",
    "    if len(param_space[\"space\"]) == 0:\n",
    "        # Generate initial set of params to be simulated! (preferable 10 instances)\n",
    "        init_sample = acquisition_fn(type=init_sampling_fn, param_space=param_space,\n",
    "                                      **fn_resolve_fn_params(init_sampling_fn, omit=[\"type\", \"param_space\", target], \n",
    "                                                             sample_size=init_sample_size, **kwargs))\n",
    "        param_space[\"space\"] = np.vstack([param_space[\"space\"], init_sample])\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Initial sample set acquired.\", count=init_sample_size)\n",
    "\n",
    "        # Simulate initial set of params\n",
    "        simulation_outcome = np.apply_along_axis(simulation_fn, 1, param_space[\"space\"],\n",
    "                                                 **fn_resolve_fn_params(simulation_fn, omit=[0], \n",
    "                                                                        param_def=param_space[\"definition\"], **kwargs))\n",
    "        param_space[\"space\"] = np.column_stack([param_space[\"space\"], simulation_outcome])\n",
    "        param_space[\"target\"] = target\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Initial sample set simulated.\")\n",
    "        if save_object_fn is not None:\n",
    "            save_object_fn(param_space, \"param_space\", stage=\"initial\")\n",
    "\n",
    "    if initialize_only:\n",
    "        return None\n",
    "\n",
    "    termination_args = fn_resolve_fn_params(termination_fn, omit=[\"iteration\", \"space\"], **kwargs)\n",
    "\n",
    "    ##############################Part 2#################################\n",
    "    \n",
    "    while not termination_fn(iteration=iteration_no, space=param_space, **termination_args):\n",
    "    # Increase iteration number\n",
    "    iteration_no += 1\n",
    "    iteration_space = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. get the best performer and save its parameters\n",
    "        iteration_space['optimal'] = fn_pull_optimal_samples(space=param_space['space'], selection_fn=selection_fn, target=target, **fn_resolve_fn_params(selection_fn, omit=['space', 'selection_fn', 'target'], **...))\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Retrieved current most optimal sample.\", count=len(iteration_space['optimal']['space']))\n",
    "      \n",
    "        # 2. fit surrogate model\n",
    "        if (iteration_no == 1) or (iteration_no % settings_obj['RETRAIN_AFTER'] == 0):\n",
    "            iteration_space['surrogate'] = {}\n",
    "            iteration_space['surrogate']['model'] = fn_fit(type=surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=['type', 'ds', 'target', 'param_def', '...'], **fn_resolve_fn_params(surrogate_fit_fn, omit=['type', 'ds', 'target', 'param_def', 'method_params'], **...))\n",
    "            param_space['surrogate'] = iteration_space['surrogate']\n",
    "            if verbose_output_fn is not None:\n",
    "                verbose_output_fn(\"Fitted surrogate model(s).\", training_set_size=len(param_space['space']))\n",
    "        else:\n",
    "            iteration_space['surrogate'] = param_space['surrogate']\n",
    "            if verbose_output_fn is not None:\n",
    "                verbose_output_fn(\"Loaded previously trained surrogate model(s).\", training_set_size=len(param_space['space']))\n",
    "      \n",
    "        # 3. optimise acquisition utility\n",
    "        iteration_space['potential'] = fn_acquisition(type=acquisition_fn, param_space=param_space, **fn_resolve_fn_params(acquisition_fn, omit=['type', 'param_space'], iter_space=iteration_space, sample_size=sample_size, omit=[target], sampling_fn=sampling_fn, surrogate_predict_fn=surrogate_predict_fn, surrogate_utility_fn=surrogate_utility_fn, inner_optimisation_fn=inner_optimisation_fn, **...), **fn_resolve_fn_params(inner_optimisation_fn, omit=['sampling_fn', 'surrogate_predict_fn', 'surrogate_model', 'surrogate_utility_fn', 'iteration_space', 'param_range', 'param_space', 'sample_size', 'omit'], target=target, **...))\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"New optimal sample set acquired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3786701723.py, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 82\u001b[1;36m\u001b[0m\n\u001b[1;33m    shared[\"param_space\"] = param_space\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2nd try !!!!!\n",
    "# Perform optimization\n",
    "while not termination_fn(iteration=iteration_no, space=param_space, **termination_args):\n",
    "    # Increase iteration number\n",
    "    iteration_no += 1\n",
    "    iteration_space = dict()\n",
    "    \n",
    "    try:\n",
    "        # 1. Get the best performer and save its parameters\n",
    "        iteration_space['optimal'] = fn_pull_optimal_samples(space=param_space['space'], selection_fn=selection_fn, target=target, **fn_resolve_fn_params(selection_fn, omit=[\"space\", \"selection_fn\", \"target\"], **params))\n",
    "\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Retrieved current most optimal sample.\", count=len(iteration_space['optimal']['space']))\n",
    "        \n",
    "        # 2. Fit surrogate model\n",
    "        if (iteration_no == 1) or (iteration_no % settings_obj['RETRAIN_AFTER'] == 0):\n",
    "            iteration_space['surrogate'] = dict()\n",
    "            iteration_space['surrogate']['model'] = fn_fit(type=surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"...\"], **params), **fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"], **params))\n",
    "            \n",
    "            param_space['surrogate'] = iteration_space['surrogate']\n",
    "            \n",
    "            if verbose_output_fn is not None:\n",
    "                verbose_output_fn(\"Fitted surrogate model(s).\", training_set_size=len(param_space['space']))\n",
    "        else:\n",
    "            iteration_space['surrogate'] = param_space['surrogate']\n",
    "            \n",
    "            if verbose_output_fn is not None:\n",
    "                verbose_output_fn(\"Loaded previously trained surrogate model(s).\", training_set_size=len(param_space['space']))\n",
    "\n",
    "        # 3. Optimise acquisition utility\n",
    "        iteration_space['potential'] = fn_acquisition(type=acquisition_fn, param_space=param_space, **fn_resolve_fn_params(acquisition_fn, omit=[\"type\", \"param_space\"], iter_space=iteration_space, sample_size=sample_size, omit=[target], sampling_fn=sampling_fn, surrogate_predict_fn=surrogate_predict_fn, surrogate_utility_fn=surrogate_utility_fn, inner_optimisation_fn=inner_optimisation_fn, **params), **fn_resolve_fn_params(inner_optimisation_fn, omit=[\"sampling_fn\", \"surrogate_predict_fn\", \"surrogate_model\", \"surrogate_utility_fn\", \"iteration_space\", \"param_range\", \"param_space\", \"sample_size\", \"omit\"], target=target, **params))\n",
    "\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"New optimal sample set acquired.\")\n",
    "            \n",
    "            \n",
    "        # 4. Simulate results of the picked-up instances\n",
    "        simulation_outcome = simulation_fn(**fn_resolve_fn_params(simulation_fn, iteration_space[\"potential\"][\"space\"], param_def=param_space[\"definition\"], omit=1))\n",
    "        iteration_space[\"potential\"][\"space\"][target] = simulation_outcome\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Simulation of the most promising samples is completed.\")\n",
    "\n",
    "        if \"complementary_utility_fn\" in iteration_space[\"potential\"] and len(iteration_space[\"potential\"][\"complementary_utility_fn\"]) > 0:\n",
    "            # 4b. Optionally simulate results of the picked-up instances by complementary acquisition functions\n",
    "            compl_space = []\n",
    "\n",
    "            for fn_header in iteration_space[\"potential\"][\"complementary_utility_fn\"]:\n",
    "                best_val_idx = iteration_space[\"potential\"][\"pop\"][fn_header].idxmax()\n",
    "                eval_set = iteration_space[\"potential\"][\"pop\"].loc[best_val_idx, iteration_space[\"potential\"][\"space\"].columns].to_frame().T\n",
    "                simulation_outcome = simulation_fn(**fn_resolve_fn_params(simulation_fn, eval_set, param_def=param_space[\"definition\"], omit=1))\n",
    "\n",
    "                eval_set[\"select_idx\"] = best_val_idx\n",
    "                eval_set[\"acq\"] = fn_header\n",
    "                eval_set[\"utility\"] = iteration_space[\"potential\"][\"pop\"].loc[best_val_idx, fn_header]\n",
    "                eval_set[\"simulation\"] = simulation_outcome\n",
    "                eval_set[\"iteration\"] = iteration_no\n",
    "                compl_space.append(eval_set)\n",
    "\n",
    "            compl_space = pd.concat(compl_space, ignore_index=True)\n",
    "\n",
    "            if \"complementary_space\" in param_space:\n",
    "                param_space[\"complementary_space\"] = pd.concat([param_space[\"complementary_space\"], compl_space], ignore_index=True)\n",
    "            else:\n",
    "                param_space[\"complementary_space\"] = compl_space\n",
    "\n",
    "            if verbose_output_fn is not None:\n",
    "                verbose_output_fn(\"Simulation of the most promising samples from complementary acquisition functions is completed.\")\n",
    "\n",
    "        # 5. Add simulated results to the base set\n",
    "        param_space[\"space\"] = pd.concat([param_space[\"space\"], iteration_space[\"potential\"][\"space\"]], ignore_index=True)\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Simulated results are added to the global set.\")\n",
    "\n",
    "        # Store required objects\n",
    "        if save_object_fn is not None and store_iteratively:\n",
    "            save_object_fn(iteration_space, \"param_space\", stage=\"optimisation\", iteration=iteration_no)\n",
    "        if save_object_fn is not None:\n",
    "            save_object_fn(param_space, \"output_object\", stage=\"partial\")\n",
    "\n",
    "        # (Optional) Assuming 'shared' is a dictionary that represents the shared environment\n",
    "        if \"shared\" in globals() and \"param_space\" in shared:\n",
    "            shared[\"param_space\"] = param_space\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############### Corrected ####################### \n",
    "\n",
    "# 4. Simulate results of the picked-up instances\n",
    "simulation_outcome = apply(iteration_space['potential']['space'], axis=1, func=simulation_fn, **fn_resolve_fn_params(simulation_fn, omit=1, param_def=param_space['definition'], **kwargs))\n",
    "iteration_space['potential']['space'][target] = simulation_outcome\n",
    "if verbose_output_fn is not None:\n",
    "    verbose_output_fn(\"Simulation of the most promising samples is completed.\")\n",
    "\n",
    "if 'complementary_utility_fn' in iteration_space['potential'] and len(iteration_space['potential']['complementary_utility_fn']) > 0:\n",
    "    # 4b. Optionally simulate results of the picked-up instances by complementary acquisition functions\n",
    "    compl_space = pd.concat([\n",
    "        pd.DataFrame([{\n",
    "            \"select_idx\": np.argmax(iteration_space['potential']['pop'][fn_header]),\n",
    "            \"acq\": fn_header,\n",
    "            \"utility\": iteration_space['potential']['pop'].loc[np.argmax(iteration_space['potential']['pop'][fn_header]), fn_header],\n",
    "            \"simulation\": apply(eval_set, axis=1, func=simulation_fn, **fn_resolve_fn_params(simulation_fn, omit=1, param_def=param_space['definition'], **kwargs)),\n",
    "            \"iteration\": iteration_no\n",
    "        }])\n",
    "        for fn_header in iteration_space['potential']['complementary_utility_fn']\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    if 'complementary_space' in param_space:\n",
    "        param_space['complementary_space'] = pd.concat([param_space['complementary_space'], compl_space], ignore_index=True)\n",
    "    else:\n",
    "        param_space['complementary_space'] = compl_space\n",
    "\n",
    "    if verbose_output_fn is not None:\n",
    "        verbose_output_fn(\"Simulation of the most promising samples from complementary acquisition functions is completed.\")\n",
    "\n",
    "# 5. Add simulated results to the base set\n",
    "param_space['space'] = pd.concat([param_space['space'], iteration_space['potential']['space']], ignore_index=True)\n",
    "if verbose_output_fn is not None:\n",
    "    verbose_output_fn(\"Simulated results are added to the global set.\")\n",
    "\n",
    "# Store required objects\n",
    "if save_object_fn is not None and store_iteratively:\n",
    "    save_object_fn(iteration_space, \"param_space\", stage=\"optimisation\", iteration=iteration_no)\n",
    "if save_object_fn is not None:\n",
    "    save_object_fn(param_space, \"output_object\", stage=\"partial\")\n",
    "\n",
    "shared[\"param_space\"] = param_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### its not the best\n",
    "## i can find some interesting lines here\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.stats import norm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def fn_bayes_optimisiation(param_init, param_def, param_addons=None, simulation_fn=None, termination_fn=None, verbose_output_fn=None,\n",
    "                           save_object_fn=None, target=\"inadequacy\", surrogate_fit_fn=None, surrogate_predict_fn=None, \n",
    "                           surrogate_utility_fn=None, init_sampling_fn=None, sampling_fn=None, acquisition_fn=None, \n",
    "                           selection_fn=None, inner_optimisation_fn=None, init_sample_size=10, sample_size=100, \n",
    "                           potential_size=1, initialize_only=False, store_iteratively=False, **kwargs):\n",
    "    \n",
    "    # Get global settings object\n",
    "    if \"RETRAIN_AFTER\" not in shared.env[\"settings\"]:\n",
    "        shared.env[\"settings\"][\"RETRAIN_AFTER\"] = 1\n",
    "    settings_obj = shared.env[\"settings\"]\n",
    "    \n",
    "    # Compile all functions\n",
    "    fn_compilation = [\"surrogate_fit_fn\", \"surrogate_predict_fn\", \"surrogate_utility_fn\", \"init_sampling_fn\", \"sampling_fn\", \n",
    "                      \"acquisition_fn\", \"selection_fn\", \"termination_fn\", \"inner_optimisation_fn\"]\n",
    "    for fn_name in fn_compilation:\n",
    "        if eval(fn_name) is None:\n",
    "            raise ValueError(f\"{fn_name} is required.\")\n",
    "        else:\n",
    "            exec(f\"{fn_name} = {eval(fn_name)}\")\n",
    "    \n",
    "    if simulation_fn is None or not callable(simulation_fn):\n",
    "        raise ValueError(\"simulation_fn is required and must be a callable function.\")\n",
    "    \n",
    "    # Check if terminal conditions are set up\n",
    "    if termination_fn is None:\n",
    "        raise ValueError(\"Termination function is required!\")\n",
    "    \n",
    "    iteration_no = 0\n",
    "    \n",
    "    # Assemble the parameter space\n",
    "    param_space = {\"space\": np.array(param_init), \"definition\": param_def}\n",
    "    if param_addons is not None:\n",
    "        param_space.update(param_addons)\n",
    "    \n",
    "    if param_space[\"space\"].shape[0] == 0:\n",
    "        # Generate initial set of parameters to be simulated\n",
    "        init_params = do_call(acquisition_fn, type=init_sampling_fn, param_space=param_space, \n",
    "                              **fn_resolve_fn_params(init_sampling_fn, omit=[\"type\", \"param_space\", \"target\"], sample_size=init_sample_size, **kwargs))\n",
    "        param_space[\"space\"] = np.vstack([param_space[\"space\"], init_params])\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Initial sample set acquired.\", count=init_sample_size)\n",
    "        \n",
    "        # Simulate initial set of parameters\n",
    "        simulation_outcome = np.apply_along_axis(lambda params: simulation_fn(params, **fn_resolve_fn_params(simulation_fn, \n",
    "                                                                    omit=[0], param_def=param_space[\"definition\"], **kwargs)), \n",
    "                                                 1, param_space[\"space\"])\n",
    "        param_space[\"space\"] = np.hstack([param_space[\"space\"], simulation_outcome.reshape(-1, 1)])\n",
    "        param_space[\"target\"] = target\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Initial sample set simulated.\")\n",
    "        if save_object_fn is not None:\n",
    "            save_object_fn(param_space, \"param_space\", stage=\"initial\")\n",
    "    \n",
    "    if initialize_only:\n",
    "        return None\n",
    "    \n",
    "    # Check termination arguments\n",
    "    if termination_fn is not None:\n",
    "        termination_args = fn_resolve_fn_params(termination_fn, omit=[\"iteration\", \"space\"], **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Termination function is required!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ B OPT##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function: assemble prediction traits that helps further applying acquisition and utility functions\n",
    "## Params:\n",
    "## - prediction_obj: object containning predictions return from surrogate_model_fn\n",
    "\n",
    "def fn_assemble_prediction_traits(prediction_obj):\n",
    "    prediction_traits = {}\n",
    "    \n",
    "    mu = np.apply_along_axis(np.mean, 1, np.hstack([np.matrix(x).reshape(-1,1) for x in prediction_obj['mu']]), nan_policy='omit')\n",
    "    prediction_traits['mu'] = mu\n",
    "    \n",
    "    sigma = np.apply_along_axis(np.mean, 1, np.hstack([np.matrix(x).reshape(-1,1) for x in prediction_obj['sigma']]), nan_policy='omit')\n",
    "    prediction_traits['sigma'] = sigma\n",
    "    \n",
    "    prediction_traits['values'] = np.hstack(prediction_obj['pred'])\n",
    "    \n",
    "    custom = {k: v for k, v in prediction_obj.items() if k not in ['mu', 'sigma', 'pred']}\n",
    "    prediction_traits['custom'] = custom\n",
    "    \n",
    "    return prediction_traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_assemble_prediction_traits_single(prediction_obj):\n",
    "    prediction_traits = {}\n",
    "    prediction_traits['mu'] = prediction_obj['mu']\n",
    "    prediction_traits['sigma'] = prediction_obj['sigma']\n",
    "    prediction_traits['values'] = prediction_obj['pred']\n",
    "    \n",
    "    prediction_traits['custom'] = {k:v for k,v in prediction_obj.items() if k not in [\"mu\",\"sigma\",\"pred\"]}\n",
    "    \n",
    "    return prediction_traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function: perform selection of optimal samples\n",
    "## Params:\n",
    "## - space: param space to be selected from\n",
    "## - target: variable name to be arranged with\n",
    "## - objective_fn: function that will take a vector and return subset of that vector that optimises certain objective. Default it minimum\n",
    "## - selection_fn: function that will perform selection after optimal have been selected. This primarily refers to selector that will \n",
    "##                 selected either all or single sample, with or without randomization\n",
    "\n",
    "def fn_pull_optimal_samples(space, target, objective_fn=min, selection_fn=None, **kwargs):\n",
    "    best_performer = {}\n",
    "    best_performer['idx'] = [i for i in range(len(space)) if space[i][target] == objective_fn([s[target] for s in space])]\n",
    "    best_performer['space'] = [space[i] for i in best_performer['idx']]\n",
    "    \n",
    "    if selection_fn is not None:\n",
    "        selected_space = selection_fn(best_performer['space'], **kwargs)\n",
    "        best_performer['space'] = selected_space['space']\n",
    "        best_performer['idx'] = [best_performer['idx'][i] for i in selected_space['idx']]\n",
    "        \n",
    "    best_performer['value'] = objective_fn([s[target] for s in best_performer['space']])\n",
    "    \n",
    "    return best_performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function: perform selection of one or many samples form optimally pulled\n",
    "## Params:\n",
    "## - space: optimal space selected\n",
    "## - random: flag indicating randomisation before selection\n",
    "## - selection_size: number of element to return from the beginning\n",
    "\n",
    "def fn_pull_optimal_selection(space, randomized_selection=False, selection_size=0):\n",
    "    space_nrow = space.shape[0]\n",
    "    idx = np.arange(space_nrow)\n",
    "    if randomized_selection:\n",
    "        idx = np.random.permutation(space_nrow)\n",
    "        space = space[idx, :]\n",
    "    \n",
    "    if selection_size <= 0 or selection_size >= space_nrow:\n",
    "        return {'space': space, 'idx': idx}\n",
    "    \n",
    "    return {'space': space[:selection_size, :], 'idx': idx[:selection_size]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function: helper function that resolve parameters of a function into a list of values, based on current environment\n",
    "\n",
    "def fn_resolve_fn_params(fn, *args, **kwargs):\n",
    "    # Get arguments list of the provided function\n",
    "    args_list = list(inspect.signature(fn).parameters.keys())\n",
    "\n",
    "    # Get all parameters provided in this function call\n",
    "    params_list = list(args) + list(kwargs.values())\n",
    "    params_names = list(kwargs.keys())\n",
    "\n",
    "    # Omit args that are requested not to be set\n",
    "    if isinstance(args[0], int):\n",
    "        omit = args[0]\n",
    "        args_list = [arg for arg in args_list if arg not in omit]\n",
    "    else:\n",
    "        omit = args[0]\n",
    "        args_list = [arg for arg in args_list if arg not in omit]\n",
    "\n",
    "    # If no other args needed, return empty\n",
    "    if not args_list:\n",
    "        return args_list\n",
    "\n",
    "    # Resolve parameter values\n",
    "    for i, arg in enumerate(args_list):\n",
    "        if arg in params_names:\n",
    "            arg_value = params_list[params_names.index(arg)]\n",
    "            args_list[i] = arg_value\n",
    "\n",
    "    return args_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_termination_max_iterations(iteration, space, max_iterations):\n",
    "    return iteration > max_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
