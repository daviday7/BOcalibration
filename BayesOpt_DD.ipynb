{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eee388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    " !pip install log4py\n",
    " !pip install dill\n",
    " !pip install jupyter_contrib_nbextensions\n",
    " !pip install ruamel.yaml\n",
    " !pip install seaborn\n",
    " !pip install scipy\n",
    " !pip install scikit-learn\n",
    " !pip install pyDOE\n",
    " !pip install rpy2\n",
    " !pip install GPy\n",
    " !pip install --upgrade numpy\n",
    " !pip install --upgrade GPy\n",
    " !pip install optimparallel\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from ruamel.yaml import YAML\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from log4py import Logger\n",
    "import importlib.util\n",
    "import random\n",
    "import dill as pickle\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# This is needed otherwise the jupyter notebook will have trouble with memory \n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='sklearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directory paths \n",
    "shared_env = {}\n",
    "\n",
    "# Server paths\n",
    "dir_path = \"/home/s212597/BOcalibration/Tallinn_Python\"\n",
    "DEFAULT_CONFIG_FILENAME = os.path.join(dir_path, \"config\", \"defaults.yml\")\n",
    "DEFAULT_LOGGING_FILENAME = os.path.join(dir_path, \"config\", \"logger.yml\")\n",
    "CONFIG_FILE= os.path.join(dir_path, \"config\",\"preday\",\"config_server_tallinn_tripsmode_qn.yml\")\n",
    "\n",
    "UPDATE_DEF = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e6a1f98",
   "metadata": {},
   "source": [
    "# Main Body\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc03115",
   "metadata": {},
   "source": [
    "## Configuration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce130fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_library(library, library_path):\n",
    "    \"\"\"\n",
    "    Load a specified library if it is properly set up.\n",
    "    \n",
    "    :param library: Name of the library.\n",
    "    :param library_path: Path to the library on disk.\n",
    "    \n",
    "    :return: A dictionary containing a status code and optionally a message or data. \n",
    "             If the library is loaded successfully, the data key contains the library data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.join(library_path, \"__init__.ipynb\")):\n",
    "        return {\"code\": 1, \"message\": f\"The library {library} was not properly set up. No __init__.ipynb file found. Thus, it is not loaded.\"}\n",
    "    \n",
    "    with open(os.path.join(library_path, \"config\", \"defaults.yml\")) as f:\n",
    "        yaml = YAML(typ=\"safe\")\n",
    "        defaults = yaml.load(f)\n",
    "    \n",
    "    return {\"data\": reduce(lambda d, k: d.get(k, {}), library_path.split(os.path.sep), {}), \"code\": 0} if defaults else {\"code\": 1, \"message\": f\"The library {library} is initialized without default settings.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbca731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logger(config_path, run_id, level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Initialize a logger based on a given configuration path and run ID.\n",
    "\n",
    "    :param config_path: Path to the configuration file containing logger settings.\n",
    "    :param run_id: Unique identifier for the current run, used to create a unique logfile name.\n",
    "    :param level: Logging level, e.g., INFO, DEBUG, ERROR, etc. Defaults to INFO.\n",
    "\n",
    "    :return: A configuration dictionary loaded from the configuration file.\n",
    "    \"\"\"\n",
    "    yaml = YAML(typ=\"safe\")\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.load(f)\n",
    "        \n",
    "    if not os.path.exists(config[\"LOG_PATH\"]):\n",
    "        os.makedirs(config[\"LOG_PATH\"], exist_ok=True)\n",
    "        \n",
    "    logfile = os.path.join(config[\"LOG_PATH\"], f\"log_{run_id}.txt\")\n",
    "    logger = Logger(logfile)  #logfile, level=level)                         \n",
    "    \n",
    "    return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bec04af2",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an argument parser.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-f', '--fingerprint', type=str, default=None, help='Run fingerprint')\n",
    "parser.add_argument('-m', '--surrogate_model', type=str, default=None, help='Surrogate model')\n",
    "parser.add_argument('-u', '--uitility_function', type=str, default=None, help='Utility function')\n",
    "parser.add_argument('-s', '--param_change_share', type=float, default=None, help='Share of parameters to be changed at each iteration')\n",
    "parser.add_argument('-v', '--param_change_variation', type=float, default=None, help='Variation of parameter change at each iteration')\n",
    "parser.add_argument('-C', '--config_file', type=str, default='config/preday/config_local.yml', help='Config file to load')\n",
    "parser.add_argument('-P', '--param_definition_file', type=str, default=None, help='Parameters file to load')\n",
    "parser.add_argument('-E', '--param_exclusion_file', type=str, default=None, help='File with exluded parameters to load')\n",
    "parser.add_argument('-A', '--city_actual_stats', type=str, default=None, help='File with true city statistics')\n",
    "parser.add_argument('-O', '--object_storage_path', type=str, default=None, help='Path where output objects will be stored')\n",
    "parser.add_argument('-X', '--sh_scripts_path', type=str, default=None, help='Path of shell scripts for executing commands over the simulator')\n",
    "parser.add_argument('-H', '--container_home', type=str, default=None, help='Home path of container runtime and configuration')\n",
    "parser.add_argument('-c', '--containerized', type=bool, default=None, help='Flag for executing simulation in container (Docker)')\n",
    "parser.add_argument('-i', '--container_image', type=str, default=None, help='Container image name')\n",
    "parser.add_argument('-b', '--initialization_only', type=bool, default=False, help='Indicator to indicate only initialization of random seed and initial sample set')\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup running and configuration fingerprint settings\n",
    "settings = {}\n",
    "settings[\"SCOPE\"] = \"preday\"\n",
    "settings[\"FINGERPRINT\"] = round(time.time() * 1000)\n",
    "\n",
    "# Load logger\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open(DEFAULT_LOGGING_FILENAME) as f:\n",
    "    log_config = yaml.load(f)\n",
    "    \n",
    "#logger = load_logger(DEFAULT_LOGGING_FILENAME,settings[\"FINGERPRINT\"], level=\"INFO\")\n",
    "settings[\"logger\"] = log_config\n",
    "\n",
    "# Load default configuration\n",
    "with open(DEFAULT_CONFIG_FILENAME) as f:\n",
    "    default_config = yaml.load(f)\n",
    "    \n",
    "settings[\"DEFAULTS\"] = default_config\n",
    "settings = {**settings[\"DEFAULTS\"], **settings}\n",
    "del settings[\"DEFAULTS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preloaded libraries\n",
    "if \"PRELOADED_LIBRARIES\" in settings and settings[\"PRELOADED_LIBRARIES\"] is not None:\n",
    "    for lib_name in settings[\"PRELOADED_LIBRARIES\"]:\n",
    "        lib_path = os.path.join(settings[\"LIB_PATH\"], lib_name)\n",
    "        lib_config = load_library(lib_name, lib_path)\n",
    "        \n",
    "        if \"data\" in lib_config:\n",
    "            settings.update(lib_config[\"data\"])\n",
    "        else:\n",
    "            if lib_config[\"code\"] == 1 and \"message\" in lib_config:\n",
    "                logger.warn(lib_config[\"message\"])\n",
    "            elif lib_config[\"code\"] == 2 and \"message\" in lib_config:\n",
    "                logger.error(lib_config[\"message\"])\n",
    "                \n",
    "with open(CONFIG_FILE) as f:\n",
    "    provided_config = yaml.load(f)\n",
    "\n",
    "settings[\"PROVIDED_CONFIGURATION\"] = provided_config\n",
    "settings = {**settings[\"PROVIDED_CONFIGURATION\"], **settings}\n",
    "del settings[\"PROVIDED_CONFIGURATION\"]\n",
    "\n",
    "\n",
    "# Load additional libraries (if specified)\n",
    "if \"LIBRARIES\" in provided_config and len(provided_config[\"LIBRARIES\"]) > 0:\n",
    "    for library in provided_config[\"LIBRARIES\"]:\n",
    "        if library == \"preday\":\n",
    "            lib_config = load_library(library, os.path.join(dir_path, \"libs\", library)) \n",
    "\n",
    "            if \"data\" in lib_config:\n",
    "                settings.update(lib_config[\"data\"])\n",
    "            elif lib_config[\"code\"] == 1 and \"message\" in lib_config and lib_config[\"message\"] is not None:\n",
    "                logger.warn(lib_config[\"message\"])\n",
    "            elif lib_config[\"code\"] == 2 and \"message\" in lib_config and lib_config[\"message\"] is not None:\n",
    "                logger.error(lib_config[\"message\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1124111",
   "metadata": {},
   "source": [
    "### Creating file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b91b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.surrogate_model is not None:\n",
    "    settings['SURROGATE_MODEL_STR'] = args.surrogate_model\n",
    "if args.uitility_function is not None:\n",
    "    settings['UTILITY_FUNC_STR'] = args.uitility_function\n",
    "if args.object_storage_path is not None:\n",
    "    settings['OBJECT_STORAGE_PATH'] = args.object_storage_path\n",
    "if args.param_definition_file is not None:\n",
    "    settings['INITIAL_PARAMS_PATH'] = args.param_definition_file\n",
    "if args.param_exclusion_file is not None:\n",
    "    settings['OMITTED_PARAMS_PATH'] = args.param_exclusion_file\n",
    "if args.city_actual_stats is not None:\n",
    "    settings['OBSERVED_STATS_PATH'] = args.city_actual_stats\n",
    "if args.container_home is not None:\n",
    "    settings['CONTAINER_HOME_PATH'] = args.container_home\n",
    "if args.sh_scripts_path is not None:\n",
    "    settings['SH_SCRIPTS'] = args.sh_scripts_path\n",
    "if args.param_change_share is not None:\n",
    "    settings['CHANGE_SHARE_ATTRIBUTES'] = args.param_change_share\n",
    "if args.param_change_variation is not None:\n",
    "    settings['VARIATION_SD'] = args.param_change_variation\n",
    "if args.containerized is not None:\n",
    "    settings['CONTAINERIZED'] = args.containerized\n",
    "if args.container_image is not None:\n",
    "    settings['IMAGE_NAME'] = args.container_image\n",
    "if args.initialization_only is not None:\n",
    "    settings['INIT_ONLY'] = args.initialization_only\n",
    "\n",
    "# Testing mode\n",
    "settings[\"MOCKING_MODE\"]=False\n",
    "\n",
    "# Creating file paths\n",
    "\n",
    "settings[\"SH_SCRIPTS\"]= os.path.join(dir_path,\"containers_preday_sing_triton\",\"scripts_sh\")\n",
    "settings['OBSERVED_STATS_PATH'] = os.path.join(dir_path,\"data_preday\",\"tallin_od_weighted_coded.csv\")\n",
    "settings['DISTRICT_MAP_PATH'] = os.path.join(dir_path,\"data_preday\",\"tallinn_grid_def.csv\")\n",
    "settings['WEIGHT_MAP_PATH'] = os.path.join(dir_path,\"data_preday\",\"tallinn_od_mask_coded.csv\")\n",
    "settings['MODE_BALANCE_PATH'] = os.path.join(dir_path,\"data_preday\",\"tallinn_mode_balance.csv\")\n",
    "settings['WORKERS_POPULATION_PATH'] = os.path.join(dir_path,\"data_preday\",\"tallinn_population_workers.csv\")\n",
    "settings['INITIAL_PARAMS_PATH'] = os.path.join(dir_path,\"data_preday\",\"preday_params_complete_tours_modes_v3.csv\")\n",
    "\n",
    "# Danish datasets paths\n",
    "# WILL BE CHANGED\n",
    "settings['time_distribution'] = os.path.join(dir_path,\"time_distribution.csv\")\n",
    "settings['trips_distribution'] = os.path.join(dir_path,\"trips_distribution.csv\")\n",
    "\n",
    "# Creating script paths \n",
    "settings[\"PARAM_FILE_PATH\"] = os.path.join(dir_path,\"scripts\", \"behavior_vc_archive_1\")          \n",
    "settings[\"PARAM_FILE_EXTENSION\"] = os.path.join(settings[\"PARAM_FILE_PATH\"],\"dpb.lua\")           \n",
    "\n",
    "settings[\"CONTAINER_RUNTIME_PATH\"] = os.path.join(settings[\"CONTAINER_HOME_PATH\"], \"runtime\")\n",
    "settings[\"OBJECT_PATH\"] = os.path.abspath(os.path.join(\".\", settings[\"OBJECT_STORAGE_PATH\"], f\"output_{settings['FINGERPRINT']}\"))\n",
    "settings[\"RANDOM_SEED_FILE\"] = os.path.join(settings[\"OBJECT_PATH\"], f\"bolfi_{settings['SCOPE']}_random_seed_object.Rds\")\n",
    "settings[\"PARAM_SPACE_FILE\"] = os.path.join(settings[\"OBJECT_PATH\"], f\"bolfi_{settings['SCOPE']}_param_space_object.Rds\")\n",
    "settings[\"OUTPUT_FILE\"] = os.path.join(settings[\"OBJECT_PATH\"], f\"bolfi_{settings['SCOPE']}_output_object.Rds\")\n",
    "settings[\"CONFIGURATION_OUTPUT_FILE\"] = os.path.join(settings[\"OBJECT_PATH\"], f\"bolfi_{settings['SCOPE']}_configuration_object.Rds\")\n",
    "\n",
    "# Adapt the local file system and necessary file loadings\n",
    "if not os.path.exists(settings[\"OBJECT_PATH\"]):\n",
    "    os.makedirs(settings[\"OBJECT_PATH\"])\n",
    "if not os.path.exists(settings[\"CONTAINER_RUNTIME_PATH\"]):\n",
    "    os.makedirs(settings[\"CONTAINER_RUNTIME_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e5d23",
   "metadata": {},
   "source": [
    "## Load the observation files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the paths to load city stats (individuals_10)\n",
    "settings['CITY_STATS'] = {}\n",
    "settings['CITY_STATS']['value'] = pd.read_csv(settings['OBSERVED_STATS_PATH'], index_col=0, encoding=\"UTF-8\").values * (settings['PARTIAL_POPULATION'] if 'PARTIAL_POPULATION' in settings else 1)\n",
    "settings['CITY_STATS']['total_trips'] = np.sum(settings['CITY_STATS']['value'], axis=None, keepdims=False)\n",
    "\n",
    "# Danish Travel Survey data\n",
    "settings['CITY_STATS'][\"trips_distribution\"] = pd.read_csv(settings['trips_distribution'], encoding=\"UTF-8\")\n",
    "settings['CITY_STATS'][\"time_distribution\"]= pd.read_csv(settings['time_distribution'], encoding=\"UTF-8\")\n",
    "\n",
    "# Talin datasets (for future Danish related datasets)\n",
    "if 'RELATIVE_OD' in settings and settings['RELATIVE_OD']:\n",
    "    settings['CITY_STATS']['value'] = settings['CITY_STATS']['value'] / np.sum(settings['CITY_STATS']['value'])\n",
    "settings['CITY_STATS']['district_map'] = pd.read_csv(settings['DISTRICT_MAP_PATH'], encoding=\"UTF-8\")\n",
    "if 'WEIGHT_MAP_PATH' in settings:\n",
    "    settings['CITY_STATS']['weights'] = pd.read_csv(settings['WEIGHT_MAP_PATH'], index_col=0, encoding=\"UTF-8\").values\n",
    "if 'MODE_BALANCE_PATH' in settings:\n",
    "    settings['CITY_STATS']['balance'] = pd.read_csv(settings['MODE_BALANCE_PATH'], encoding=\"UTF-8\")\n",
    "if 'WORKERS_POPULATION_PATH' in settings:\n",
    "    settings['CITY_STATS']['workers'] = pd.read_csv(settings['WORKERS_POPULATION_PATH'], encoding=\"UTF-8\")\n",
    "\n",
    "OD_cells = np.arange(len(settings['CITY_STATS']['value']))\n",
    "settings['CITY_STATS']['emptyOD'] = np.zeros((len(OD_cells), len(OD_cells)))\n",
    "settings['CITY_STATS']['emptyOD'] = pd.DataFrame(settings['CITY_STATS']['emptyOD'], index=OD_cells, columns=OD_cells)\n",
    "\n",
    "# If omit file provided, load it\n",
    "settings[\"OMIT_LIST\"] = []\n",
    "if \"OMITTED_PARAMS_PATH\" in settings and not pd.isna(settings[\"OMITTED_PARAMS_PATH\"]):\n",
    "    omit_df = pd.read_csv(settings[\"OMITTED_PARAMS_PATH\"])\n",
    "    settings[\"OMIT_LIST\"] = omit_df[\"parameter\"].tolist()\n",
    "\n",
    "# If external method is used, adapt the directories for input and output to be the same as the current run home directory\n",
    "if \"EXTERNAL_METHOD\" in settings:                                                                 # external \n",
    "    settings[\"METHOD_INPUT_PATH\"] = os.path.join(os.path.abspath(settings[\"OBJECT_PATH\"]), \"\")\n",
    "    settings[\"METHOD_OUTPUT_PATH\"] = os.path.join(os.path.abspath(settings[\"OBJECT_PATH\"]), \"\")\n",
    "\n",
    "    settings[\"CONTAINER_HOME_PATH\"] = os.path.abspath(settings[\"CONTAINER_HOME_PATH\"])\n",
    "    settings[\"CONTAINER_RUNTIME_PATH\"] = os.path.abspath(settings[\"CONTAINER_RUNTIME_PATH\"])\n",
    "\n",
    "    if \"EXTERNAL_FEEDBACK_SCRIPT\" in settings:\n",
    "        shutil.copy(settings[\"EXTERNAL_FEEDBACK_SCRIPT\"], settings[\"OBJECT_PATH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f800338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If READ_MEMORY setting is enabled, and a file for parameter space object exists in the defined path \n",
    "# but not in the location defined by 'PARAM_SPACE_FILE', then copy the file to 'PARAM_SPACE_FILE'.\n",
    "if (settings['READ_MEMORY'] and os.path.exists(f\"./{settings['OBJECT_STORAGE_PATH']}/bolfi_{settings['SCOPE']}_param_space_object.Rds\") and \n",
    "    not os.path.exists(settings['PARAM_SPACE_FILE'])):\n",
    "    \n",
    "    shutil.copy(f\"./{settings['OBJECT_STORAGE_PATH']}/bolfi_{settings['SCOPE']}_param_space_object.Rds\", settings['PARAM_SPACE_FILE'])\n",
    "\n",
    "# If READ_MEMORY setting is enabled, and a file for random seed object exists in the defined path \n",
    "# but not in the location defined by 'RANDOM_SEED_FILE', then copy the file to 'RANDOM_SEED_FILE'.\n",
    "if (settings['READ_MEMORY'] and os.path.exists(f\"./{settings['OBJECT_STORAGE_PATH']}/bolfi_{settings['SCOPE']}_random_seed_object.Rds\") and \n",
    "    not os.path.exists(settings['RANDOM_SEED_FILE'])):\n",
    "    \n",
    "    shutil.copy(f\"./{settings['OBJECT_STORAGE_PATH']}/bolfi_{settings['SCOPE']}_random_seed_object.Rds\", settings['RANDOM_SEED_FILE'])\n",
    "       \n",
    " # Set function names for surrogate modeling, prediction, and utility based on the selected surrogate model and utility function strings.   \n",
    "settings['SURROGATE_MODEL_FN'] = f\"fn_fit_{settings['SURROGATE_MODEL_STR']}\"\n",
    "settings['SURROGATE_PREDICT_FN'] = f\"fn_predict_{settings['SURROGATE_MODEL_STR']}\"\n",
    "settings['SURROGATE_UTILITY_FN'] = f\"fn_utility_{settings['UTILITY_FUNC_STR']}\"\n",
    "settings['HEADER'] = list(settings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for verbose_output\n",
    "def verbose_output(msg, **kwargs):\n",
    "    \"\"\"\n",
    "    Logs a message along with provided parameters in a structured format.\n",
    "\n",
    "    :param msg: The main message to log.\n",
    "    :param kwargs: Additional parameters to be included in the log message.\n",
    "    \"\"\"\n",
    "    message_params_str = ', '.join([f\"{key}: {value}\" for key, value in kwargs.items()])\n",
    "    logger.info(f\"{msg} |=> {message_params_str}\")\n",
    "\n",
    "def save_object(obj,settings, filename=\"space\",  **kwargs):\n",
    "    \"\"\"\n",
    "    Serialize and save an object to a pickle file.\n",
    "\n",
    "    :param obj: The object to be saved.\n",
    "    :param settings: Configuration or settings data (currently not used in the function body but might be used for future enhancements).\n",
    "    :param filename: The base name of the file to which the object should be saved (defaults to \"space\").\n",
    "    :param kwargs: Additional parameters, not used currently but can be used for future enhancements.\n",
    "    \"\"\"\n",
    "    with open('iteration.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d5c323e",
   "metadata": {},
   "source": [
    "## Preday functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preday imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Union\n",
    "from pyDOE import lhs\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3568b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_load_params_space_definition(filepath: str, omit: Union[List[str], str] = list(), sys_modules: List[str] = [\"preday\"], \n",
    "                                    tunnel_constraint: bool = True, update_initial: bool = False):\n",
    "    \"\"\"\n",
    "    Load parameter space definitions from a CSV file and process them.\n",
    "\n",
    "    :param filepath: Path to the CSV file containing parameter space definitions.\n",
    "    :param omit: Columns or parameters to omit from the output.\n",
    "    :param sys_modules: List of system modules to consider.\n",
    "    :param tunnel_constraint: Boolean flag indicating whether tunnel constraints should be applied.\n",
    "    :param update_initial: Boolean flag to control updating the initial values. Not used in current implementation.\n",
    "    :return: A dictionary with the processed parameter space definitions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predefined lists for categorizing models\n",
    "    no_tours_models = [\"dpb\", \"dps\", \"dpt\", \"nte\", \"ntw\", \"nto\", \"nts\", \"isg\", \"imd\", \"tuw\"]\n",
    "    mode_balance_models = [\"tme\", \"tmw\", \"stmd\", \"tmdo\", \"tmds\", \"tmdw\", \"wdme\", \"wdmso\", \"wdmw\"]\n",
    "    low_priority_models = [\"tws\", \"sttd\", \"ttde\", \"ttdw\", \"ttdo\", \"itd\", \"ptrc\", \"pvtrc\", \"zpa\"]\n",
    "\n",
    "    # Read the CSV file\n",
    "    file_content = pd.read_csv(filepath)\n",
    "    \n",
    "    # Filter the data based on provided modules and inclusion flag\n",
    "    file_content = file_content[(file_content['module'].isin(sys_modules)) & (file_content['include'])]\n",
    "\n",
    "    # Split the 'param' column into separate 'param_name' and 'value' columns based on '=' delimiter\n",
    "    file_content[['param_name', 'value']] = file_content['param'].str.extract(r'^(.+?)\\s*=\\s*(.+)$')\n",
    "\n",
    "    # Further process and extract relevant information from the 'param' column\n",
    "    file_content['declaration'] = file_content['param'].str.extract(r'^(.+?)(?=beta|cons)')\n",
    "    file_content['param_name_left_hand'] = file_content['param_name']\n",
    "    file_content['param_name'] = file_content['param_name'].str.replace(r'(local |bundled_variables\\.)', '', regex=True).str.strip()\n",
    "    file_content['parameter'] = file_content['model'] + '_' + file_content['param_name']\n",
    "    file_content['raw_value'] = file_content['value'].str.strip()\n",
    "    file_content['value'] = pd.to_numeric(file_content['value'].str.replace(r'\\s', '', regex=True))\n",
    "\n",
    "    # Classify models into direct influence categories\n",
    "    file_content['direct_influence'] = file_content['model'].apply(lambda x: 'tours' if x in no_tours_models else ('mode' if x in mode_balance_models else 'none'))\n",
    "\n",
    "    # Setup lower and upper limit columns if not present\n",
    "    if 'lower_limit' not in file_content.columns:\n",
    "        file_content['lower_limit'] = 0\n",
    "    if 'upper_limit' not in file_content.columns:\n",
    "        file_content['upper_limit'] = np.inf\n",
    "    file_content['enabled'] = file_content['lower_limit'] != file_content['upper_limit']\n",
    "\n",
    "    # Handle omit parameter\n",
    "    if not isinstance(omit, list) and len(omit) == 1:\n",
    "        omit_col_name = omit\n",
    "        omit = file_content.loc[file_content[omit_col_name], 'parameter'].tolist()\n",
    "        file_content = file_content.drop(columns=[omit_col_name])\n",
    "    file_content = file_content[~file_content['parameter'].isin(omit)]\n",
    "\n",
    "    # Prepare column headers\n",
    "    column_header = file_content['parameter'].tolist()\n",
    "    residual_pool_header = [\"time\", \"trips\", \"od\"]\n",
    "\n",
    "    # Apply tunnel constraints if specified\n",
    "    if tunnel_constraint:\n",
    "        t_width = 2.0\n",
    "        t_restricted_width = 0.05\n",
    "        # Set lower and upper limit values based on initial values and constraints\n",
    "        file_content['lower_limit'] = file_content.apply(lambda row: row['initial'] if not row['enabled'] else (row['initial'] - t_restricted_width if re.search(r\"logsum\", row['parameter']) else row['initial'] - t_width), axis=1)\n",
    "        file_content['upper_limit'] = file_content.apply(lambda row: row['initial'] if not row['enabled'] else (row['initial'] + t_restricted_width if re.search(r\"logsum\", row['parameter']) else row['initial'] + t_width), axis=1)\n",
    "        file_content['tunnel_width'] = t_width\n",
    "\n",
    "    # Create final definition and space dataframes\n",
    "    definition = file_content.drop(columns=['value', 'raw_value']).assign(\n",
    "        changed=file_content['parameter'].apply(lambda x: x not in omit),\n",
    "        init_lower_limit=file_content['lower_limit'],\n",
    "        init_upper_limit=file_content['upper_limit']\n",
    "    )\n",
    "    space = file_content[['value']].T.set_axis(column_header, axis=1).head(0)\n",
    "    value_pool = {column: [] for column in column_header}\n",
    "    residual_pool = {column: [] for column in residual_pool_header}\n",
    "\n",
    "    return {\n",
    "        'definition': definition,\n",
    "        'space': space,\n",
    "        'value_pool': value_pool,\n",
    "        'residual_pool': residual_pool\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e402f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_preday_sampling(param_space,sample_size,vals=None, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to generate sampling of preday parameter space.\n",
    "\n",
    "    Args:\n",
    "        param_space (dict): Dictionary containing parameter space definitions.\n",
    "        sample_size (int): Size of the sample to generate.\n",
    "        vals (array, optional): Previously generated values.\n",
    "        **kwargs: Keyword arguments for specific sampling methods and options.\n",
    "\n",
    "    Returns:\n",
    "        numpy.array: Generated sample.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_preday_sampling\")\n",
    "           \n",
    "    # check if the given vals and param_space arguments are None or empty\n",
    "        \n",
    "    is_initial = True if (vals is None or param_space['space'].shape[0] == 0) else False\n",
    "    \n",
    "    # get the indexes of enabled parameters\n",
    "    enabled_idx = np.arange(param_space['definition'].shape[0])\n",
    "    if 'enabled' in param_space['definition'].columns:                            #.dtype.names:\n",
    "        enabled_idx = np.where(param_space['definition']['enabled'] == True)[0]\n",
    "    \n",
    "    # get the names of enabled parameters\n",
    "    dim_names = param_space['definition']['parameter']\n",
    "\n",
    "    # get the number of enabled parameters\n",
    "    sample_dim = len(enabled_idx)\n",
    "    \n",
    "    # create the generated sample numpy array\n",
    "    genereted_sample = np.tile(param_space['definition']['initial'], (sample_size, 1))\n",
    "    \n",
    "    # if the given vals and param_space arguments are None or empty, generate a new sample\n",
    "    if is_initial:\n",
    "                \n",
    "        # Use the pyDOE lhs function and fn_scaleup_standard to generate samples\n",
    "        genereted_sample[:, enabled_idx] = fn_scaleup_standard(lhs(sample_size, sample_dim),param_space['definition'].iloc[enabled_idx].values)\n",
    "        genereted_sample = pd.DataFrame(genereted_sample, columns=dim_names)\n",
    "\n",
    "        ## Add the initial param values\n",
    "        initial_values = pd.DataFrame(param_space['definition']['initial'].values.reshape(1, -1), columns=dim_names)\n",
    "        genereted_sample = pd.concat([initial_values, genereted_sample], ignore_index=True)\n",
    "    \n",
    "        return genereted_sample\n",
    "    \n",
    "    # if the SAMPLING_OPTIMAL_TOLERANCE keyword argument is given and its value is greater than 0,\n",
    "    # find the minimum value of target_col column in the param_space array and \n",
    "    # filter the rows where the target_col value is less than the sum of the minimum value and SAMPLING_OPTIMAL_TOLERANCE\n",
    "    if 'SAMPLING_OPTIMAL_TOLERANCE' in kwargs and kwargs['SAMPLING_OPTIMAL_TOLERANCE'] > 0:\n",
    "        min_inadequacy = np.nanmin(param_space['space'][param_space['space'][shared_env['settings']['target_col']].notna()][shared_env['settings']['target_col']])\n",
    "        vals = param_space['space'][param_space['space'][shared_env['settings']['target_col']] < (min_inadequacy + kwargs['SAMPLING_OPTIMAL_TOLERANCE'])][dim_names].dropna()\n",
    "        \n",
    "        if vals.shape[0] == 1:\n",
    "            vals = np.array([tuple(vals.values.tolist()[0])], dtype=param_space['definition'].dtype)\n",
    "        else:\n",
    "            vals = vals.to_records(index=False)\n",
    "    \n",
    "    # if there is only one row in the vals array, tile it to create a new vals array with the same shape as the generated sample\n",
    "    if vals.shape[0] == 1:\n",
    "        vals = np.tile(vals[:, enabled_idx], (sample_size, 1))\n",
    "    else:\n",
    "        vals = vals[:, enabled_idx]\n",
    "    \n",
    "    # set the intensities for each sampling method\n",
    "    tours_intensity = [0.6, 0.6]  # high and low intensity\n",
    "    mode_intensity = [0.5, 0.5]\n",
    "    od_intensity = [0.5, 0.5]\n",
    "    none_intensity = [0.2, 0.2]\n",
    "    sampling_intensities = {\n",
    "        'time': tours_intensity,\n",
    "        'trips': mode_intensity,\n",
    "        'od': od_intensity,\n",
    "        'none': none_intensity\n",
    "    }\n",
    "\n",
    "    # Enable only parameters in the active subspace\n",
    "    enabled_idx = np.where(param_space['active'])[0]\n",
    "    dim_names = param_space['names'][enabled_idx]\n",
    "    sample_dim = len(enabled_idx)\n",
    "    \n",
    "    # Set up intensity values for each parameter\n",
    "    sampling_intensities = shared_env['settings']['SAMPLING_INTENSITY']\n",
    "    sampling_intensity = {}\n",
    "    for s_intensity in sampling_intensities:\n",
    "        \n",
    "        if (shared_env['param_space']['residual_pool'][s_intensity] is None \n",
    "            or len(shared_env['param_space']['residual_pool'][s_intensity]) < 2):\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][0]\n",
    "            continue\n",
    "        \n",
    "        tail_diff = np.abs(np.diff(shared_env['param_space']['residual_pool'][s_intensity])[-3:])\n",
    "        thrs = shared_env['settings']['EPSILON'][s_intensity]\n",
    "        \n",
    "        if not np.any(tail_diff > thrs['focused_sampling']):\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][0] \n",
    "        elif not np.any(tail_diff > thrs['spread_sampling']):\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][1]\n",
    "        else:\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][0]\n",
    "            \n",
    "    # Generate sample for each parameter\n",
    "    generated_sample = np.zeros((sample_size, len(dim_names)))\n",
    "    for i, c_i in enumerate(enabled_idx):\n",
    "        \n",
    "        p_def = param_space['definition'][c_i]\n",
    "        p_intensity = sampling_intensity[p_def['direct_influence']]\n",
    "        p_replaced_size = int(np.ceil(sample_size * p_intensity))\n",
    "        p_current = vals[np.random.choice(np.arange(vals.shape[0])), c_i]\n",
    "        \n",
    "        p_lower_bound = p_def['lower_limit']\n",
    "        p_upper_bound = p_def['upper_limit']\n",
    "        p_tunnel_bound = 2\n",
    "        if 'tunnel_width' in p_def:\n",
    "            p_tunnel_bound = p_def['tunnel_width']\n",
    "            p_lower_bound = p_current - p_tunnel_bound\n",
    "            p_upper_bound = p_current + p_tunnel_bound\n",
    "        \n",
    "        p_sample = np.repeat(p_current, sample_size)\n",
    "        p_sample_replace = np.random.choice(np.arange(sample_size), p_replaced_size, replace=False)\n",
    "        \n",
    "        # Uniform sampling\n",
    "        # p_sample[p_sample_replace] = np.random.uniform(p_lower_bound, p_upper_bound, p_replaced_size)\n",
    "        \n",
    "        # Normal around the optimal sampling\n",
    "        p_sample[p_sample_replace] = np.random.normal(p_current, p_tunnel_bound, p_replaced_size)\n",
    "        \n",
    "        generated_sample[:, i] = p_sample\n",
    "        \n",
    "    # Add initial parameter values to the generated sample\n",
    "    if not shared_env['settings'].get('IMPUTED_INITIAL_VALUE_SET', False):\n",
    "        if not np.allclose(param_space['space'][0, :len(enabled_idx)], \n",
    "                            np.array([p['initial'] for p in param_space['definition'][enabled_idx]])):\n",
    "            init_param_values = np.array([p['initial'] for p in param_space['definition']])\n",
    "            generated_sample = np.vstack((init_param_values, generated_sample))\n",
    "        shared_env['settings']['IMPUTED_INITIAL_VALUE_SET'] = True\n",
    "    \n",
    "    return generated_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_perform_simulation(value, param_def):\n",
    "    \"\"\"\n",
    "    Function to perform a simulation based on the given value and parameter definition.\n",
    "\n",
    "    Args:\n",
    "        value (pd.Series or pd.DataFrame): Value or set of values for the parameters.\n",
    "        param_def (pd.DataFrame): Definition of the parameters.\n",
    "\n",
    "    Returns:\n",
    "        float: Computed inadequacy from the simulation.\n",
    "    \"\"\"\n",
    "    print(\"fn_perform_simulation\")\n",
    "    # Check if the value is a Series\n",
    "    if isinstance(value, pd.Series):\n",
    "        value1=value.copy()\n",
    "        value1.reset_index(drop=True, inplace=True)\n",
    "        param_def['value'] = value1.tolist()\n",
    "\n",
    "    # Check if the value is a DataFrame\n",
    "    elif isinstance(value, pd.DataFrame):\n",
    "        param_def[\"value\"]=value\n",
    "\n",
    "    # Value is neither a Series nor a DataFrame\n",
    "    else:\n",
    "        print(\"Value is neither a Series nor a DataFrame\")\n",
    "   \n",
    "    param_def.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Simulate\n",
    "    param_iter_run = fn_simulation(param_def)\n",
    "\n",
    "    # Warnings for particular param not being applied\n",
    "    # ... TODO\n",
    "\n",
    "    # Update the value pool\n",
    "    fn_update_value_pools(value, param_iter_run[\"base_residuals\"], param_def)\n",
    "\n",
    "    \n",
    "    return param_iter_run[\"inadequacy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_simulation(params):\n",
    "    \"\"\"\n",
    "    Simulate a model or system using given parameters.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame or dict): Input parameters for the simulation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing configuration output, inadequacy value, and base residuals.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_simulation\")\n",
    "\n",
    "    # Update the simulation configuration files with the given parameters\n",
    "    param_config_output = fn_simulation_config(params)\n",
    "\n",
    "    # Execute the actual simulation process, using a predefined global object \"setting_obj\"\n",
    "    fn_simulation_call(setting_obj)\n",
    "\n",
    "    # Process the results of the simulation to get the activity outcomes\n",
    "    outcome, travel_time_nrmse, total_trips_nrmse, OD_rmse = fn_process_activities()\n",
    "\n",
    "    # Prepare the inadequacy dictionary based on the outcome of activities\n",
    "    inadequacy = {\"value\": outcome}\n",
    "\n",
    "    # If the simulation is being run in a MOCKING_MODE (presumably a testing mode), \n",
    "    # add mock data to the inadequacy dictionary\n",
    "    if settings['MOCKING_MODE']:\n",
    "        inadequacy[\"base\"] = {\n",
    "            'tours': [np.random.normal(0, 10)],\n",
    "            'mode': [np.random.normal(0, 10)],\n",
    "            'od': [np.random.normal(0, 10)]\n",
    "        }\n",
    "\n",
    "    # Regardless of MOCKING_MODE, always generate a set of random base values for the inadequacy dictionary\n",
    "    inadequacy[\"base\"] = {\n",
    "        'time': [travel_time_nrmse],\n",
    "        'trips': [total_trips_nrmse],\n",
    "        'od': [OD_rmse]\n",
    "    }\n",
    "\n",
    "    # Return a dictionary that includes configuration output, inadequacy values, and residuals\n",
    "    return {\n",
    "        \"config_output\": param_config_output,\n",
    "        \"inadequacy\": inadequacy[\"value\"],\n",
    "        \"base_residuals\": inadequacy[\"base\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_simulation_config(params):\n",
    "    \"\"\"\n",
    "    Update the simulation configuration using given parameters.\n",
    "\n",
    "    Args:\n",
    "        params (pd.DataFrame or list of dict): Input parameters to update the simulation configuration.\n",
    "\n",
    "    Returns:\n",
    "        list: List of statuses indicating the success or failure of each parameter update.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_simulation_config\")\n",
    "\n",
    "    # Initialize a list to store the status of each parameter update\n",
    "    status = []\n",
    "\n",
    "    # Convert the input params DataFrame into a list of dictionaries\n",
    "    params = params.to_dict(\"records\")\n",
    "\n",
    "    # Iterate over each parameter and update the model configuration using a helper function\n",
    "    for p in params:\n",
    "        # The function fn_param_update is presumed to apply the parameter to the model \n",
    "        # and return a status indicating success or failure\n",
    "        status.append(fn_param_update(p['model'], p['param_name'], p['declaration'], p['param_name_left_hand'], p['value']))\n",
    "\n",
    "    # If the simulation is containerized and a script is set to push parameters,\n",
    "    # then execute the push parameters script\n",
    "    if setting_obj[\"CONTAINERIZED\"] and setting_obj[\"SH_PUSH_PARAM\"] is not None:\n",
    "        push_status = fn_param_push()\n",
    "\n",
    "        # If there's an issue with pushing the parameters, update the status list\n",
    "        # to indicate that all parameter updates failed\n",
    "        if push_status != 0:\n",
    "            status = [push_status] * len(status)\n",
    "    \n",
    "    # Return the list of status codes\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de32365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_param_update(model, param_name, declaration, param_name_left_hand, value):\n",
    "    \"\"\"\n",
    "    Update the specified parameter's value in the given lua model file.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the lua model file (without the .lua extension).\n",
    "        param_name (str): The name of the parameter to be updated.\n",
    "        declaration (str): The declaration associated with the parameter (e.g., local).\n",
    "        param_name_left_hand (str): Left-hand side representation of the parameter (likely deprecated or unused here).\n",
    "        value (any): The new value to be assigned to the parameter.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the parameter declaration in the lua file is in the format:\n",
    "        <declaration> <param_name> = <some_value>\n",
    "        And it will replace <some_value> with the provided value.\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"fn_param_update\")\n",
    "\n",
    "    # Define the directory path where the lua files are located\n",
    "    lua_folder_path = \"/home/s212597/simmobility/scripts/lua/mid/behavior_vc/\"\n",
    "    \n",
    "    # Construct the full path to the desired lua file based on the given model name\n",
    "    lua_file_path = os.path.join(lua_folder_path, f'{model}.lua')\n",
    "\n",
    "    # Open the lua file and read its contents\n",
    "    with open(lua_file_path, 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Search for the line that contains the target parameter name\n",
    "    param_with_value = next((line for line in filedata.split('\\n') if param_name in line), None)\n",
    "    \n",
    "    # If the parameter is found in the file, replace its value with the new one\n",
    "    if param_with_value:\n",
    "        filedata = filedata.replace(param_with_value, f'{declaration} {param_name} = {value}')\n",
    "\n",
    "    # Write the modified content back to the lua file\n",
    "    with open(lua_file_path, 'w') as file:\n",
    "        file.write(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89995e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_simulation_call(setting_obj):\n",
    "    \"\"\"\n",
    "    Execute a simulation process in a specific directory and check for errors.\n",
    "\n",
    "    Args:\n",
    "        setting_obj (dict): A dictionary containing various settings. This argument is currently unused in the function.\n",
    "\n",
    "    Returns:\n",
    "        int: A return code indicating success (0) or failure (non-zero).\n",
    "        \n",
    "    Note:\n",
    "        The function currently assumes a fixed simulation path and command.\n",
    "        In case of an error, the function will raise an exception with the simulation's error messages.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"fn_simulation_call\")\n",
    "\n",
    "    # Define the path to the simulation and the command to run it\n",
    "    simulation_path = \"/home/s212597/simmobility\"\n",
    "    command = \"./SimMobility_Medium ./simulation_maas_base_case.xml ./simrun_MidTerm_maas_base_case.xml\"\n",
    "\n",
    "    # Execute the simulation command in the specified directory\n",
    "    with subprocess.Popen(command, cwd=simulation_path, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n",
    "        try:\n",
    "            # Wait for the process to complete and capture any output (stdout and stderr)\n",
    "            stdout, stderr = process.communicate()\n",
    "            \n",
    "            # Uncomment the below lines if you want to print the simulation output (useful for debugging)\n",
    "            #print(f\"Simulation Stdout:\\n{stdout.decode('utf-8')}\")\n",
    "            #print(f\"Simulation Stderr:\\n{stderr.decode('utf-8')}\")\n",
    "            \n",
    "            # If the simulation returns a non-zero exit code, it means an error has occurred\n",
    "            if process.returncode != 0:\n",
    "                raise RuntimeError(\"Simulation terminated with an error!\")\n",
    "        except Exception as e:\n",
    "            # If there's any issue in the above process, capture the error and terminate the simulation process\n",
    "            print(f\"Original Error: {e}\")\n",
    "            process.kill()\n",
    "            print(\"Simulation Error\")\n",
    "            # Raise the captured error to the calling function or user\n",
    "            raise\n",
    "\n",
    "    # Return 0 to indicate a successful function execution\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_process_activities():\n",
    "    \"\"\"\n",
    "    Process activity schedules and return calculated output statistics.\n",
    "\n",
    "    The function reads an activity schedule CSV file and computes output statistics\n",
    "    based on the activity data. It utilizes the `fn_output_od_mode_balance` function\n",
    "    to compute the desired outcomes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing calculated output statistics.\n",
    "        \n",
    "    Note:\n",
    "        This function assumes a global variable `shared_env` that contains \n",
    "        the settings, including the path to the activity schedule CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"fn_process_activities\")\n",
    "    \n",
    "    # Define the expected column names for the activity schedule CSV file\n",
    "    columns = [\"person_id\", \"tour_no\", \"tour_type\", \"stop_no\", \"stop_type\", \"stop_location\", \"stop_zone\", \"stop_mode\", \"primary_stop\", \"arrival_time\", \"departure_time\", \"prev_stop_location\", \"prev_stop_zone\", \"prev_stop_departure_time\", \"pid\"]\n",
    "\n",
    "    # Read the specified CSV file into a pandas DataFrame using the defined column names\n",
    "    activity_schedule = pd.read_csv(shared_env[\"settings\"][\"ACTIVITY_FILE\"], names=columns, usecols=range(len(columns)))\n",
    "\n",
    "    # Calculate the output statistics using the read activity schedule and another function (`fn_output_od_mode_balance`)\n",
    "    output_stats, travel_time_nrmse, total_trips_nrmse, OD_nrmse = fn_output_od_mode_balance(activity_schedule)\n",
    "\n",
    "    # Return the calculated output statistics\n",
    "    return output_stats, travel_time_nrmse, total_trips_nrmse, OD_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_output_od_mode_balance(activity):\n",
    "    \"\"\"\n",
    "    Process the simulation's output activity schedule and calculate the \n",
    "    inadequacy compared to real-world statistics.\n",
    "\n",
    "    The function takes the activity schedule dataframe generated from a \n",
    "    simulation run and calculates two main statistics: the distribution of travel \n",
    "    time and the distribution of trips. These statistics are then compared to \n",
    "    real-world statistics to evaluate the simulation's performance.\n",
    "\n",
    "    Args:\n",
    "        activity (pd.DataFrame): Dataframe containing the activity schedule \n",
    "                                 from the simulation run.\n",
    "\n",
    "    Returns:\n",
    "        float: A weighted root mean squared error indicating the inadequacy \n",
    "               of the simulation compared to real-world statistics.\n",
    "    \"\"\"\n",
    "    print(\"fn_output_od_balance\")\n",
    "    # Extracting the necessary matrixes from the daily activity schedule\n",
    "    # Travel times distribution table: \n",
    "    df_das= activity.copy()\n",
    "    # Calculate travel time for each record\n",
    "    df_das['travel_time'] = df_das['departure_time'] - df_das['arrival_time']\n",
    "\n",
    "    # Group by tour_type and stop_mode, summing the travel times\n",
    "    df_agg = df_das.groupby(['tour_type', 'stop_mode']).travel_time.sum().reset_index()\n",
    "\n",
    "    # Map 'tour_type' and 'stop_mode' values\n",
    "    mode_mapping = {\n",
    "        'Car': 'Car',\n",
    "        'MRT': 'Public',  \n",
    "        'Walk':'Walk', \n",
    "        'BusTravel': 'Public', \n",
    "        'PrivateBus': 'Public', \n",
    "        'Car Sharing 2': 'Car',\n",
    "        'Taxi': 'Car',\n",
    "        'Car Sharing 3': 'Car', \n",
    "        'Motorcycle':'Other', \n",
    "    }\n",
    "    tour_type_mapping = {\n",
    "        'Work': 'Work',\n",
    "        'Education': 'Education',\n",
    "        'Shop': 'Shop',\n",
    "        'Other': 'Other',\n",
    "    }\n",
    "\n",
    "    df_agg['stop_mode'] = df_agg['stop_mode'].map(mode_mapping)\n",
    "    df_agg['tour_type'] = df_agg['tour_type'].map(tour_type_mapping)\n",
    "\n",
    "    # Pivot the dataframe to match the target structure\n",
    "    ttime_distribution = df_agg.pivot_table(index='stop_mode', columns='tour_type', values='travel_time', aggfunc='sum').reset_index()\n",
    "\n",
    "    # Rename the column 'stop_mode' to 'Mode'\n",
    "    ttime_distribution.rename(columns={'stop_mode': 'Mode'}, inplace=True)\n",
    "\n",
    "    # Convert the travel times to percentages based on grand total travel time\n",
    "    grand_total_time = ttime_distribution.iloc[:, 1:].sum().sum()\n",
    "    ttime_distribution.iloc[:, 1:] = (ttime_distribution.iloc[:, 1:].div(grand_total_time) * 100).round(2)\n",
    "    ttime_distribution.fillna(0, inplace=True)\n",
    "    as_ttime_distribution = ttime_distribution.copy()\n",
    "\n",
    "    # Trips distribution table:\n",
    "    df_das['stop_mode'] = df_das['stop_mode'].map(mode_mapping)\n",
    "\n",
    "    # Aggregate the data\n",
    "    df_agg = df_das.groupby(['tour_type', 'stop_mode']).size().reset_index(name='count_trips')\n",
    "\n",
    "    # Pivot the dataframe to match the target structure\n",
    "    ttrips_distribution = df_agg.pivot(index='stop_mode', columns='tour_type', values='count_trips').reset_index()\n",
    "\n",
    "    # Rename the column 'stop_mode' to 'Mode'\n",
    "    ttrips_distribution.rename(columns={'stop_mode': 'Mode'}, inplace=True)\n",
    "\n",
    "    # Compute grand total trips after pivoting\n",
    "    grand_total_trips = ttrips_distribution.iloc[:, 1:].sum().sum()\n",
    "\n",
    "    # Create a copy for percentage calculation\n",
    "    ttrips_distribution_percentage = ttrips_distribution.copy()\n",
    "\n",
    "    # Convert counts into percentages based on grand total trips\n",
    "    for column in ttrips_distribution_percentage.columns:\n",
    "        if column != 'Mode':\n",
    "            ttrips_distribution_percentage[column] = (ttrips_distribution_percentage[column] / grand_total_trips * 100).round(4)\n",
    "\n",
    "    ttrips_distribution_percentage.fillna(0, inplace=True)\n",
    "    as_ttrips_distribution = ttrips_distribution_percentage.copy()\n",
    "\n",
    "    # Load the real world statistics\n",
    "    rw_ttime_distribution = shared_env[\"settings\"][\"CITY_STATS\"][\"trips_distribution\"]\n",
    "    rw_ttrips_distribution = shared_env[\"settings\"][\"CITY_STATS\"][\"time_distribution\"]\n",
    "\n",
    "    # Calculate the Normalize RMSE\n",
    "    travel_time_nrmse = calculate_nrmse(rw_ttime_distribution, as_ttime_distribution)\n",
    "    total_trips_nrmse = calculate_nrmse(rw_ttrips_distribution, as_ttrips_distribution) \n",
    "\n",
    "    # Create Origin-Destination matrix from Daily Activity Schedule\n",
    "    df1 = das_to_od_fn(activity)\n",
    "    OD_nrmse = calculate_nrmse_tod(df1)\n",
    "\n",
    "    # Importance weights (1/3 each)\n",
    "    importance_weights = [1/3, 1/3, 1/3]\n",
    "    weighted_error = importance_weights[0] * travel_time_nrmse + importance_weights[1] * total_trips_nrmse + importance_weights[2] * OD_nrmse\n",
    "    output = weighted_error.copy()\n",
    "\n",
    "    return output, travel_time_nrmse, total_trips_nrmse, OD_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NRMSE\n",
    "def calculate_nrmse(real_df, sim_df):\n",
    "    # Merge on Mode to ensure they are aligned\n",
    "    merged = real_df.merge(sim_df, on=\"Mode\")\n",
    "\n",
    "    # Find min and max\n",
    "    min_df = real_df.select_dtypes(include=['number']).min().min()\n",
    "    max_df = real_df.select_dtypes(include=['number']).max().max()\n",
    "\n",
    "    # Calculate the squared differences\n",
    "    squared_diffs = (merged.filter(like='_x') - merged.filter(like='_y').values)**2\n",
    "    \n",
    "    # Calculate MSE and then RMSE\n",
    "    mse = squared_diffs.mean().mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Calculate the NRMSE, normalized on max and min values\n",
    "    nrmse = rmse / (max_df - min_df)\n",
    "    \n",
    "    return nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NRMSE\n",
    "def calculate_nrmse_tod(real_df):\n",
    "\n",
    "    # Group the DataFrame by the outer level\n",
    "    grouped_df = real_df.groupby(by='ToD')\n",
    "    nrmse_list = []\n",
    "    tot_list = []\n",
    "\n",
    "    # Iterate over the groups\n",
    "    for outer_value, group in grouped_df:\n",
    "\n",
    "        min_df = group['trips'].min()\n",
    "        max_df = group['trips'].max()\n",
    "        count_df = group['trips'].sum()\n",
    "\n",
    "        # Fill potential NaN values\n",
    "        group['trips'] = group['trips'].fillna(0)\n",
    "        group['trips_noise'] = group['trips_noise'].fillna(0)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(group['trips'], group['trips_noise']))\n",
    "\n",
    "        # Calculate the NRMSE, normalized on max and min values\n",
    "        nrmse = rmse / (max_df - min_df)\n",
    "\n",
    "        nrmse_list.append(nrmse)\n",
    "        tot_list.append(count_df)\n",
    "        \n",
    "    result = sum(x * y for x, y in zip(nrmse_list, tot_list)) / sum(tot_list)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def das_to_od_fn(das):\n",
    "\n",
    "    # Reduce and adjust dataset\n",
    "    df = das[[\"stop_mode\", \"tour_type\", \"departure_time\"]]\n",
    "    df['departure_time'] = df['departure_time'] % 24\n",
    "\n",
    "    # Create time of dat column\n",
    "    df[\"ToD\"] = df[\"departure_time\"].apply(lambda row: (\n",
    "        '1' if (row < 300/60 or row >= 1260/60) else\n",
    "        '2' if row < 360/60 else\n",
    "        '3' if row < 420/60 else\n",
    "        '4' if row < 480/60 else\n",
    "        '5' if row < 540/60 else\n",
    "        '6' if row < 900/60 else\n",
    "        '7' if row < 960/60 else\n",
    "        '8' if row < 1020/60 else\n",
    "        '9' if row < 1080/60 else\n",
    "        '10' if row < 1260/60 else\n",
    "        'Unknown'\n",
    "    )) \n",
    "\n",
    "    # Count trips, and map modes\n",
    "    df.loc[:, \"trips\"] = 1\n",
    "    mode_mapping = {'BusTravel': 'PT', 'Car Sharing 2': 'Car', 'Car Sharing 3': 'Car', 'MRT': 'PT', 'Motorcycle' : 'Car', 'PrivateBus': 'PT', 'Taxi': 'Car'}\n",
    "    df['stop_mode'] = df['stop_mode'].replace(mode_mapping)\n",
    "    df = df.groupby(by=[\"ToD\", \"tour_type\", \"stop_mode\"]).agg({\"trips\": 'sum'}).reset_index()\n",
    "\n",
    "    ##### TEMPORARY #####\n",
    "    # Temporary matrix with noise - substitute with the observed data OD matrix from COMPASS\n",
    "    # When the zonal systems are matching (SimMobility-COMPASS)\n",
    "    df_noise = pd.read_csv(\"/home/s212597/BOcalibration/noise_OD.csv\", index_col = \"Unnamed: 0\")\n",
    "    # Observed data is:\n",
    "    #df_obs = pd.read_csv(\"/home/s212597/BOcalibration/COMPASS_OD/COMPASS_OD_4056.csv\", index_col = \"Unnamed: 0\")\n",
    "    \n",
    "    df['trips_noise'] = df_noise['trips']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_update_value_pools(params, base_residuals, param_def):\n",
    "    \"\"\"\n",
    "    Update the pools of residual values in the shared environment, potentially \n",
    "    adding randomness if the simulation is running in 'MOCKING_MODE'.\n",
    "    \n",
    "    This function takes the residuals (or differences) from the base scenario and \n",
    "    adds them to the shared environment's residual pool. If the simulation is \n",
    "    running in 'MOCKING_MODE', random values (following a normal distribution) \n",
    "    are added to these residuals.\n",
    "\n",
    "    Args:\n",
    "        params (dict): Dictionary containing the parameters of the current simulation.\n",
    "        base_residuals (dict): Dictionary containing residuals (differences) from \n",
    "                               the base scenario for each key metric.\n",
    "        param_def (object): An object or structure (not directly used in this \n",
    "                            function) that may contain further details or definitions \n",
    "                            about the parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_update_value_pools\")\n",
    "\n",
    "    # If in MOCKING_MODE, add randomness to the residuals\n",
    "    if setting_obj['MOCKING_MODE']:\n",
    "        for key in base_residuals:\n",
    "            base_residuals[key] = [x + np.random.normal() for x in base_residuals[key]]\n",
    "\n",
    "    # Extract the residual pool and its components from the shared environment\n",
    "    l_residual_object = shared_env['param_space']['residual_pool']\n",
    "    l_residual_components = [x for x in l_residual_object.keys() if x != 'none']\n",
    "\n",
    "    # Update each component in the residual pool with the new residuals\n",
    "    for l_comp in l_residual_components:\n",
    "        shared_env['param_space']['residual_pool'][l_comp] = np.append(l_residual_object[l_comp], base_residuals[l_comp])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7d89973",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3746060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BO functions imports\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.signal import convolve\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.sparse import csc_matrix\n",
    "from typing import Dict, Any, List, Optional\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import multiprocessing\n",
    "import GPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, Formula\n",
    "import inspect\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pyDOE import lhs\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import inspect\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Optional, Any, List\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad680e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def fn_acquisition(type, param_space, **kwargs): \n",
    "    \"\"\"\n",
    "    A function to initiate and return an acquisition function object based on the provided type(function).\n",
    "    \n",
    "    Parameters:\n",
    "    - type (function or class): The type (or class) of the acquisition function to be instantiated.\n",
    "    - param_space (object): The parameter space over which the acquisition function operates.\n",
    "    - **kwargs: Additional keyword arguments to be passed to the acquisition function type during initialization.\n",
    "\n",
    "    Returns:\n",
    "    - object: An instance of the specified acquisition function type, initialized with the provided parameter space and any additional arguments.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print a debug or tracking message\n",
    "    print(\"fn_acquisition_call_type\")\n",
    "    \n",
    "    # Instantiate and return the acquisition function object based on the provided type and arguments\n",
    "    return type(param_space, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_fit_rf(ds, target, method_params=None, test_ds=None, keep_inbag=True):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest regressor on the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - ds (pd.DataFrame or np.array): The training dataset.\n",
    "    - target (str): The name of the target column.\n",
    "    - method_params (dict, optional): Parameters for the Random Forest regressor. \n",
    "        Defaults to {\"RF_NSIZE\": 1, \"RF_NTREE\": 500}.\n",
    "    - test_ds (pd.DataFrame or np.array, optional): The test dataset. If provided, \n",
    "        the performance of the model on this dataset is returned.\n",
    "    - keep_inbag (bool, optional): Whether or not to calculate out-of-bag (OOB) score. \n",
    "        Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the trained model and, if test_ds is provided, \n",
    "        its performance on the test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print function name for debugging or tracking purposes\n",
    "    print(\"fn_fit_rf\")\n",
    "    \n",
    "    # If method_params is not provided, set default values\n",
    "    if method_params is None:\n",
    "        method_params = {\"RF_NSIZE\": 1, \"RF_NTREE\": 500}\n",
    "\n",
    "    # Set default value for 'RF_MTRY' if not provided or if its value is None\n",
    "    if 'RF_MTRY' not in method_params or method_params['RF_MTRY'] is None:\n",
    "        method_params['RF_MTRY'] = int(np.floor(np.sqrt(ds.shape[1])))\n",
    "       \n",
    "    # If the input dataset is one-dimensional, reshape it\n",
    "    if ds.ndim == 1:\n",
    "        ds = ds.reshape(-1, 1)\n",
    "    \n",
    "    # If the input is a NumPy array, convert it to a DataFrame with column names\n",
    "    if isinstance(ds, np.ndarray):\n",
    "        ds = pd.DataFrame(ds, columns=[f'col_{i}' for i in range(ds.shape[1])])    \n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X = ds.drop(target, axis=1).values\n",
    "    y = ds[target].values\n",
    "    \n",
    "    # Initialize the Random Forest regressor with the provided parameters\n",
    "    model = RandomForestRegressor(n_estimators=method_params[\"RF_NTREE\"], \n",
    "                                  min_samples_leaf=method_params[\"RF_NSIZE\"], \n",
    "                                  max_features=method_params[\"RF_MTRY\"], \n",
    "                                  oob_score=keep_inbag)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    output = {\"model\": model}\n",
    "    \n",
    "    # If a test dataset is provided, predict on it and calculate the RMSE\n",
    "    if test_ds is not None:\n",
    "        y_true = test_ds[target].values\n",
    "        X_test = test_ds.drop(target, axis=1).values\n",
    "        y_pred = model.predict(X_test)\n",
    "        output[\"performance\"] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8943ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_utility_ei(traits, optimal_response, complement_u=False, estimated_sigma=None):\n",
    "    \"\"\"\n",
    "    Calculate utility based on Expected Improvement (EI) and possibly other methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - traits (dict): A dictionary containing keys 'mu' (mean) and 'sigma' (standard deviation).\n",
    "    - optimal_response (float): Optimal response value.\n",
    "    - complement_u (bool, optional): Flag indicating whether to use complementary utility functions. Defaults to False.\n",
    "    - estimated_sigma (np.array, optional): An optional value for sigma if it's estimated differently. If provided, it overrides the 'sigma' in traits.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: A matrix of utility values.\n",
    "    \"\"\"\n",
    "    optimal_response = float(optimal_response)\n",
    "    \n",
    "    # Extract mean and standard deviation from the traits dictionary\n",
    "    m = traits['mu']\n",
    "    s = traits['sigma']\n",
    "    \n",
    "    ## If sigma is estimated with other other than empirical estimation\n",
    "    if estimated_sigma is not None:\n",
    "        s = estimated_sigma\n",
    "    \n",
    "    # Set any 0 values in the first column of s to NaN to avoid division by zero\n",
    "    s[s[:, 0] == 0, 0] = np.nan\n",
    "    \n",
    "    # Calculate gamma value which is a scaled difference between optimal response and mean\n",
    "    gamma = (optimal_response - m) / s\n",
    "\n",
    "    # Calculate utility value matrix using the formula for Expected Improvement\n",
    "    u_val_mat = s * (gamma * norm.cdf(gamma) + norm.pdf(gamma))\n",
    "  \n",
    "    #u_val_mat = np.hstack((u_val_mat, np.zeros((u_val_mat.shape[0], 7))))      # useful if multiple methods are tested\n",
    "    \n",
    "    if complement_u:\n",
    "        \n",
    "        u_val_mat = np.hstack((u_val_mat, np.zeros((u_val_mat.shape[0], 7))))  # here it  makes sense \n",
    "        list_gMix = fn_utility_global_gmix(traits, optimal_response)\n",
    "        complement_utilities_fn = [\n",
    "            {'f': fn_utility_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response}},\n",
    "            {'f': fn_utility_gmix_agg_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_ei, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_agg_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_wmax_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}},\n",
    "            {'f': fn_utility_gmix_max_lcb, 'params': {'traits': traits, 'optimal_response': optimal_response, 'list_MM': list_gMix}}\n",
    "        ]\n",
    "              \n",
    "        results = [fn['f'](**fn['params']) for fn in complement_utilities_fn]\n",
    "        c_utilities_val = np.hstack([fn['f'](**fn['params']) for fn in complement_utilities_fn])\n",
    "\n",
    "        # Combine the Expected Improvement utility values with the complementary utility values\n",
    "        u_val_mat = np.hstack((u_val_mat, c_utilities_val))\n",
    "\n",
    "    return u_val_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaleup_standard(ds, param_def):\n",
    "    \"\"\"\n",
    "    Scale up parameters to their original ranges based on the provided parameter definitions.\n",
    "    \n",
    "    Parameters:\n",
    "    - ds (np.array): A NumPy array containing the parameter values to be scaled up.\n",
    "    - param_def (pd.DataFrame): A DataFrame with 'parameter', 'lower_limit', and 'upper_limit' columns, indicating the parameter names and their respective limits.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: Scaled up parameter values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the param_def DataFrame to a dictionary with parameter names as keys for easier access\n",
    "    param_dict = {row['parameter']: {'lower_limit': row['lower_limit'], 'upper_limit': row['upper_limit']} for _, row in param_def.iterrows()}\n",
    "    params_used = [col for col in param_dict.keys()]\n",
    "    input_space_cols = []\n",
    "\n",
    "    for i, col_name in enumerate(params_used):\n",
    "        l_lower = float(param_dict[col_name][\"lower_limit\"])\n",
    "        l_upper = float(param_dict[col_name][\"upper_limit\"])\n",
    "        l_range = l_upper - l_lower\n",
    "\n",
    "        # Rescale the columns from the numpy array\n",
    "        rescaled_col = (ds[:,i] * l_range) + l_lower\n",
    "        input_space_cols.append(rescaled_col)\n",
    "\n",
    "    # Stack arrays in sequence horizontally (column wise)\n",
    "    i_space = np.column_stack(input_space_cols)\n",
    "\n",
    "    return i_space\n",
    "\n",
    "def process_chunk(args):\n",
    "    \"\"\"\n",
    "    Process a chunk of Latin Hypercube Samples (LHS) in parallel. \n",
    "    Each chunk corresponds to a portion of the required sample size, which is divided across available CPU cores.\n",
    "    \n",
    "    Parameters:\n",
    "    - args (tuple): Arguments to process a chunk. It includes chunk id, size, extra space, sample dimensions, and parameter definitions.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: Processed sample for the chunk.\n",
    "    \"\"\"\n",
    "    ch_id, ch_size, ch_extra, sample_dim, definition = args\n",
    "    if ch_id == cpu_count() - 2:                                \n",
    "        ch_size = ch_size - ch_extra\n",
    "\n",
    "    # Added a random seed for each CPU core\n",
    "    np.random.seed()\n",
    "\n",
    "    p_sample = lhs(sample_dim, samples=ch_size)\n",
    "    p_sample = scaleup_standard(p_sample, definition)    \n",
    "    return p_sample\n",
    "\n",
    "def fn_sampling_lhs(vals=None, param_space=None, sample_size=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate a Latin Hypercube Sample (LHS) for a given parameter space.\n",
    "    \n",
    "    Parameters:\n",
    "    - vals (dict, optional): Dictionary containing values.\n",
    "    - param_space (dict): Dictionary with parameter space information including parameter definitions.\n",
    "    - sample_size (int): Number of samples to be generated using LHS.\n",
    "    - **kwargs: Additional keyword arguments, especially 'OPTIMISATION_CORES' to define number of CPU cores.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary with parameter names as keys and their corresponding LHS samples as values.\n",
    "    \"\"\"\n",
    "    print(\"fn_sampling_lhs\")\n",
    "        \n",
    "    kwargs['OPTIMISATION_CORES']=cpu_count() - 2\n",
    "       \n",
    "    \n",
    "    dim_names = param_space[\"definition\"][\"parameter\"]\n",
    "    sample_dim = len(param_space['definition'])\n",
    "    \n",
    "    if 'OPTIMISATION_CORES' in kwargs and kwargs['OPTIMISATION_CORES'] > 1:\n",
    "        chunk_size = int(np.ceil(sample_size / kwargs['OPTIMISATION_CORES']))\n",
    "        last_chunk_extra = chunk_size * kwargs['OPTIMISATION_CORES'] - sample_size\n",
    "\n",
    "        with Pool(processes=kwargs['OPTIMISATION_CORES']) as pool:\n",
    "\n",
    "            prior_sample = pool.map(process_chunk, [(i, chunk_size, last_chunk_extra, sample_dim, param_space['definition']) for i in range(kwargs['OPTIMISATION_CORES'])])\n",
    "        prior_sample = np.concatenate(prior_sample)\n",
    "    else:\n",
    "        prior_sample = lhs(sample_dim, samples=sample_size)\n",
    "        prior_sample = scaleup_standard(prior_sample, param_space['definition'])\n",
    "        \n",
    "    return dict(zip(dim_names, prior_sample.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0fefd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimisation_uniform_prior(vals, sampling_fn, surrogate_predict_fn, surrogate_model, surrogate_utility_fn, iteration_space, param_range, param_space, omit, target, sample_size=1, variation_rate=0.3, opt_parallel=True, opt_cores=cpu_count()-2, opt_store_intermediate=True, is_normalization_required=False, eval_complement=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Optimise a given space using a surrogate model with uniform prior sampling.\n",
    "    \n",
    "    Parameters:\n",
    "    - vals (dict): Initial values to start the optimization process.\n",
    "    - sampling_fn (function): Function to generate a sample from the parameter space.\n",
    "    - surrogate_predict_fn (function): Surrogate prediction function.\n",
    "    - surrogate_model (object): The surrogate model used for prediction.\n",
    "    - surrogate_utility_fn (function): Function to evaluate utility based on surrogate predictions.\n",
    "    - iteration_space (dict): Information about optimal values in the iteration.\n",
    "    - param_range (dict): Ranges of parameters.\n",
    "    - param_space (dict): Space definition of the parameters.\n",
    "    - omit (list): List of parameters to omit.\n",
    "    - target (str): Target column in the sample.\n",
    "    - sample_size (int, optional): Number of samples to generate. Default is 1.\n",
    "    - variation_rate (float, optional): Variation rate for optimization. Default is 0.3.\n",
    "    - opt_parallel (bool, optional): Flag to decide whether to use parallel processing. Default is True.\n",
    "    - opt_cores (int, optional): Number of CPU cores to use for parallel processing. Default is available CPU cores minus 2.\n",
    "    - opt_store_intermediate (bool, optional): Flag to decide whether to store intermediate results. Default is True.\n",
    "    - is_normalization_required (bool, optional): Flag to decide if normalization is required. Default is False.\n",
    "    - eval_complement (bool, optional): Flag to decide if complementary evaluations are required. Default is False.\n",
    "    - **kwargs: Additional keyword arguments for sampling_fn.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Contains the optimized space, utility, and optionally the population samples and their utility values.\n",
    "    \"\"\"\n",
    "    print(\"fn_optimisation_uniform_prior\")\n",
    "\n",
    "    # Generate a prior sample using the provided sampling function\n",
    "\n",
    "    prior_sample = sampling_fn(vals, param_space, sample_size, **kwargs)\n",
    "    prior_names = list(prior_sample.columns)\n",
    "    # If parallel processing is enabled\n",
    "    if opt_parallel:\n",
    "        sample_chunks = np.array_split(prior_sample, opt_cores)\n",
    "        \n",
    "        with Pool(opt_cores) as pool:\n",
    "            # Evaluate the utility for each sample chunk in parallel\n",
    "            eval_targets = pool.map(lambda s_chunk: fn_eval_utility(\n",
    "                s_chunk,\n",
    "                surrogate_predict_fn=surrogate_predict_fn,\n",
    "                surrogate_model=surrogate_model,\n",
    "                surrogate_utility_fn=surrogate_utility_fn,\n",
    "                optimal_value=iteration_space['optimal']['value'],\n",
    "                col_names=list(vals.keys()),\n",
    "                is_normalization_required=is_normalization_required,\n",
    "                param_def=param_space['definition'],\n",
    "                eval_complement=eval_complement\n",
    "            ), sample_chunks)\n",
    "        \n",
    "        eval_target = pd.concat(eval_targets, axis=0)\n",
    "    else:\n",
    "        # If not using parallel processing, evaluate the utility directly\n",
    "        eval_target = fn_eval_utility(\n",
    "            prior_sample,\n",
    "            surrogate_predict_fn=surrogate_predict_fn,\n",
    "            surrogate_model=surrogate_model,\n",
    "            surrogate_utility_fn=surrogate_utility_fn,\n",
    "            optimal_value=iteration_space['optimal']['value'],\n",
    "            col_names=list(vals.keys()),\n",
    "            is_normalization_required=is_normalization_required,\n",
    "            param_def=param_space['definition'],\n",
    "            eval_complement=eval_complement\n",
    "        )\n",
    "    \n",
    "    prior_sample[target] = eval_target.iloc[:, 0].values\n",
    "    \n",
    "    if eval_complement and eval_target.shape[1] > 1:\n",
    "        prior_sample[eval_target.columns] = eval_target.values\n",
    "    \n",
    "    # Get the best value and corresponding set from the prior sample\n",
    "    best_val_idx = prior_sample[target].idxmax()\n",
    "    optimal_set = prior_sample.drop(target, axis=1).iloc[best_val_idx]\n",
    "    \n",
    "    output_obj = {}\n",
    "    output_obj['space'] = optimal_set.to_frame().T\n",
    "    output_obj['space'].columns = list(vals.keys())\n",
    "    output_obj['utility'] = prior_sample[target].iloc[best_val_idx]\n",
    "    \n",
    "    if opt_store_intermediate or eval_complement:\n",
    "        output_obj['pop'] = prior_sample\n",
    "        output_obj['pop_utility'] = prior_sample[target]\n",
    "        \n",
    "        if eval_complement:\n",
    "            output_obj['complementary_utility_fn'] = eval_target.columns.drop([target] + prior_names)\n",
    "    \n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08332fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleton_optimisation(idx, init_grid,lower_bound,upper_bound,surrogate_model,reduced_vals,reduced_param_space):\n",
    "    \"\"\"\n",
    "    Perform optimization for a single starting point (singleton) using the L-BFGS-B method.\n",
    "    \n",
    "    Parameters:\n",
    "    - idx (int): Index of the initial point in the init_grid.\n",
    "    - init_grid (array-like): Initial set of points to start the optimization from.\n",
    "    - lower_bound (list): List of lower bounds for each parameter in the optimization space.\n",
    "    - upper_bound (list): List of upper bounds for each parameter in the optimization space.\n",
    "    - surrogate_model (object): The surrogate model used for making predictions.\n",
    "    - reduced_vals (dict): Dictionary of parameter values after reduction.\n",
    "    - reduced_param_space (dict): Space definition of the parameters after reduction.\n",
    "    \n",
    "    Returns:\n",
    "    - object: An object containing the optimal set of parameters and other optimization info.\n",
    "    \"\"\" \n",
    "    # Convert init_grid to a pandas DataFrame for easy indexing       \n",
    "    init_grid=pd.DataFrame(init_grid)\n",
    "    \n",
    "    # Convert the separate lists of lower and upper bounds into a list of tuples\n",
    "    bounds = list(zip(lower_bound, upper_bound))\n",
    "    \n",
    "    # Use the L-BFGS-B method to optimize the utility function based on the surrogate model\n",
    "    optimal_set = minimize(\n",
    "        fun=fn_eval_utility, \n",
    "        x0=init_grid[idx], \n",
    "        args=(surrogate_predict_fn, surrogate_model, surrogate_utility_fn, iteration_space[\"optimal\"][\"value\"], list(reduced_vals), reduced_param_space[\"definition\"], is_normalization_required),\n",
    "        method=\"L-BFGS-B\",\n",
    "        bounds=bounds,                        #list(zip(lower_bound, upper_bound)),\n",
    "        options={\"maxiter\": 1000}, #changed to 10\n",
    "    )\n",
    "                        \n",
    "    return optimal_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimisation_quasi_newton(\n",
    "    vals, sampling_fn, surrogate_predict_fn, surrogate_model, surrogate_utility_fn, iteration_space,\n",
    "    param_range, param_space, sample_size=1, opt_parallel=True, opt_cores=None,\n",
    "    opt_best_init=False, opt_init_pop_shuffle_rate=0.1, opt_store_intermediate=False, is_normalization_required=False,\n",
    "    **kwargs):\n",
    "    \"\"\"\n",
    "    Perform quasi-Newton optimization using an initial set of points and surrogate models.\n",
    "\n",
    "    Parameters:\n",
    "    - vals (DataFrame): Parameter values.\n",
    "    - sampling_fn (function): Function to sample from the parameter space.\n",
    "    ...\n",
    "    - is_normalization_required (bool): Flag to check if normalization is required before predictions.\n",
    "    - kwargs: Additional keyword arguments for the sampling_fn.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the optimal parameter set and utility value.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_optimisation_quasi_newton\")\n",
    "\n",
    "    # Extract enabled indices based on the 'enabled' column in param_space's 'definition'\n",
    "    enabled_idx = [i for i in range(len(param_space[\"definition\"])) if param_space[\"definition\"][\"enabled\"][i]]\n",
    "\n",
    "    # Reduce the vals and param_space based on the enabled indices\n",
    "    reduced_vals = vals.iloc[:, enabled_idx]\n",
    "    reduced_param_space = {\n",
    "        \"space\": [param_space[\"space\"].iloc[:, i] for i in enabled_idx] + [param_space[\"space\"].iloc[:, -1]],  # Assuming the last column is the target variable (inadequacy)\n",
    "        \"definition\": param_space[\"definition\"].iloc[enabled_idx]\n",
    "    }\n",
    "\n",
    "    # Extract lower and upper bounds from the reduced parameter space\n",
    "    lower_bound = [row[\"lower_limit\"] for _, row in reduced_param_space[\"definition\"].iterrows()]\n",
    "    upper_bound = [row[\"upper_limit\"] for _, row in reduced_param_space[\"definition\"].iterrows()]\n",
    "    param_order = list(reduced_vals.columns)\n",
    "\n",
    "    # Create initial grid using the sampling function\n",
    "    initGrid = list(sampling_fn(reduced_vals, reduced_param_space, sample_size, **kwargs).values())\n",
    "\n",
    "    # If parallel optimization is enabled, optimize each starting point in parallel using the 'singleton_optimisation' function\n",
    "    if opt_parallel:\n",
    "        with Pool(opt_cores or (cpu_count() - 2)) as p:\n",
    "            optimal_sets = p.starmap(\n",
    "                singleton_optimisation,\n",
    "                [(i, initGrid, lower_bound, upper_bound, surrogate_model, reduced_vals, reduced_param_space) for i in range(len(initGrid[0]))]\n",
    "            )\n",
    "    else:\n",
    "        optimal_sets = [singleton_optimisation(i, initGrid, lower_bound, upper_bound, surrogate_model, reduced_vals, reduced_param_space) for i in range(len(initGrid[0]))]\n",
    "\n",
    "    # Filter out optimizations that didn't converge successfully\n",
    "    optimal_sets = [s for s in optimal_sets if s.success]\n",
    "\n",
    "    # Commented visualization block would visualize the optimization results if uncommented\n",
    "    ############################################### Visualizations ##############################################################\n",
    "    ## Extracting utility values from optimal_sets\n",
    "    #utilities = [s.fun for s in optimal_sets]\n",
    "    #\n",
    "    #print(utilities)\n",
    "    #print(np.std(utilities))\n",
    "    #print(np.min(utilities), np.max(utilities))\n",
    "    #print(np.percentile(utilities, [1, 99]))  # 1st and 99th percentiles\n",
    "    #\n",
    "    ## Utility Distribution of Sampled Points:\n",
    "    #plt.figure(figsize=(10, 5))\n",
    "    #sns.histplot(utilities, bins=15, kde=True, color='blue')\n",
    "    #plt.xlabel('Utility Value')\n",
    "    #plt.ylabel('Frequency')\n",
    "    #plt.title('Utility Distribution of Sampled Points')\n",
    "    #plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    #\n",
    "    ## Convergence of Utility Over Runs:\n",
    "    #plt.figure(figsize=(10, 5))\n",
    "    #plt.plot(range(1, len(utilities)+1), utilities, marker='o', linestyle='-', color='b')\n",
    "    #plt.xlabel('Run')\n",
    "    #plt.ylabel('Utility')\n",
    "    #plt.title('Utility Convergence Over Runs')\n",
    "    #plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    #param_values = np.array([s.x for s in optimal_sets])\n",
    "    #\n",
    "    ## Parameter Distribution Over Runs:\n",
    "    #plt.figure(figsize=(10, 5))\n",
    "    #plt.plot(range(1, len(param_values[:, 0])+1), param_values[:, 0], label='Parameter 1', marker='o', linestyle='-', color='blue')\n",
    "    #plt.plot(range(1, len(param_values[:, 1])+1), param_values[:, 1], label='Parameter 2', marker='o', linestyle='-', color='red')\n",
    "    #plt.xlabel('Run')\n",
    "    #plt.ylabel('Parameter Value')\n",
    "    #plt.title('Parameter Values Over Runs')\n",
    "    #plt.legend()\n",
    "    #plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    #\n",
    "    #\n",
    "    ## sns.histplot(utilities, bins=20, kde=True, color='blue')\n",
    "    #\n",
    "    ################################################################################################\n",
    "    # Identify the best result(s) from the optimal_sets\n",
    "    best_vals = np.array([s.fun for s in optimal_sets])\n",
    "    best_val_indices = np.where(best_vals == np.max(best_vals))\n",
    "\n",
    "    # Extract the index of the optimal set (if multiple, use the first one)\n",
    "    best_val_idx = best_val_indices[0][0] if len(best_val_indices[0]) > 1 else best_val_indices[0]\n",
    "\n",
    "    # Ensure 'best_val_idx' is an integer\n",
    "    best_val_idx = best_val_idx.item() if isinstance(best_val_idx, np.ndarray) and best_val_idx.size == 1 else best_val_idx # Removed: [0]\n",
    "\n",
    "    optimal_set = optimal_sets[best_val_idx]\n",
    "\n",
    "    # Create the output object and store results\n",
    "    output_obj = {\n",
    "        \"space\": [param_space[\"definition\"][\"initial\"][i] for i in range(len(param_space[\"definition\"]))],  # Can also use 'vals' variable\n",
    "        \"utility\": optimal_set.fun\n",
    "    }\n",
    "\n",
    "    for i, index in enumerate(enabled_idx):\n",
    "        output_obj[\"space\"][index] = optimal_set.x[i]\n",
    "\n",
    "    output_obj[\"space\"] = np.array(output_obj[\"space\"]).reshape(1, -1)\n",
    "\n",
    "    # If opt_store_intermediate is True, store the parameter values and utility values of all the optimal sets\n",
    "    if opt_store_intermediate:\n",
    "        output_obj[\"pop\"] = np.vstack([s.x for s in optimal_sets])\n",
    "        output_obj[\"pop_utility\"] = best_vals\n",
    "\n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_pull_optimal_selection(space, randomized_selection=False, selection_size=0):\n",
    "    \"\"\"\n",
    "    Selects a subset of rows from a given DataFrame (space). \n",
    "    Depending on the given parameters, this selection can be either random or sequential.\n",
    "    \n",
    "    Args:\n",
    "        space (pd.DataFrame): The DataFrame from which rows are to be selected.\n",
    "        randomized_selection (bool, optional): If True, rows are selected in random order.\n",
    "                                               If False, rows are selected in their existing order. \n",
    "                                               Defaults to False.\n",
    "        selection_size (int, optional): The number of rows to select. If <= 0 or >= number of \n",
    "                                        rows in space, the entire space is returned.\n",
    "                                        Defaults to 0, which implies returning the entire space.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing two items:\n",
    "              - 'space': a subset of the original DataFrame.\n",
    "              - 'idx': the indices of the selected rows.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_pull_optimal_selection\")\n",
    "\n",
    "    # Retrieve the default value for the selection_size parameter from the function's signature\n",
    "    selection_size = inspect.signature(fn_pull_optimal_selection).parameters['selection_size'].default\n",
    "    \n",
    "    # Get the total number of rows in the DataFrame and create a sequence of indices\n",
    "    space_nrow = len(space)\n",
    "    idx = np.arange(space_nrow)\n",
    "    \n",
    "    # If randomized selection is requested, permute the indices and reorder the DataFrame accordingly\n",
    "    if randomized_selection:\n",
    "        idx = np.random.permutation(space_nrow)\n",
    "        space = space.iloc[idx, :]\n",
    "    \n",
    "    # If the selection size is outside valid bounds, return the entire DataFrame and its indices\n",
    "    if selection_size <= 0 or selection_size >= space_nrow:\n",
    "        return {'space': space, 'idx': idx}\n",
    "    \n",
    "    # Otherwise, return the desired subset of the DataFrame and its corresponding indices\n",
    "    return {'space': space.iloc[:selection_size, :], 'idx': idx[:selection_size]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_termination_max_iterations(iteration, space, max_iterations):\n",
    "    \"\"\"\n",
    "    Determines if a given iteration exceeds the maximum allowed iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    - iteration (int): The current iteration number.\n",
    "    - space (DataFrame or other data type): The search space or state information (not used in this function but kept for uniformity).\n",
    "    - max_iterations (int): The maximum number of iterations allowed.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the current iteration exceeds the maximum allowed iterations, otherwise False.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"fn_termination_max_iterations\")\n",
    "    \n",
    "    # Return True if current iteration is greater than the allowed maximum, otherwise return False\n",
    "    return iteration > max_iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fn_resolve_fn_params(fn, omit=[], **kwargs):\n",
    "    \"\"\"\n",
    "    Resolves the parameters of the target function `fn` by matching them with the provided keyword arguments. \n",
    "    If a parameter isn't provided, it will be assigned a default value of None.\n",
    "    Parameters can also be omitted from the final resolved list based on the provided `omit` argument.\n",
    "    \n",
    "    Args:\n",
    "        fn (function): The target function for which parameters need to be resolved.\n",
    "        omit (list, optional): A list of parameters (either names as strings or indices) \n",
    "                               that should be omitted from the final resolved list.\n",
    "                               Defaults to an empty list.\n",
    "        **kwargs: Variable length keyword arguments that will be used to resolve the target function's parameters.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are parameter names and values are the resolved values for those parameters.\n",
    "              Parameters not provided in **kwargs will have a value of None.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_resolve_fn_params\")\n",
    "\n",
    "    # Extract the list of parameters from the function's signature\n",
    "    args_list = inspect.signature(fn).parameters\n",
    "\n",
    "    # Retrieve the dictionary of parameters passed to this function\n",
    "    params_list = kwargs\n",
    "\n",
    "    # If indices are provided in the omit list, convert them to parameter names\n",
    "    if all(isinstance(i, int) for i in omit): \n",
    "        omit = [list(args_list.keys())[i] for i in omit]\n",
    "\n",
    "    # Remove the parameters specified in the omit list\n",
    "    args_list = {k: v for k, v in args_list.items() if k not in omit}\n",
    "\n",
    "    # If the resulting list is empty, return it directly\n",
    "    if len(args_list) == 0:\n",
    "        return args_list\n",
    "\n",
    "    # Resolve values for the remaining parameters\n",
    "    for arg in args_list:\n",
    "        if arg in params_list:\n",
    "            args_list[arg] = params_list[arg]\n",
    "        else:\n",
    "            args_list[arg] = None  # assign a default value of None if not provided\n",
    "\n",
    "    return args_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_pull_optimal_samples(space, target, objective_fn=min, selection_fn=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Selects optimal samples based on a given target and optional objective function.\n",
    "    \n",
    "    Args:\n",
    "        space (pd.DataFrame): The parameter space (dataframe) from which samples are selected.\n",
    "        target (str): The column name in the `space` dataframe to be arranged with and considered for optimization.\n",
    "        objective_fn (function, optional): A function that takes in a list and returns the optimal value \n",
    "                                           based on a certain criterion. Defaults to the `min` function.\n",
    "        selection_fn (function, optional): A function that will further refine the selected samples after \n",
    "                                          initial optimization. This can be used to select a subset of samples, \n",
    "                                          or apply randomization etc. Defaults to None.\n",
    "        **kwargs: Additional keyword arguments to be passed to the selection function.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'idx': List of index positions of the optimal samples in the original space dataframe.\n",
    "            - 'space': A dataframe containing the optimal samples.\n",
    "            - 'value': The optimal value computed using the objective function.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"fn_pull_optimal_samples\")\n",
    "\n",
    "    # Convert the target column to numeric, and handle any conversion errors by filling with zero\n",
    "    space[target] = pd.to_numeric(space[target], errors='coerce').fillna(0)\n",
    "\n",
    "    # Identify the indexes of the rows in the dataframe that have the minimum value of the target column\n",
    "    best_performer = {}\n",
    "    best_performer['idx'] = space[space[target] == space[target].min()].index.tolist()\n",
    "    best_performer['space'] = space.iloc[best_performer['idx']]\n",
    "\n",
    "    # If a selection function is provided, apply it to further refine the optimal samples\n",
    "    if selection_fn is not None:\n",
    "        selected_space = selection_fn(best_performer['space'], **kwargs)\n",
    "        best_performer['space'] = selected_space['space']\n",
    "        best_performer['idx'] = [best_performer['idx'][i] for i in selected_space['idx']]\n",
    "\n",
    "    # Calculate the best value using the provided objective function\n",
    "    best_performer['value'] = objective_fn(best_performer['space'][target].tolist())\n",
    "    \n",
    "    return best_performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_reference_sample(sps, target, ref_type=0,  **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a reference sample from the provided parameter space based on a specified criterion.\n",
    "    \n",
    "    Args:\n",
    "        sps (pd.DataFrame or dict): The parameter space from which the reference sample is selected.\n",
    "        target (str): The column name in the `sps` data to be used for the reference selection.\n",
    "        ref_type (int, optional): Specifies the criterion for selecting the reference sample. \n",
    "                                  Allowed values:\n",
    "                                  - 0: Select the sample with the minimum value in the target column ('optimal').\n",
    "                                  - 1: Select the last sample in the dataframe ('last').\n",
    "                                  Defaults to 0.\n",
    "        **kwargs: Additional keyword arguments (not used in the current function but can be utilized for extensions).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing the reference sample. If no sample meets the criterion, returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"fn_reference_sample\")\n",
    "    \n",
    "    # Check if the provided ref_type is one of the allowed values\n",
    "    assert ref_type in [0, 1], \"Chosen type of a reference sample point is not correct. Allowed values are either 0 ('optimal') or 1 ('last').\"\n",
    "    \n",
    "    # Convert the input to a DataFrame\n",
    "    sps = pd.DataFrame(sps)\n",
    "    \n",
    "    # Ensure there are samples in the dataframe\n",
    "    if sps.shape[0] > 0:\n",
    "        if ref_type == 0:\n",
    "            # Return the sample with the minimum value in the target column\n",
    "            return sps.loc[[sps[target].astype(float).idxmin()]]\n",
    "        elif ref_type == 1:\n",
    "            # Return the last sample in the dataframe\n",
    "            return sps.tail(n=1)\n",
    "\n",
    "    # Return None if no suitable sample was found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_scaleup_standard(ds, param_def):\n",
    "    \"\"\"\n",
    "    Scales up a given dataset based on provided parameter definitions.\n",
    "\n",
    "    This function takes in a dataset `ds` and scales its values based on defined\n",
    "    lower and upper limits from `param_def`. The resulting values represent \n",
    "    a scaled-up input space in the defined bounds. Any parameters not present \n",
    "    in `param_def` are retained as is.\n",
    "\n",
    "    Args:\n",
    "        ds (pd.Series or pd.DataFrame): The dataset to be scaled up. If the dataset \n",
    "                                        is a Series, it's transformed into a DataFrame.\n",
    "        param_def (dict or pd.DataFrame): Parameter definitions containing model parameters \n",
    "                                          and their respective lower and upper bounds.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A scaled-up input space with values within the defined parameter bounds.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"fn_scaleup_standard\")\n",
    "    \n",
    "    # Convert the input dataset to DataFrame if it's a Series\n",
    "    if ds.ndim == 1:\n",
    "        ds = pd.DataFrame(ds).T\n",
    "        ds.columns = param_def['parameter']\n",
    "    \n",
    "    # Convert param_def dict to DataFrame with the specific columns\n",
    "    param_def = pd.DataFrame(param_def, columns=['model', 'param', 'module', 'include', 'initial', 'lower_limit',\n",
    "       'upper_limit', 'provided_initial', 'param_name', 'declaration',\n",
    "       'param_name_left_hand', 'parameter', 'direct_influence', 'enabled',\n",
    "       'tunnel_width', 'changed', 'init_lower_limit', 'init_upper_limit'])\n",
    "    \n",
    "    # Ensure the dataset is in the right shape\n",
    "    ds = pd.DataFrame(ds).T\n",
    "    ds.columns = param_def['parameter']\n",
    "        \n",
    "    # Extract relevant parameters and their bounds\n",
    "    param_mat = param_def[['parameter', 'lower_limit', 'upper_limit']].set_index('parameter')\n",
    "    params_used = list(set(ds.columns).intersection(set(param_mat.index)))\n",
    "    params_omitted = list(set(ds.columns).difference(set(params_used)))\n",
    "    \n",
    "    input_space_cols = []\n",
    "    # Scale each parameter based on its defined bounds\n",
    "    for col_name in params_used:\n",
    "        l_lower = param_mat.loc[col_name, 'lower_limit']\n",
    "        l_upper = param_mat.loc[col_name, 'upper_limit']\n",
    "        l_range = l_upper - l_lower\n",
    "        input_space_cols.append(((ds[col_name] * l_range) + l_lower).values)\n",
    "        \n",
    "    # Combine the scaled parameters\n",
    "    i_space = np.column_stack(input_space_cols)\n",
    "    colnames = params_used\n",
    "    \n",
    "    # Retain parameters that were not scaled\n",
    "    if len(params_omitted) > 0:\n",
    "        i_space = np.column_stack((i_space, ds[params_omitted].values))\n",
    "        colnames += params_omitted\n",
    "        \n",
    "    # Convert the array back to a DataFrame\n",
    "    i_space = pd.DataFrame(i_space, columns=colnames)\n",
    "    \n",
    "    return i_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ee7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimal_acquisition(param_space, iter_space=None, sample_size=1, omit=[], \n",
    "                           inner_optimisation_fn=fn_optimisation_uniform_prior, \n",
    "                           sampling_fn=fn_sampling_lhs, \n",
    "                           surrogate_predict_fn=None, \n",
    "                           surrogate_utility_fn=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Determines the optimal acquisition based on provided parameter space and other configurations.\n",
    "\n",
    "    This function makes use of surrogate models, sampling functions, and other parameters\n",
    "    to obtain an optimal point in the given parameter space.\n",
    "\n",
    "    Args:\n",
    "        param_space (dict): The parameter space to be optimized. \n",
    "                            Should contain 'space' and 'definition' keys.\n",
    "        iter_space (dict, optional): Information on iteration and surrogate model.\n",
    "        sample_size (int, optional): Number of samples to be drawn. Default is 1.\n",
    "        omit (list, optional): List of parameter names to be omitted from the reference sample.\n",
    "        inner_optimisation_fn (function, optional): The optimization function used internally. \n",
    "                                                    Default is 'fn_optimisation_uniform_prior'.\n",
    "        sampling_fn (function, optional): The sampling function to be used. Default is 'fn_sampling_lhs'.\n",
    "        surrogate_predict_fn (function, optional): Function to predict using surrogate model.\n",
    "        surrogate_utility_fn (function, optional): Function to calculate utility using surrogate model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Optimal parameter space.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"fn_optimal_acquisition\")\n",
    "\n",
    "    # Ensuring provided functions are callable\n",
    "    assert callable(inner_optimisation_fn), \"Provided optimisation function is not a valid function!\"\n",
    "    assert callable(sampling_fn), \"Provided sampling function is not a valid function!\"\n",
    "    \n",
    "    # Extract the parameter space\n",
    "    sps = param_space['space']\n",
    "\n",
    "    # Set bounds based on the first parameter in param_space or default to [-10, 10]\n",
    "    if param_space['definition'].shape[0] > 0:\n",
    "        lower_limit = float(param_space['definition'][\"lower_limit\"][0])\n",
    "        upper_limit = float(param_space['definition'][\"upper_limit\"][0])\n",
    "    else:\n",
    "        lower_limit = -10\n",
    "        upper_limit = 10\n",
    "\n",
    "    # Obtain reference sample based on optimal (ref_type=0) for mutation\n",
    "    ref_param_instance = fn_reference_sample(sps, target, ref_type=0, **kwargs)\n",
    "\n",
    "    # Drop any parameters specified to be omitted\n",
    "    if omit:\n",
    "        ref_param_instance = ref_param_instance.drop(columns=omit)\n",
    "        \n",
    "    # If 'vals' is present in kwargs, remove it\n",
    "    if 'vals' in kwargs:\n",
    "        kwargs.pop('vals')\n",
    "\n",
    "    # Run the inner optimization function to obtain the optimal parameter space\n",
    "    optimal_ps = inner_optimisation_fn(vals=ref_param_instance, \n",
    "                                       sampling_fn=sampling_fn,\n",
    "                                       surrogate_predict_fn=surrogate_predict_fn, \n",
    "                                       surrogate_model=iter_space['surrogate']['model'], \n",
    "                                       surrogate_utility_fn=surrogate_utility_fn, \n",
    "                                       iteration_space=iter_space, \n",
    "                                       param_range={\"lower_limit\": lower_limit, \"upper_limit\": upper_limit},\n",
    "                                       param_space=param_space,\n",
    "                                       sample_size=sample_size,\n",
    "                                       **kwargs)    \n",
    "\n",
    "    return optimal_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48687c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_fit(type_fn, ds, target, param_def, is_normalization_required=False, method_params=None, \n",
    "           cv_parallel=True, cv_cores=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Function to fit machine learning models based on given settings.\n",
    "\n",
    "    Args:\n",
    "        type_fn (function): The model training function.\n",
    "        ds (DataFrame): Dataset to use for training.\n",
    "        target (str): Name of the target column in the dataset.\n",
    "        param_def (dict or DataFrame): Definitions and settings for the parameters.\n",
    "        is_normalization_required (bool): Whether to normalize the dataset.\n",
    "        method_params (dict, optional): Parameters for the type_fn function.\n",
    "        cv_parallel (bool): Whether to parallelize the cross-validation process.\n",
    "        cv_cores (int, optional): Number of cores to use for parallelized CV. \n",
    "                                  Default to the number of CPU cores minus 2.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing models and corresponding indices.\n",
    "    \"\"\"\n",
    "    print(\"fn_fit\")\n",
    "    col_idx_target = ds.columns.get_loc(target)\n",
    "    method_params = inspect.signature(fn_fit).parameters['method_params'].default\n",
    "\n",
    "    # Filter dataset columns based on the 'enabled' flag in param_def\n",
    "    if 'enabled' in param_def:\n",
    "        print(\"Apply filtering in accordance to the enabled feature\")\n",
    "        enabled_idx = param_def[param_def['enabled']].index.tolist() + [col_idx_target]\n",
    "        ds = ds.iloc[:, enabled_idx]\n",
    "        col_idx_target = len(enabled_idx) - 1\n",
    "\n",
    "    # Apply normalization if required\n",
    "    if is_normalization_required:\n",
    "        ds = fn_normalize_min_max(ds, param_def)\n",
    "        \n",
    "    # Handle multiple method_params (cross-validation on hyperparameter grid)\n",
    "    if method_params is not None and len(method_params) > 1:\n",
    "        method_params_expanded = list(product(*method_params.values()))\n",
    "        \n",
    "        if len(method_params_expanded) > 1:\n",
    "            # Create cross-validation folds\n",
    "            cv_datasets = fn_do_folding(ds)\n",
    "            \n",
    "            # Parallelize the cross-validation process if enabled\n",
    "            if cv_parallel:\n",
    "                with Pool(cv_cores or (cpu_count() - 2)) as p:\n",
    "                    method_params_performance = p.starmap(\n",
    "                        fit_and_evaluate_model, \n",
    "                        [(type_fn, train, target, method_param, test) for train, test in cv_datasets for method_param in method_params_expanded]\n",
    "                    )\n",
    "            else:\n",
    "                method_params_performance = [\n",
    "                    fit_and_evaluate_model(type_fn, train, target, method_param, test)\n",
    "                    for train, test in cv_datasets for method_param in method_params_expanded\n",
    "                ]\n",
    "\n",
    "            mean_performance = np.mean([mp['performance'] for mp in method_params_performance])\n",
    "            best_performer_idx = np.argmin(mean_performance)\n",
    "            best_method_params = method_params_expanded[best_performer_idx]\n",
    "\n",
    "            list_models = [\n",
    "                {\n",
    "                    'model': type_fn(ds.iloc[:, np.append(sub_ds_idx, col_idx_target)], target, method_params=best_method_params),\n",
    "                    'sub_ds_idx': sub_ds_idx\n",
    "                }\n",
    "                for sub_ds_idx in fn_split_space(ds)\n",
    "            ]\n",
    "            return list_models\n",
    "    \n",
    "    # Handle single method_params\n",
    "    list_models = [\n",
    "        {\n",
    "            'model': type_fn(ds.iloc[:, np.append(sub_ds_idx, col_idx_target)], target, method_params=method_params),\n",
    "            'sub_ds_idx': sub_ds_idx\n",
    "        }\n",
    "        for sub_ds_idx in fn_split_space(ds)\n",
    "    ]\n",
    "    return list_models\n",
    "\n",
    "\n",
    "def fit_and_evaluate_model(type_fn, train_ds, target, method_params, test_ds):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model using the given training dataset and method parameters.\n",
    "\n",
    "    Args:\n",
    "        type_fn (function): The model training function.\n",
    "        train_ds (DataFrame): Training dataset.\n",
    "        target (str): Name of the target column.\n",
    "        method_params (dict): Parameters for the type_fn function.\n",
    "        test_ds (DataFrame): Test dataset for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing performance and trained model.\n",
    "    \"\"\"\n",
    "    print(\"fit_and_evaluate_model\")\n",
    "    model = type_fn(train_ds, target, method_params=method_params)\n",
    "    performance = evaluate_model(model, test_ds, target)  \n",
    "    return {'performance': performance, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8765ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, test_ds, target):\n",
    "    \"\"\"\n",
    "    Evaluates the model using Mean Squared Error (MSE).\n",
    "\n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    test_ds: Test dataset (Excluding the target variable)\n",
    "    target: Actual target variable from the test dataset\n",
    "\n",
    "    Returns:\n",
    "    float: Mean squared error of the model on the test dataset\n",
    "    \"\"\"\n",
    "    # Generate predictions for the test dataset\n",
    "    predictions = model.predict(test_ds)\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mse = mean_squared_error(target, predictions)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aef15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_normalize_min_max(ds, param_def):\n",
    "    \"\"\"\n",
    "    Normalize the input dataset columns based on min-max normalization using limits provided in param_def.\n",
    "\n",
    "    Args:\n",
    "        ds (DataFrame or Series): Input dataset to be normalized.\n",
    "        param_def (DataFrame): Parameter definitions containing 'parameter', 'lower_limit', and 'upper_limit' \n",
    "                               columns which define the limits for normalization.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Normalized dataset where the columns mentioned in param_def are scaled between 0 and 1.\n",
    "\n",
    "    Note:\n",
    "        Any columns in the input dataset that aren't mentioned in param_def are returned as-is without normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"fn_normalize_min_max\")\n",
    "\n",
    "    # Convert a Series input into a DataFrame\n",
    "    if ds.ndim == 1:\n",
    "        ds = pd.DataFrame(ds).T\n",
    "\n",
    "    # Extract parameter information for normalization\n",
    "    param_mat = param_def[['parameter', 'lower_limit', 'upper_limit']].set_index('parameter')\n",
    "\n",
    "    # Identify columns in the dataset that have normalization information\n",
    "    params_used = list(set(param_mat.index) & set(ds.columns))\n",
    "    \n",
    "    # Identify columns in the dataset that don't have normalization information\n",
    "    params_omitted = list(set(ds.columns) - set(params_used))\n",
    "\n",
    "    input_space_cols = []\n",
    "\n",
    "    # Normalize columns using min-max normalization\n",
    "    for col_name in params_used:\n",
    "        col = ds[col_name]\n",
    "        col_min = param_mat.at[col_name, 'lower_limit']\n",
    "        col_max = param_mat.at[col_name, 'upper_limit']\n",
    "        normalized_col = (col - col_min) / (col_max - col_min)\n",
    "        input_space_cols.append(normalized_col)\n",
    "\n",
    "    # Combine normalized columns into a DataFrame\n",
    "    input_space = pd.concat(input_space_cols, axis=1)\n",
    "    input_space.columns = params_used\n",
    "\n",
    "    # If there are columns that weren't normalized, add them to the output unchanged\n",
    "    if len(params_omitted) > 0:\n",
    "        ds_omitted = ds[params_omitted]\n",
    "        input_space = pd.concat([input_space, ds_omitted], axis=1)\n",
    "        \n",
    "    return input_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_split_space(ds):\n",
    "    \"\"\"\n",
    "    Splits the dataset into multiple subspaces based on configuration settings.\n",
    "    \n",
    "    Args:\n",
    "        ds (DataFrame): The dataset that needs to be divided into subspaces.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of integer arrays representing column indices for each subspace.\n",
    "\n",
    "    Note:\n",
    "        The function uses configuration settings from the shared_env global variable to \n",
    "        determine the number of subspaces, their sizes, and whether they should overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"fn_split_space\")\n",
    "    \n",
    "    # Default number of subspaces\n",
    "    no_subspaces = 1\n",
    "    \n",
    "    # If defined in settings, update the number of subspaces\n",
    "    if \"SUBSPACES_NUMBER\" in shared_env[\"settings\"]:\n",
    "        no_subspaces = shared_env[\"settings\"][\"SUBSPACES_NUMBER\"]\n",
    "    \n",
    "    # Default size of each subspace\n",
    "    size_subspace = int(np.ceil((ds.shape[1]-1)/no_subspaces))\n",
    "    \n",
    "    # If defined in settings, update the size of each subspace based on the proportion provided\n",
    "    if \"SUBSPACES_SIZE\" in shared_env[\"settings\"]:\n",
    "        size_subspace = int(np.ceil((ds.shape[1]-1) * shared_env[\"settings\"][\"SUBSPACES_SIZE\"]))\n",
    "        \n",
    "    # Default behavior is that subspaces do not overlap\n",
    "    overlap_subspaces = False\n",
    "    if \"SUBSPACES_OVERLAP\" in shared_env[\"settings\"]:\n",
    "        overlap_subspaces = shared_env[\"settings\"][\"SUBSPACES_OVERLAP\"]\n",
    "        \n",
    "    # Check if overlapping is mandatory due to size mismatch\n",
    "    if (ds.shape[1]-1) < (no_subspaces * size_subspace):\n",
    "        overlap_subspaces = True\n",
    "        \n",
    "    output = []\n",
    "\n",
    "    # Generate subspaces with overlapping columns\n",
    "    if overlap_subspaces:\n",
    "        for idx in range(no_subspaces):\n",
    "            col_idx = np.sort(np.random.choice(ds.shape[1]-1, size_subspace, replace=False))\n",
    "            output.append(col_idx)\n",
    "\n",
    "    # Generate subspaces without overlapping columns\n",
    "    else:\n",
    "        col_set = np.arange(ds.shape[1]-1).astype(int)\n",
    "        for i in range(no_subspaces):\n",
    "            gen_vector = np.sort(np.random.choice(col_set, size_subspace, replace=False))\n",
    "            output.append(gen_vector)\n",
    "            \n",
    "            # Remove already selected columns from the potential list for next iteration\n",
    "            col_set = np.setdiff1d(col_set, gen_vector)\n",
    "  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_eval_utility(ds, surrogate_predict_fn, surrogate_model, surrogate_utility_fn, optimal_value, col_names, param_def, direction=1, is_normalization_required=False, eval_complement=False):\n",
    "    \"\"\"\n",
    "    Evaluates the utility of the provided dataset based on surrogate predictions.\n",
    "    \n",
    "    Args:\n",
    "        ds (array-like): Input data to be evaluated.\n",
    "        surrogate_predict_fn (callable): Function used to make predictions using the surrogate model.\n",
    "        surrogate_model (object): The surrogate model instance.\n",
    "        surrogate_utility_fn (callable): Function to compute utility based on surrogate predictions.\n",
    "        optimal_value (float): Target or optimal value to compare against.\n",
    "        col_names (list): List of column names for the dataset.\n",
    "        param_def (DataFrame): Parameter definitions used for further processing or transformations.\n",
    "        direction (int, optional): Direction of optimization. Defaults to 1.\n",
    "        is_normalization_required (bool, optional): Whether normalization is required before prediction. Defaults to False.\n",
    "        eval_complement (bool, optional): Whether to evaluate the complement utility. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        float: Computed utility value.\n",
    "        \n",
    "    Note:\n",
    "        This function evaluates the utility of a dataset based on predictions from a surrogate model. \n",
    "        The surrogate model provides an approximation of the actual underlying function, which can be \n",
    "        computationally expensive to evaluate. The computed utility guides the optimization process in \n",
    "        selecting the next sample point.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the dataset has 2 dimensions\n",
    "    if ds.ndim == 1:\n",
    "        ds = np.expand_dims(ds, 0)\n",
    "    ds = pd.DataFrame(ds, columns=col_names)\n",
    "    \n",
    "    # Get surrogate model predictions for the dataset\n",
    "    surrogate_predictions = fn_predict(surrogate_predict_fn, surrogate_model, ds, param_def=param_def)\n",
    "    \n",
    "    # Assemble traits or characteristics from the predictions\n",
    "    surrogate_traits = fn_assemble_prediction_traits(surrogate_predictions)\n",
    "    \n",
    "    # Calculate utility based on surrogate predictions and provided utility function\n",
    "    s_utility = surrogate_utility_fn(traits=surrogate_traits, optimal_response=optimal_value, complement_u=eval_complement)\n",
    "    \n",
    "    return s_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict(type, surrogate_model, ds, param_def=None, is_normalization_required=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Predict the output for a dataset based on a given surrogate model.\n",
    "    \n",
    "    Args:\n",
    "        type (callable): Prediction function type.\n",
    "        surrogate_model (list of dicts): List of surrogate models along with sub-dataset indices.\n",
    "        ds (DataFrame): Input dataset for which predictions are required.\n",
    "        param_def (DataFrame, optional): Parameter definitions used for further processing or transformations.\n",
    "        is_normalization_required (bool, optional): Whether normalization is required before prediction. Defaults to False.\n",
    "        **kwargs: Additional arguments to be passed to the prediction function.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the predicted outputs organized by headers.\n",
    "        \n",
    "    Note:\n",
    "        This function utilizes the surrogate model (an approximation of the actual function) to predict values for\n",
    "        the given dataset `ds`. The surrogate model consists of sub-models, each responsible for a subset of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform surrogate_model to only include 'model' and 'sub_ds_idx' keys for ease of use later\n",
    "    new_data = []\n",
    "    for item in surrogate_model:\n",
    "        new_data.append({\n",
    "            'model': item['model']['model'],\n",
    "            'sub_ds_idx': item['sub_ds_idx']\n",
    "        })\n",
    "    surrogate_model = new_data\n",
    "    \n",
    "    # Update the surrogate model's structure for easier access\n",
    "    surrogate_model = [[sub_model['model'], sub_model['sub_ds_idx']] for sub_model in surrogate_model]\n",
    "\n",
    "    # Filter dataset columns based on the 'enabled' feature in param_def\n",
    "    if param_def is not None and \"enabled\" in param_def:\n",
    "        enabled_idx = np.where(param_def[\"enabled\"] == True)[0]\n",
    "        ds = ds.iloc[:, enabled_idx]\n",
    "    \n",
    "    # Normalize dataset if required\n",
    "    if is_normalization_required and param_def is not None:\n",
    "        ds = fn_normalize_min_max(ds, param_def)\n",
    "    \n",
    "    # Predict using each surrogate sub-model on its respective sub-dataset\n",
    "    list_predictions = [type(l_model_subspace[0], ds.iloc[:, l_model_subspace[1]], **kwargs) for l_model_subspace in surrogate_model]\n",
    "\n",
    "    # Organize the predictions based on headers\n",
    "    output_headers = list(list_predictions[0].keys())\n",
    "    output = {h: [prediction[h] for prediction in list_predictions] for h in output_headers}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2479448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict_rf(surrogate_model, ds):\n",
    "    \"\"\"\n",
    "    Predict the output for a dataset using a RandomForestRegressor surrogate model.\n",
    "    \n",
    "    Args:\n",
    "        surrogate_model (RandomForestRegressor): A trained RandomForest surrogate model.\n",
    "        ds (DataFrame): Input dataset for which predictions are required.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the predicted values ('mu'), the standard deviation \n",
    "              of predictions from individual trees ('sigma'), and the individual predictions\n",
    "              from all the trees in the forest ('pred').\n",
    "        \n",
    "    Note:\n",
    "        The RandomForestRegressor consists of multiple DecisionTreeRegressors. This function \n",
    "        not only returns the aggregated prediction (average across all trees) but also \n",
    "        provides predictions from each individual tree. The standard deviation of these \n",
    "        individual predictions can be useful to measure the uncertainty or variability of the predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the aggregated prediction from the RandomForest\n",
    "    pred = surrogate_model.predict(ds)\n",
    "\n",
    "    # Extract individual predictions from each DecisionTreeRegressor within the RandomForest\n",
    "    individual_predictions = np.array([tree.predict(ds) for tree in surrogate_model.estimators_])\n",
    "\n",
    "    result = {\n",
    "        'pred': individual_predictions,  # Individual tree predictions\n",
    "        'mu': pred,                      # Mean prediction\n",
    "        'sigma': np.std(individual_predictions, axis=0),  # Standard deviation of tree predictions\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_assemble_prediction_traits(prediction_obj):\n",
    "    \"\"\"\n",
    "    Assemble and structure the traits of a given prediction object.\n",
    "    \n",
    "    Args:\n",
    "        prediction_obj (dict): A dictionary containing predicted values, standard deviations,\n",
    "                               and potentially other custom prediction properties.\n",
    "                               \n",
    "    Returns:\n",
    "        dict: A dictionary that contains aggregated and structured prediction traits \n",
    "              such as mean predicted value ('mu'), mean standard deviation ('sigma'), \n",
    "              all predicted values from each model ('values'), and any other custom \n",
    "              prediction properties ('custom').\n",
    "    \n",
    "    Overview:\n",
    "        This function is designed to consolidate prediction details from multiple models \n",
    "        (e.g., trees in a random forest) into aggregated traits for easier analysis.\n",
    "        \n",
    "        It calculates the average predicted value and standard deviation from all the models \n",
    "        and structures them, along with other prediction-related data, into a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_traits = {}\n",
    "    \n",
    "    # Aggregate the mean predictions across all models\n",
    "    # Note: Each model's prediction is reshaped into a column and the mean is taken across models (axis 0)\n",
    "    prediction_traits['mu'] = np.mean(np.array([np.array(x).reshape(-1,1) for x in prediction_obj['mu']]), axis=0)\n",
    "    \n",
    "    # Aggregate the mean standard deviations across all models\n",
    "    prediction_traits['sigma'] = np.mean(np.array([np.array(x).reshape(-1,1) for x in prediction_obj['sigma']]), axis=0)\n",
    "    \n",
    "    # Collect all predictions from each model and stack them column-wise\n",
    "    prediction_traits['values'] = np.column_stack(prediction_obj['pred'])\n",
    "    \n",
    "    # Extract any other custom prediction-related properties from the input prediction object\n",
    "    # These are any properties that aren't the mean predictions, standard deviations, or individual model predictions\n",
    "    prediction_traits['custom'] = {key: value for key, value in prediction_obj.items() if key not in [\"mu\", \"sigma\", \"pred\"]}\n",
    "    \n",
    "    return prediction_traits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac9236a8",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55258112",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Setting up random seed\n",
    "if settings['READ_MEMORY'] and os.path.exists(settings['RANDOM_SEED_FILE']):\n",
    "    with open(settings['RANDOM_SEED_FILE'], 'rb') as f:\n",
    "        random_seeds = np.load(f, allow_pickle=True).item()\n",
    "    print(\"Random seed configuration has been loaded from a file.\")\n",
    "else:\n",
    "    random_seeds = {}\n",
    "    current_time = time.time()\n",
    "    random_seeds['seed'] = int(current_time % (int(current_time/10000) * 10000))\n",
    "    random_seeds['set'] = random.sample(range(-1000, 1000), 100) + [random_seeds['seed']]\n",
    "    with open(settings['RANDOM_SEED_FILE'], 'wb') as f:\n",
    "        np.save(f, random_seeds)\n",
    "    print(\"Random seed configuration has been generated and saved into a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating additional paths\n",
    "settings['CONTAINER_HOME_PATH']=\"C:\\\\Users\\\\david\\\\Documents\\\\DTU\\\\_Thesis\\\\BOcalibration\\\\VSCode\"\n",
    "settings['ACTIVITY_FILE'] = '/home/s212597/simmobility/activity_schedule'  \n",
    "settings['INITIAL_PARAMS_PATH']='/home/s212597/BOcalibration/parameters.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aab19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOLFI_INPUT will store the Bayesian Optimization for Likelihood-Free Inference (BOLFI) input, \n",
    "# which typically defines the parameter space for the optimization problem.\n",
    "BOLFI_INPUT = {}\n",
    "\n",
    "# Check if the program should read from memory and if the parameter space file already exists.\n",
    "if settings['READ_MEMORY'] and os.path.exists(settings['PARAM_SPACE_FILE']):\n",
    "    # If the conditions are satisfied, the file is read and its contents are loaded into BOLFI_INPUT.\n",
    "    with open(settings['PARAM_SPACE_FILE'], 'rb') as f:\n",
    "        BOLFI_INPUT = pickle.load(f)\n",
    "    \n",
    "    # If an update is required, reload the parameter space definition.\n",
    "    if UPDATE_DEF:\n",
    "        BOLFI_INPUT['definition'] = fn_load_params_space_definition(settings['INITIAL_PARAMS_PATH'], omit=settings['OMIT_LIST'])['definition']\n",
    "    print(\"Initial parameter space has been loaded from a file.\")\n",
    "\n",
    "# If not loading from memory or the file doesn't exist, generate the parameter space.\n",
    "else:\n",
    "    BOLFI_INPUT = fn_load_params_space_definition(settings['INITIAL_PARAMS_PATH'], omit=settings['OMIT_LIST'])\n",
    "    \n",
    "    # Save the newly generated parameter space to a file for future use.\n",
    "    with open(settings['PARAM_SPACE_FILE'], 'wb') as f:\n",
    "        pickle.dump(BOLFI_INPUT, f)\n",
    "    print(\"Initial parameter space has been generated and saved into a file.\")\n",
    "\n",
    "# If the dimensionality of the parameter space is not explicitly defined in the settings,\n",
    "# set it based on the length of the 'definition' in BOLFI_INPUT.\n",
    "if 'PARAM_SPACE_DIM' not in settings:\n",
    "    settings['PARAM_SPACE_DIM'] = len(BOLFI_INPUT['definition']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532074f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target name\n",
    "settings['target_col'] = 'inadequacy'\n",
    "\n",
    "# Specify epsilon\n",
    "settings['EPSILON'] = {}\n",
    "settings['EPSILON']['time'] = {}\n",
    "settings['EPSILON']['time']['focused_sampling'] = 0.001\n",
    "settings['EPSILON']['time']['spread_sampling'] = 0.1\n",
    "settings['EPSILON']['trips'] = {}\n",
    "settings['EPSILON']['trips']['focused_sampling'] = 0.05\n",
    "settings['EPSILON']['trips']['spread_sampling'] = 1.0\n",
    "settings['EPSILON']['od'] = {}\n",
    "settings['EPSILON']['od']['focused_sampling'] = 100\n",
    "settings['EPSILON']['od']['spread_sampling'] = 10000\n",
    "\n",
    "# Save settings initially\n",
    "import pickle\n",
    "with open(settings['CONFIGURATION_OUTPUT_FILE'], 'wb') as f:\n",
    "    pickle.dump(settings, f)\n",
    "\n",
    "# Broadcast the shared environment\n",
    "shared_env = {}\n",
    "shared_env['settings'] = settings\n",
    "shared_env['param_space'] = BOLFI_INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b95475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering the columns\n",
    "new_order = ['model', 'param', 'module', 'include', 'initial', 'lower_limit',\n",
    "       'upper_limit', 'provided_initial', 'param_name', 'declaration',\n",
    "       'param_name_left_hand', 'parameter', 'direct_influence', 'enabled',\n",
    "       'tunnel_width', 'changed', 'init_lower_limit', 'init_upper_limit']\n",
    "\n",
    "param_def = BOLFI_INPUT['definition'].reindex(columns=new_order, copy=False)\n",
    "param_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04676a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameters for optimisation\n",
    "\n",
    "param_init=BOLFI_INPUT['space']\n",
    "#param_def=BOLFI_INPUT['definition']\n",
    "param_addons=BOLFI_INPUT.get('residual_pool')\n",
    "surrogate_fit_fn = settings[\"SURROGATE_MODEL_FN\"]\n",
    "surrogate_predict_fn = settings['SURROGATE_PREDICT_FN']\n",
    "surrogate_utility_fn = settings['SURROGATE_UTILITY_FN']\n",
    "init_sampling_fn = settings['INIT_SAMPLING_FUNC_STR']\n",
    "sampling_fn = settings['SAMPLING_FUNC_STR']\n",
    "acquisition_fn = \"fn_optimal_acquisition\"\n",
    "selection_fn = \"fn_pull_optimal_selection\"\n",
    "termination_fn=\"fn_termination_max_iterations\"\n",
    "inner_optimisation_fn='fn_optimisation_quasi_newton'\n",
    "simulation_fn=fn_perform_simulation\n",
    "initialize_only=settings['INIT_ONLY']\n",
    "max_iterations=settings['MAX_ITERS']\n",
    "target=settings['target_col']\n",
    "init_sample_size=settings['INIT_SAMPLE_SIZE']\n",
    "sample_size=settings['ITER_SAMPLE_SIZE']\n",
    "objective_fn=min\n",
    "method_params=settings['METHOD_PARAMS']\n",
    "opt_cores=settings['OPTIMISATION_CORES']\n",
    "cv_cores=settings['OPTIMISATION_CORES']\n",
    "sampling_sd=lambda mu: abs(mu) * settings['VARIATION_SD']\n",
    "setting_obj=settings\n",
    "enforce_bounds=True\n",
    "opt_store_intermediate=True\n",
    "is_normalization_required=(settings['SURROGATE_MODEL_FN'] in [\"fn_fit_gp\", \"fn_fit_mlegp\"])\n",
    "#eval_complement=False\n",
    "eval_complement=True\n",
    "verbose_output_fn=verbose_output\n",
    "store_iteratively = False\n",
    "save_object_fn=save_object\n",
    "opt_cores=settings['OPTIMISATION_CORES']\n",
    "cv_cores=settings['OPTIMISATION_CORES']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2f4f7",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a691ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get global settings object\n",
    "if \"RETRAIN_AFTER\" not in shared_env[\"settings\"]:\n",
    "    shared_env[\"settings\"][\"RETRAIN_AFTER\"] = 3\n",
    "settings_obj = shared_env[\"settings\"]\n",
    "\n",
    "# Compile all functions\n",
    "try:\n",
    "    fn_compilation = [\"surrogate_fit_fn\", \"surrogate_predict_fn\", \"surrogate_utility_fn\", \"init_sampling_fn\", \"sampling_fn\", \"acquisition_fn\", \"selection_fn\", \"termination_fn\", \"inner_optimisation_fn\"]\n",
    "    for fn in fn_compilation:\n",
    "        exec(\"%s = %s\" % (fn, eval(fn)))\n",
    "\n",
    "    if not callable(simulation_fn):\n",
    "        simulation_fn = eval(simulation_fn)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n",
    "# Check if terminal conditions are set up\n",
    "if termination_fn is None:\n",
    "    raise ValueError(\"Termination function is required! It must return logical value T (terminate)/F (continue). At input get the iteration number, params and inadequacy through iterations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the param space\n",
    "param_space = {\"space\":param_init,\n",
    "              \"definition\":param_def\n",
    "              }\n",
    "\n",
    "param_space.update(param_addons)\n",
    "\n",
    "if len(param_space['space']) == 0:\n",
    "    # Generate initial set of params to be simulated! (preferably 10 instances)\n",
    "    init_sampling_params = {\n",
    "        'type': init_sampling_fn,\n",
    "        'param_space': param_space\n",
    "    }\n",
    "    \n",
    "    init_sampling_params = fn_resolve_fn_params(init_sampling_fn, sample_size=init_sample_size, omit=['target'], **init_sampling_params)\n",
    "    init_samples = fn_acquisition(param_space=param_space,sample_size=sample_size, type=init_sampling_fn) #delete sample_size     \n",
    "    param_space['space'] =pd.concat([param_space['space'], init_samples], ignore_index=True)\n",
    "      \n",
    "    # Simulate initial set of params\n",
    "    simulation_fn_params = {\n",
    "        'param_def': param_space['definition']\n",
    "    }\n",
    "\n",
    "    simulation_fn_params = fn_resolve_fn_params(simulation_fn, **simulation_fn_params)\n",
    "    simulation_fn_params_copy = simulation_fn_params.copy()\n",
    "    simulation_fn_params_copy.pop('value')\n",
    "    simulation_outcome = param_space['space'].apply(lambda row: simulation_fn(row, **simulation_fn_params_copy), axis=1)\n",
    "    simulation_outcome.columns = target\n",
    "    param_space['space'] = pd.concat([param_space['space'], simulation_outcome], axis=1)\n",
    "\n",
    "termination_fn_params = fn_resolve_fn_params(termination_fn, **{'space': param_space})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2919d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename the last column to 'inadequacy'\n",
    "param_space['space'].columns = param_space['space'].columns[:-1].tolist() + ['inadequacy']\n",
    "\n",
    "# # Format the 'inadequacy' column values with 6 decimal places\n",
    "#param_space['space']['inadequacy'] = param_space['space']['inadequacy'].apply(lambda x: format(x[0], '.6f'))\n",
    "param_space['space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file to save the param_space \n",
    "#param_space[\"space\"].to_csv('param_space_space.csv', index=False)\n",
    "#param_space = {}\n",
    "#param_space[\"space\"]=pd.read_csv('/home/s212597/BOcalibration/saved_outputs/param_space_space_DD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Configure the logging system\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Change the initial sample size to align with the DTU hypercomputer's cpu cores\n",
    "sample_size=30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a1d9086",
   "metadata": {},
   "source": [
    "# Iterative algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform optimization\n",
    "iteration_no=0\n",
    "iteration_space = {}\n",
    "visuals=[]\n",
    "\n",
    "while not termination_fn(iteration=iteration_no, space=param_space,max_iterations=5): #*termination_args):\n",
    "    print(\"-------------------- Iteration Number ----------------------------\",iteration_no)\n",
    "    # 1. get the best performer and save its parameters\n",
    "    simulation_fn_params = fn_resolve_fn_params(selection_fn, _omit=[\"space\", \"selection_fn\", \"target\"])    #   **locals())\n",
    "    simulation_fn_params.pop('space', None)\n",
    "\n",
    "    print(\"simulation fn params \",simulation_fn_params)\n",
    "    iteration_space['optimal'] = fn_pull_optimal_samples(space=param_space['space'], selection_fn=selection_fn, target=target, **simulation_fn_params)\n",
    "    #print(\"---------------------iteration_space[optimal]-------------------\",iteration_space['optimal'])\n",
    "\n",
    "    if verbose_output_fn is not None:\n",
    "        verbose_output_fn(\"Retrieved current most optimal sample.\", count=len(iteration_space['optimal']['space']))\n",
    "    \n",
    "    print('--------------------------------------------- Get the best performer - Done --------------------------------------------------')\n",
    "\n",
    "    # 2. fit surrogate model\n",
    "    if (iteration_no == 1) or (iteration_no % settings_obj['RETRAIN_AFTER'] == 0):\n",
    "        fn_fit_params = fn_resolve_fn_params(fn_fit, omit=[\"type\", \"ds\", \"target\", \"param_def\"])\n",
    "        surrogate_fit_fn_params = fn_resolve_fn_params(surrogate_fit_fn, omit=[\"type\", \"ds\", \"target\", \"param_def\", \"method_params\"])\n",
    "\n",
    "        # Ensure 'type_fn' is not in the dictionaries\n",
    "        if 'type_fn' in fn_fit_params:\n",
    "            del fn_fit_params['type_fn']\n",
    "        if 'type_fn' in surrogate_fit_fn_params:\n",
    "            del surrogate_fit_fn_params['type_fn']\n",
    "\n",
    "        iteration_space.setdefault('surrogate', {})\n",
    "        iteration_space['surrogate']['model'] = fn_fit(surrogate_fit_fn, ds=param_space['space'], target=target, param_def=param_space['definition'], **fn_fit_params, **surrogate_fit_fn_params)\n",
    "        param_space['surrogate'] = iteration_space['surrogate']\n",
    "\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Fitted surrogate model(s).\", training_set_size=len(param_space['space']))\n",
    "    else:\n",
    "        iteration_space['surrogate'] = param_space['surrogate']\n",
    "        if verbose_output_fn is not None:\n",
    "            verbose_output_fn(\"Loaded previously trained surrogate model(s).\", training_set_size=len(param_space['space']))\n",
    "    print('---------------------------------------------Fit surrogate model - Done --------------------------------------------------')\n",
    "    # 3. optimise acquisition utility\n",
    "\n",
    "    # Fixing the structure \n",
    "    new_data = []\n",
    "    # Changed the below line with the below block:    \n",
    "    #param_space['surrogate']=new_data\n",
    "    # Check if 'surrogate' is a list in the dictionary\n",
    "    if 'surrogate' in param_space and isinstance(param_space['surrogate'], list):\n",
    "        for surrogate_item in param_space['surrogate']:\n",
    "            model_item = surrogate_item.get('model', {})  # Use get() to handle the case when 'model' is not present\n",
    "            new_data.append({\n",
    "                'model': model_item.get('model', None),  # Replace with the actual key for the model\n",
    "                'sub_ds_idx': surrogate_item.get('sub_ds_idx', None)  # Replace with the actual key for sub_ds_idx\n",
    "            })\n",
    "\n",
    "        # Update the value associated with the key 'surrogate'\n",
    "        param_space['surrogate'] = new_data\n",
    "    else:\n",
    "        print(\"'surrogate' not found or is not a list in param_space.\")\n",
    "\n",
    "    # Resolve function parameters for acquisition function\n",
    "    acquisition_fn_params = fn_resolve_fn_params(\n",
    "        acquisition_fn, \n",
    "        omit=['type','param_space'],  #,target],  #    \"sample_size \" ,target], \n",
    "        iter_space=iteration_space, \n",
    "        sample_size=sample_size, \n",
    "        sampling_fn=sampling_fn, \n",
    "        surrogate_predict_fn=surrogate_predict_fn, \n",
    "        surrogate_utility_fn=surrogate_utility_fn, \n",
    "        inner_optimisation_fn=inner_optimisation_fn, \n",
    "        \n",
    "    )\n",
    "\n",
    "    # Resolve function parameters for inner optimisation function\n",
    "    inner_optimisation_fn_params = fn_resolve_fn_params(\n",
    "        inner_optimisation_fn, \n",
    "        omit=['sampling_fn','surrogate_predict_fn', 'surrogate_model', 'surrogate_utility_fn', 'iteration_space', 'param_range','param_space','sample_size',\"omit\"] \n",
    "        #target=target, \n",
    "        \n",
    "    )\n",
    "\n",
    "    # Remove 'kwargs' from the dictionaries\n",
    "    acquisition_fn_params.pop('kwargs', None)\n",
    "    inner_optimisation_fn_params.pop('kwargs', None)\n",
    "\n",
    "    # Function call\n",
    "    iteration_space['potential'] = fn_acquisition(\n",
    "        type=acquisition_fn, \n",
    "        param_space=param_space, \n",
    "        **acquisition_fn_params,\n",
    "        **inner_optimisation_fn_params\n",
    "    )\n",
    "\n",
    "    visualization_data=iteration_space['potential'].copy()\n",
    "    print('---------------------------------------------Optimize acquisition utility - Done --------------------------------------------------')\n",
    "    if iteration_no == 1:\n",
    "        iteration_space['surrogate'] = iteration_space['surrogate']['model'][0]['model']\n",
    "\n",
    "    # 4. simulate results of the picked-up instances\n",
    "\n",
    "    # Get additional parameters for simulation_fn\n",
    "    # Convert the values to a DataFrame\n",
    "    df12 = pd.DataFrame([iteration_space[\"potential\"][\"space\"][0]])\n",
    "    # Rename columns using column names from param_space excluding the last one\n",
    "    df12.columns = param_space[\"space\"].columns[:-1]\n",
    "    simulation_outcome =df12.apply(lambda row: simulation_fn(row, **simulation_fn_params_copy), axis=1)\n",
    "\n",
    "    # Combine the existing data in iteration_space['potential']['space'] with the new simulation_outcome\n",
    "    iteration_space['potential']['space'] = np.column_stack((iteration_space['potential']['space'], simulation_outcome))\n",
    "\n",
    "    # Add a new column name to the dataframe\n",
    "    iteration_space['potential']['space'] = pd.DataFrame(iteration_space['potential']['space'])\n",
    "    iteration_space['potential']['space'].columns = list(iteration_space['potential']['space'].columns[:-1]) + [target]\n",
    "    iteration_space['potential']['space'].columns = iteration_space[\"optimal\"][\"space\"].columns\n",
    "\n",
    "    # Call the verbose_output_fn if it exists\n",
    "    if verbose_output_fn is not None:\n",
    "        verbose_output_fn(\"Simulation of the most promising samples is completed.\")\n",
    "\n",
    "    print('---------------------------------------------Simulate results of the optimal sample set - Done --------------------------------------------------')\n",
    "\n",
    "    # 5. add simulated results to the base set\n",
    "    param_space['space'] = pd.concat([param_space['space'], pd.DataFrame(iteration_space['potential']['space'])], ignore_index=True)\n",
    "    #print(\"-----------------------------param_space final--------------------------------------------------\",param_space[\"space\"])\n",
    "    if verbose_output_fn is not None: \n",
    "        verbose_output_fn(\"Simulated results are added to the global set.\")\n",
    "        \n",
    "    # Store NRMSEs at each iteration and append\n",
    "    if iteration_no == 0:\n",
    "        respool_dict = {}\n",
    "        respool_dict = shared_env['param_space']['residual_pool']\n",
    "        respool_dict['time'] = respool_dict['time'].tolist()\n",
    "        respool_dict['trips'] = respool_dict['trips'].tolist()\n",
    "        respool_dict['od'] = respool_dict['od'].tolist()\n",
    "\n",
    "        print(\"FIRST: respool_dict: \\n\", respool_dict)\n",
    "    else:\n",
    "        respool_dict['time'] = respool_dict['time'].tolist()\n",
    "        respool_dict['trips'] = respool_dict['trips'].tolist()\n",
    "        respool_dict['od'] = respool_dict['od'].tolist()\n",
    "        respool_dict['time'].append(shared_env['param_space']['residual_pool']['time'][-1])\n",
    "        respool_dict['trips'].append(shared_env['param_space']['residual_pool']['trips'][-1])\n",
    "        respool_dict['od'].append(shared_env['param_space']['residual_pool']['od'][-1])\n",
    "\n",
    "    # Store required objects\n",
    "    if save_object_fn is not None and store_iteratively: \n",
    "        print(type(settings))\n",
    "        save_object_fn(iteration_space, settings, \"param_space\", stage = \"optimisation\", iteration = iteration_no)\n",
    "    if save_object_fn is not None: \n",
    "        save_object_fn(param_space, settings,\"output_object\", stage = \"partial\")\n",
    "    if \"shared_env\" in globals() and \"param_space\" in shared_env: \n",
    "        temp_respool = shared_env['param_space']['residual_pool']\n",
    "        shared_env['param_space'] = param_space\n",
    "        shared_env['param_space']['residual_pool'] = temp_respool\n",
    "    \n",
    "    print('---------------------------------------------Add simulated results to the parameter space - Done --------------------------------------------------')\n",
    "\n",
    "    # Increase iteration number\n",
    "    iteration_no += 1\n",
    "    visuals.append(visualization_data)\n",
    "    for row in param_space[\"space\"][\"inadequacy\"]:\n",
    "        if row < 0.05:\n",
    "            print(\"Row causing termination:\", row)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It creates the dump file that includes the final param space\n",
    "with open('param_space_dpx.pkl', 'wb') as f:\n",
    "    pickle.dump(param_space, f)\n",
    "\n",
    "#It creates the dump file that includes all the utility values for visualizations purposes\n",
    "with open('visuals_dpx.pkl', 'wb') as f:\n",
    "    pickle.dump(visuals, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate initial sample set's inadecuacy\n",
    "\n",
    "initial = param_space[\"definition\"].set_index(\"parameter\")[\"value\"].transpose().to_frame().T\n",
    "initial.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "original_outcome = initial.apply(lambda row: simulation_fn(row, **simulation_fn_params_copy), axis=1)\n",
    "initial['inadequacy']=original_outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 performers\n",
    "optimal_samples5 = param_space[\"space\"].nsmallest(5, 'inadequacy')\n",
    "optimal_samples5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b8b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "240px",
    "width": "258px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
